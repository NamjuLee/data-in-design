/*! For license information please see 2667.e0110f6d.chunk.js.LICENSE.txt */
"use strict";(self.webpackChunkData_in_Design=self.webpackChunkData_in_Design||[]).push([[2667],{57513:(t,e,n)=>{var s=n(33753),i=n(79494);(0,i.tp)().prototype.abs=function(){return this.throwIfDisposed(),(0,s.t)(this)};var a=n(4759);(0,i.tp)().prototype.acos=function(){return this.throwIfDisposed(),(0,a.H)(this)};var r=n(40983);(0,i.tp)().prototype.acosh=function(){return this.throwIfDisposed(),(0,r.F)(this)};var o=n(87242);(0,i.tp)().prototype.add=function(t){return this.throwIfDisposed(),(0,o.W)(this,t)};var l=n(51802);(0,i.tp)().prototype.all=function(t,e){return this.throwIfDisposed(),(0,l.Q)(this,t,e)};var u=n(41819);(0,i.tp)().prototype.any=function(t,e){return this.throwIfDisposed(),(0,u.b)(this,t,e)};var c=n(23068);(0,i.tp)().prototype.argMax=function(t){return this.throwIfDisposed(),(0,c.F)(this,t)};var h=n(58482);(0,i.tp)().prototype.argMin=function(t){return this.throwIfDisposed(),(0,h.X)(this,t)};var p=n(45583),d=n(71426);(0,i.tp)().prototype.asScalar=function(){return this.throwIfDisposed(),(0,d.vA)(1===this.size,(()=>"The array must have only 1 element.")),(0,p.t)(this,[])};var f=n(47794);(0,i.tp)().prototype.asType=function(t){return this.throwIfDisposed(),(0,f.w)(this,t)},(0,i.tp)().prototype.as1D=function(){return this.throwIfDisposed(),(0,p.t)(this,[this.size])},(0,i.tp)().prototype.as2D=function(t,e){return this.throwIfDisposed(),(0,p.t)(this,[t,e])},(0,i.tp)().prototype.as3D=function(t,e,n){return this.throwIfDisposed(),(0,p.t)(this,[t,e,n])},(0,i.tp)().prototype.as4D=function(t,e,n,s){return this.throwIfDisposed(),(0,p.t)(this,[t,e,n,s])},(0,i.tp)().prototype.as5D=function(t,e,n,s,i){return this.throwIfDisposed(),(0,p.t)(this,[t,e,n,s,i])};var m=n(13010);(0,i.tp)().prototype.asin=function(){return this.throwIfDisposed(),(0,m.q)(this)};var g=n(46464);(0,i.tp)().prototype.asinh=function(){return this.throwIfDisposed(),(0,g.y)(this)};var y=n(7803);(0,i.tp)().prototype.atan=function(){return this.throwIfDisposed(),(0,y.r)(this)};var b=n(46349);(0,i.tp)().prototype.atan2=function(t){return this.throwIfDisposed(),(0,b.F)(this,t)};var k=n(48147);(0,i.tp)().prototype.atanh=function(){return this.throwIfDisposed(),(0,k.r)(this)};var w=n(28650);(0,i.tp)().prototype.avgPool=function(t,e,n,s){return this.throwIfDisposed(),(0,w.$)(this,t,e,n,s)};var v=n(2557);(0,i.tp)().prototype.batchToSpaceND=function(t,e){return this.throwIfDisposed(),(0,v.G)(this,t,e)};var I=n(27237);(0,i.tp)().prototype.batchNorm=function(t,e,n,s,i){return this.throwIfDisposed(),(0,I.$)(this,t,e,n,s,i)};var N=n(4920);(0,i.tp)().prototype.broadcastTo=function(t){return this.throwIfDisposed(),(0,N.h)(this,t)},(0,i.tp)().prototype.cast=function(t){return this.throwIfDisposed(),(0,f.w)(this,t)};var S=n(95396);(0,i.tp)().prototype.ceil=function(){return this.throwIfDisposed(),(0,S.m)(this)};var x=n(53829);(0,i.tp)().prototype.clipByValue=function(t,e){return this.throwIfDisposed(),(0,x.z)(this,t,e)};var T=n(94429);(0,i.tp)().prototype.concat=function(t,e){return this.throwIfDisposed(),t instanceof i.qY&&(t=[t]),(0,T.x)([this,...t],e)};var z=n(89870);(0,i.tp)().prototype.conv1d=function(t,e,n,s,i,a){return this.throwIfDisposed(),(0,z.k)(this,t,e,n,s,i,a)};var A=n(1137);(0,i.tp)().prototype.conv2dTranspose=function(t,e,n,s,i){return this.throwIfDisposed(),(0,A.w)(this,t,e,n,s,i)};var F=n(14969);(0,i.tp)().prototype.conv2d=function(t,e,n,s,i,a){return this.throwIfDisposed(),(0,F.X)(this,t,e,n,s,i,a)};var D=n(35894);(0,i.tp)().prototype.cos=function(){return this.throwIfDisposed(),(0,D.g)(this)};var C=n(71164);(0,i.tp)().prototype.cosh=function(){return this.throwIfDisposed(),(0,C.y)(this)};var E=n(63739);(0,i.tp)().prototype.cumprod=function(t,e,n){return this.throwIfDisposed(),(0,E.L)(this,t,e,n)};var _=n(97167);(0,i.tp)().prototype.cumsum=function(t,e,n){return this.throwIfDisposed(),(0,_.r)(this,t,e,n)};var M=n(41719);(0,i.tp)().prototype.depthToSpace=function(t,e){return this.throwIfDisposed(),(0,M.R)(this,t,e)};var R=n(49899);(0,i.tp)().prototype.depthwiseConv2d=function(t,e,n,s,i,a){return this.throwIfDisposed(),(0,R.G)(this,t,e,n,s,i,a)};var L=n(83789);(0,i.tp)().prototype.dilation2d=function(t,e,n,s,i){return this.throwIfDisposed(),(0,L.X)(this,t,e,n,s,i)};var O=n(90044);(0,i.tp)().prototype.divNoNan=function(t){return this.throwIfDisposed(),(0,O.e)(this,t)};var W=n(24974);(0,i.tp)().prototype.div=function(t){return this.throwIfDisposed(),(0,W.y)(this,t)};var B=n(36434);(0,i.tp)().prototype.dot=function(t){return this.throwIfDisposed(),(0,B.O)(this,t)};var P=n(83645);(0,i.tp)().prototype.elu=function(){return this.throwIfDisposed(),(0,P.P)(this)};var U=n(37457);(0,i.tp)().prototype.equal=function(t){return this.throwIfDisposed(),(0,U.L)(this,t)};var H=n(48810);(0,i.tp)().prototype.erf=function(){return this.throwIfDisposed(),(0,H.Y)(this)};var V=n(28826);(0,i.tp)().prototype.euclideanNorm=function(t,e){return this.throwIfDisposed(),(0,V.p)(this,t,e)};var j=n(25462);(0,i.tp)().prototype.exp=function(){return this.throwIfDisposed(),(0,j.o)(this)};var G=n(74023);(0,i.tp)().prototype.expandDims=function(t){return this.throwIfDisposed(),(0,G.U)(this,t)};var q=n(72520);(0,i.tp)().prototype.expm1=function(){return this.throwIfDisposed(),(0,q.I)(this)};var Z=n(63062);(0,i.tp)().prototype.fft=function(){return this.throwIfDisposed(),(0,Z.h)(this)},(0,i.tp)().prototype.flatten=function(){return this.throwIfDisposed(),(0,p.t)(this,[this.size])};var K=n(48587);(0,i.tp)().prototype.floor=function(){return this.throwIfDisposed(),(0,K.R)(this)};var J=n(63612);(0,i.tp)().prototype.floorDiv=function(t){return this.throwIfDisposed(),(0,J.w)(this,t)};var Y=n(56178);(0,i.tp)().prototype.gather=function(t,e,n){return this.throwIfDisposed(),(0,Y.k)(this,t,e,n)};var X=n(9996);(0,i.tp)().prototype.greaterEqual=function(t){return this.throwIfDisposed(),(0,X.D)(this,t)};var Q=n(22759);(0,i.tp)().prototype.greater=function(t){return this.throwIfDisposed(),(0,Q.r)(this,t)};var $=n(1405);(0,i.tp)().prototype.ifft=function(){return this.throwIfDisposed(),(0,$.K)(this)};var tt=n(57307);(0,i.tp)().prototype.irfft=function(){return this.throwIfDisposed(),(0,tt.g)(this)};var et=n(9027);(0,i.tp)().prototype.isFinite=function(){return this.throwIfDisposed(),(0,et.M)(this)};var nt=n(76005);(0,i.tp)().prototype.isInf=function(){return this.throwIfDisposed(),(0,nt.E)(this)};var st=n(13467);(0,i.tp)().prototype.isNaN=function(){return this.throwIfDisposed(),(0,st.y)(this)};var it=n(18076);(0,i.tp)().prototype.leakyRelu=function(t){return this.throwIfDisposed(),(0,it.H)(this,t)};var at=n(827);(0,i.tp)().prototype.lessEqual=function(t){return this.throwIfDisposed(),(0,at.I)(this,t)};var rt=n(58320);(0,i.tp)().prototype.less=function(t){return this.throwIfDisposed(),(0,rt.M)(this,t)};var ot=n(66584);(0,i.tp)().prototype.localResponseNormalization=function(t,e,n,s){return this.throwIfDisposed(),(0,ot.K)(this,t,e,n,s)};var lt=n(44824);(0,i.tp)().prototype.logSigmoid=function(){return this.throwIfDisposed(),(0,lt.n)(this)};var ut=n(71194);(0,i.tp)().prototype.logSoftmax=function(t){return this.throwIfDisposed(),(0,ut.H)(this,t)};var ct=n(42175);(0,i.tp)().prototype.logSumExp=function(t,e){return this.throwIfDisposed(),(0,ct.V)(this,t,e)};var ht=n(75911);(0,i.tp)().prototype.log=function(){return this.throwIfDisposed(),(0,ht.R)(this)};var pt=n(21386);(0,i.tp)().prototype.log1p=function(){return this.throwIfDisposed(),(0,pt.K)(this)};var dt=n(45692);(0,i.tp)().prototype.logicalAnd=function(t){return this.throwIfDisposed(),(0,dt.n)(this,t)};var ft=n(15344);(0,i.tp)().prototype.logicalNot=function(){return this.throwIfDisposed(),(0,ft.N)(this)};var mt=n(47920);(0,i.tp)().prototype.logicalOr=function(t){return this.throwIfDisposed(),(0,mt.z)(this,t)};var gt=n(63708);(0,i.tp)().prototype.logicalXor=function(t){return this.throwIfDisposed(),(0,gt.r)(this,t)};var yt=n(5162);(0,i.tp)().prototype.matMul=function(t,e,n){return this.throwIfDisposed(),(0,yt.N)(this,t,e,n)};var bt=n(6044);(0,i.tp)().prototype.maxPool=function(t,e,n,s){return this.throwIfDisposed(),(0,bt.j)(this,t,e,n,s)};var kt=n(60891);(0,i.tp)().prototype.max=function(t,e){return this.throwIfDisposed(),(0,kt.T)(this,t,e)};var wt=n(82131);(0,i.tp)().prototype.maximum=function(t){return this.throwIfDisposed(),(0,wt.P)(this,t)};var vt=n(55396);(0,i.tp)().prototype.mean=function(t,e){return this.throwIfDisposed(),(0,vt.i)(this,t,e)};var It=n(21465);(0,i.tp)().prototype.min=function(t,e){return this.throwIfDisposed(),(0,It.j)(this,t,e)};var Nt=n(6561);(0,i.tp)().prototype.minimum=function(t){return this.throwIfDisposed(),(0,Nt.B)(this,t)};var St=n(84252);(0,i.tp)().prototype.mirrorPad=function(t,e){return this.throwIfDisposed(),(0,St.F)(this,t,e)};var xt=n(73617);(0,i.tp)().prototype.mod=function(t){return this.throwIfDisposed(),(0,xt.z)(this,t)};var Tt=n(20803);(0,i.tp)().prototype.mul=function(t){return this.throwIfDisposed(),(0,Tt.l)(this,t)};var zt=n(1011);(0,i.tp)().prototype.neg=function(){return this.throwIfDisposed(),(0,zt.H)(this)};var At=n(92155);(0,i.tp)().prototype.norm=function(t,e,n){return this.throwIfDisposed(),(0,At.x)(this,t,e,n)};var Ft=n(50135);(0,i.tp)().prototype.notEqual=function(t){return this.throwIfDisposed(),(0,Ft.E)(this,t)};var Dt=n(26865);(0,i.tp)().prototype.oneHot=function(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:1,n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:0;return this.throwIfDisposed(),(0,Dt.M)(this,t,e,n)};var Ct=n(64100);(0,i.tp)().prototype.onesLike=function(){return this.throwIfDisposed(),(0,Ct.P)(this)};var Et=n(85048);(0,i.tp)().prototype.pad=function(t,e){return this.throwIfDisposed(),(0,Et.e)(this,t,e)};var _t=n(47751);(0,i.tp)().prototype.pool=function(t,e,n,s,i,a){return this.throwIfDisposed(),(0,_t.d)(this,t,e,n,s,i,a)};var Mt=n(77587);(0,i.tp)().prototype.pow=function(t){return this.throwIfDisposed(),(0,Mt.n)(this,t)};var Rt=n(99855);(0,i.tp)().prototype.prelu=function(t){return this.throwIfDisposed(),(0,Rt.N)(this,t)};var Lt=n(29556);(0,i.tp)().prototype.prod=function(t,e){return this.throwIfDisposed(),(0,Lt._)(this,t,e)};var Ot=n(46309);(0,i.tp)().prototype.reciprocal=function(){return this.throwIfDisposed(),(0,Ot.V)(this)};var Wt=n(85251);(0,i.tp)().prototype.relu=function(){return this.throwIfDisposed(),(0,Wt.V)(this)};var Bt=n(99721);(0,i.tp)().prototype.relu6=function(){return this.throwIfDisposed(),(0,Bt.j)(this)},(0,i.tp)().prototype.reshapeAs=function(t){return this.throwIfDisposed(),(0,p.t)(this,t.shape)},(0,i.tp)().prototype.reshape=function(t){return this.throwIfDisposed(),(0,p.t)(this,t)};var Pt=n(22214);(0,i.tp)().prototype.resizeBilinear=function(t,e,n){return this.throwIfDisposed(),(0,Pt.v)(this,t,e,n)};var Ut=n(92823);(0,i.tp)().prototype.resizeNearestNeighbor=function(t,e,n){return this.throwIfDisposed(),(0,Ut.b)(this,t,e,n)};var Ht=n(47419);(0,i.tp)().prototype.reverse=function(t){return this.throwIfDisposed(),(0,Ht.B)(this,t)};var Vt=n(49486);(0,i.tp)().prototype.rfft=function(){return this.throwIfDisposed(),(0,Vt.z)(this)};var jt=n(96509);(0,i.tp)().prototype.round=function(){return this.throwIfDisposed(),(0,jt.L)(this)};var Gt=n(26777);(0,i.tp)().prototype.rsqrt=function(){return this.throwIfDisposed(),(0,Gt.Z)(this)};var qt=n(42022);(0,i.tp)().prototype.selu=function(){return this.throwIfDisposed(),(0,qt.W)(this)};var Zt=n(63117);(0,i.tp)().prototype.separableConv2d=function(t,e,n,s,i,a){return this.throwIfDisposed(),(0,Zt.w)(this,t,e,n,s,i,a)};var Kt=n(74213);(0,i.tp)().prototype.sigmoid=function(){return this.throwIfDisposed(),(0,Kt.r)(this)};var Jt=n(25740);(0,i.tp)().prototype.sign=function(){return this.throwIfDisposed(),(0,Jt._)(this)};var Yt=n(49519);(0,i.tp)().prototype.sin=function(){return this.throwIfDisposed(),(0,Yt.F)(this)};var Xt=n(8143);(0,i.tp)().prototype.sinh=function(){return this.throwIfDisposed(),(0,Xt.L)(this)};var Qt=n(10463);(0,i.tp)().prototype.slice=function(t,e){return this.throwIfDisposed(),(0,Qt.d)(this,t,e)};var $t=n(86719);(0,i.tp)().prototype.softmax=function(t){return this.throwIfDisposed(),(0,$t.V)(this,t)};var te=n(9973);(0,i.tp)().prototype.softplus=function(){return this.throwIfDisposed(),(0,te.l)(this)};var ee=n(63585);(0,i.tp)().prototype.spaceToBatchND=function(t,e){return this.throwIfDisposed(),(0,ee.e)(this,t,e)};var ne=n(35181);(0,i.tp)().prototype.split=function(t,e){return this.throwIfDisposed(),(0,ne.l)(this,t,e)};var se=n(70191);(0,i.tp)().prototype.sqrt=function(){return this.throwIfDisposed(),(0,se.R)(this)};var ie=n(7738);(0,i.tp)().prototype.square=function(){return this.throwIfDisposed(),(0,ie.E)(this)};var ae=n(30208);(0,i.tp)().prototype.squaredDifference=function(t){return this.throwIfDisposed(),(0,ae.P)(this,t)};var re=n(60613);(0,i.tp)().prototype.squeeze=function(t){return this.throwIfDisposed(),(0,re.r)(this,t)};var oe=n(901);(0,i.tp)().prototype.stack=function(t,e){this.throwIfDisposed();const n=t instanceof i.qY?[this,t]:[this,...t];return(0,oe.t)(n,e)};var le=n(60319);(0,i.tp)().prototype.step=function(t){return this.throwIfDisposed(),(0,le.P)(this,t)};var ue=n(91231);(0,i.tp)().prototype.stridedSlice=function(t,e,n,s,i,a,r,o){return this.throwIfDisposed(),(0,ue.Y)(this,t,e,n,s,i,a,r,o)};var ce=n(37951);(0,i.tp)().prototype.sub=function(t){return this.throwIfDisposed(),(0,ce.j)(this,t)};var he=n(47382);(0,i.tp)().prototype.sum=function(t,e){return this.throwIfDisposed(),(0,he.c)(this,t,e)};var pe=n(44894);(0,i.tp)().prototype.tan=function(){return this.throwIfDisposed(),(0,pe.M)(this)};var de=n(66532);(0,i.tp)().prototype.tanh=function(){return this.throwIfDisposed(),(0,de.y)(this)};var fe=n(43017);(0,i.tp)().prototype.tile=function(t){return this.throwIfDisposed(),(0,fe.V)(this,t)},(0,i.tp)().prototype.toBool=function(){return this.throwIfDisposed(),(0,f.w)(this,"bool")},(0,i.tp)().prototype.toFloat=function(){return this.throwIfDisposed(),(0,f.w)(this,"float32")},(0,i.tp)().prototype.toInt=function(){return this.throwIfDisposed(),(0,f.w)(this,"int32")};var me=n(92765);(0,i.tp)().prototype.topk=function(t,e){return this.throwIfDisposed(),(0,me.r)(this,t,e)};var ge=n(2154);(0,i.tp)().prototype.transpose=function(t){return this.throwIfDisposed(),(0,ge.m)(this,t)};var ye=n(59694);(0,i.tp)().prototype.unique=function(t){return this.throwIfDisposed(),(0,ye.A)(this,t)};var be=n(65149);(0,i.tp)().prototype.unsortedSegmentSum=function(t,e){return this.throwIfDisposed(),(0,be.z)(this,t,e)};var ke=n(3984);(0,i.tp)().prototype.unstack=function(t){return this.throwIfDisposed(),(0,ke.K)(this,t)};var we=n(93986);(0,i.tp)().prototype.where=function(t,e){return this.throwIfDisposed(),(0,we._)(t,this,e)};var ve=n(63290);(0,i.tp)().prototype.zerosLike=function(){return this.throwIfDisposed(),(0,ve.P)(this)}},82667:(t,e,n)=>{n.d(e,{ZFI:()=>s,mT8:()=>i.linspace,O0W:()=>Xa,YYh:()=>i.losses,ilg:()=>$a,KtR:()=>i.tensor2d,DZQ:()=>i.tidy,BaG:()=>i.train,ZSL:()=>i.util});var s={};n.r(s),n.d(s,{Layer:()=>bi,RNN:()=>no,RNNCell:()=>so,activation:()=>Ol,add:()=>ql,alphaDropout:()=>Lu,average:()=>Zl,averagePooling1d:()=>nu,averagePooling2d:()=>au,averagePooling3d:()=>lu,avgPool1d:()=>su,avgPool2d:()=>ru,avgPool3d:()=>uu,avgPooling1d:()=>iu,avgPooling2d:()=>ou,avgPooling3d:()=>cu,batchNormalization:()=>$l,bidirectional:()=>Au,categoryEncoding:()=>Uu,centerCrop:()=>Bu,concatenate:()=>Kl,conv1d:()=>Al,conv2d:()=>Fl,conv2dTranspose:()=>Dl,conv3d:()=>Cl,conv3dTranspose:()=>El,convLstm2d:()=>Su,convLstm2dCell:()=>xu,cropping2D:()=>Ml,dense:()=>Wl,depthwiseConv2d:()=>Ll,dot:()=>Ql,dropout:()=>Bl,elu:()=>Il,embedding:()=>Gl,flatten:()=>Ul,gaussianDropout:()=>Ru,gaussianNoise:()=>Mu,globalAveragePooling1d:()=>hu,globalAveragePooling2d:()=>pu,globalMaxPool1d:()=>Du,globalMaxPool2d:()=>Cu,globalMaxPooling1d:()=>du,globalMaxPooling2d:()=>fu,gru:()=>bu,gruCell:()=>ku,input:()=>tr,inputLayer:()=>vl,layerNormalization:()=>tu,leakyReLU:()=>Sl,lstm:()=>wu,lstmCell:()=>vu,masking:()=>Ou,maxPool1d:()=>Eu,maxPool2d:()=>_u,maxPooling1d:()=>mu,maxPooling2d:()=>gu,maxPooling3d:()=>yu,maximum:()=>Jl,minimum:()=>Yl,multiply:()=>Xl,permute:()=>jl,prelu:()=>xl,randomWidth:()=>Hu,reLU:()=>Nl,repeatVector:()=>Hl,rescaling:()=>Wu,reshape:()=>Vl,resizing:()=>Pu,rnn:()=>Tu,separableConv2d:()=>_l,simpleRNN:()=>Iu,simpleRNNCell:()=>Nu,softmax:()=>Tl,spatialDropout1d:()=>Pl,stackedRNNCells:()=>zu,thresholdedReLU:()=>zl,timeDistributed:()=>Fu,upSampling2d:()=>Rl,zeroPadding2d:()=>eu});var i=n(80400),a=n(39554),r=n(47794),o=n(20803),l=n(60319);const u={kernelName:a.ljI,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,l.P)((0,r.w)(n,"float32"),-1))}}};var c=n(24974),h=n(1011),p=n(51997),d=n(70191),f=n(7738),m=n(37951);const g={kernelName:a.Vvy,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=(0,f.E)((0,r.w)(n,"float32")),s=(0,d.R)((0,m.j)((0,p.d)(1),e));return(0,h.H)((0,c.y)(t,s))}}}},y={kernelName:a.PH8,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=(0,d.R)((0,m.j)((0,f.E)((0,r.w)(n,"float32")),1));return(0,c.y)(t,e)}}}};var b=n(48805),k=n(45583),w=n(47382);const v={kernelName:a.OMN,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=b.assertAndGetBroadcastShape(n.shape,s.shape);return{a:()=>{let e=t;const s=b.getReductionAxes(n.shape,i);return s.length>0&&(e=(0,w.c)(e,s)),(0,k.t)(e,n.shape)},b:()=>{let e=t;const n=b.getReductionAxes(s.shape,i);return n.length>0&&(e=(0,w.c)(e,n)),(0,k.t)(e,s.shape)}}}},I={kernelName:a.EkD,saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}};var N=n(63290);const S={kernelName:a.Jp_,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,N.P)(n)}}},x={kernelName:a.p_m,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,N.P)(n)}}},T={kernelName:a.QKF,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,d.R)((0,m.j)((0,p.d)(1),(0,f.E)((0,r.w)(n,"float32")))))}}};var z=n(87242);const A={kernelName:a.epO,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=(0,d.R)((0,z.W)((0,p.d)(1),(0,f.E)((0,r.w)(n,"float32"))));return(0,c.y)(t,e)}}}},F={kernelName:a.lxb,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,b.assertAndGetBroadcastShape)(n.shape,s.shape);return{a:()=>{const e=(0,z.W)((0,f.E)(n),(0,f.E)(s));let a=(0,o.l)(t,(0,c.y)(s,e));const r=(0,b.getReductionAxes)(n.shape,i);return r.length>0&&(a=(0,w.c)(a,r)),(0,k.t)(a,n.shape)},b:()=>{const e=(0,z.W)((0,f.E)(n),(0,f.E)(s));let a=(0,h.H)((0,o.l)(t,(0,c.y)(n,e)));const r=(0,b.getReductionAxes)(s.shape,i);return r.length>0&&(a=(0,w.c)(a,r)),(0,k.t)(a,s.shape)}}}},D={kernelName:a.TyE,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,z.W)((0,f.E)((0,r.w)(n,"float32")),1))}}},C={kernelName:a.zP9,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,m.j)((0,p.d)(1),(0,f.E)((0,r.w)(n,"float32"))))}}};var E=n(82164),_=n(64148),M=n(71426),R=n(17538),L=n(47892);const O=(0,L.op)({avgPool3dGrad_:function(t,e,n,s,i,r){const o=(0,_.YT)(t,"dy","avgPool3dGrad"),l=(0,_.YT)(e,"input","avgPool3dGrad");let u=o,c=l,h=!1;4===l.rank&&(h=!0,u=(0,k.t)(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=(0,k.t)(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]])),M.vA(5===u.rank,(()=>"Error in avgPool3dGrad: dy must be rank 5 but got rank "+"".concat(u.rank,"."))),M.vA(5===c.rank,(()=>"Error in avgPool3dGrad: input must be rank 5 but got rank "+"".concat(c.rank,"."))),(0,R.s_)("avgPool3dGrad",i,r);const p={dy:u,input:c},d={filterSize:n,strides:s,pad:i,dimRoundingMode:r},f=E.T2.runKernel(a.wwC,p,d);return h?(0,k.t)(f,[f.shape[1],f.shape[2],f.shape[3],f.shape[4]]):f}}),W={kernelName:a.cS,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:a,pad:r,dimRoundingMode:o}=n;return{x:()=>O(t,s,i,a,r,o)}}};const B=(0,L.op)({avgPoolGrad_:function(t,e,n,s,i){const r=(0,_.YT)(t,"dy","avgPoolGrad"),o=(0,_.YT)(e,"input","avgPoolGrad");M.vA(o.rank===r.rank,(()=>"Rank of input (".concat(o.rank,") does not match rank of dy (").concat(r.rank,")")));let l=o,u=r,c=!1;3===o.rank&&(c=!0,l=(0,k.t)(o,[1,o.shape[0],o.shape[1],o.shape[2]]),u=(0,k.t)(r,[1,r.shape[0],r.shape[1],r.shape[2]])),M.vA(4===u.rank,(()=>"Error in avgPoolGrad: dy must be rank 4 but got rank "+"".concat(u.rank,"."))),M.vA(4===l.rank,(()=>"Error in avgPoolGrad: input must be rank 4 but got rank "+"".concat(l.rank,".")));const h={dy:u,input:l},p={filterSize:n,strides:s,pad:i},d=E.T2.runKernel(a.VCH,h,p);return c?(0,k.t)(d,[d.shape[1],d.shape[2],d.shape[3]]):d}}),P={kernelName:a.ho8,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:a,pad:r}=n;return{x:()=>B(t,s,i,a,r)}}};var U=n(5162);const H={kernelName:a.jAQ,inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:a,transposeB:r}=n;return a||r?!a&&r?{a:()=>(0,U.N)(t,i,!1,!1),b:()=>(0,U.N)(t,s,!0,!1)}:a&&!r?{a:()=>(0,U.N)(i,t,!1,!0),b:()=>(0,U.N)(s,t,!1,!1)}:{a:()=>(0,U.N)(i,t,!0,!0),b:()=>(0,U.N)(t,s,!0,!0)}:{a:()=>(0,U.N)(t,i,!1,!0),b:()=>(0,U.N)(s,t,!0,!1)}}};var V=n(63585);const j={kernelName:a.Ik2,gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>(0,V.e)(t,s,i)}}},G={kernelName:a.LB5,gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,a=s.shape,r=Array.from(a);for(let l=i.length-1;l>=0;l--)if(i[l]===a[l])r[l]=1;else if(1!==i[l])throw new Error("broadcastTo(): [".concat(i,"] cannot be broadcast to [").concat(a,"]."));const o=[];for(let l=0;l<r.length;l++)r[l]>1&&o.push(l);return{x:()=>(0,w.c)(t,o,!0)}}},q={kernelName:a.KXH,gradFunc:t=>({x:()=>t.clone()})},Z={kernelName:a.QDP,gradFunc:t=>({x:()=>(0,N.P)(t)})};var K=n(9996),J=n(827),Y=n(45692),X=n(93986);const Q={kernelName:a.vaV,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:a}=n;return{x:()=>(0,X._)((0,Y.n)((0,K.D)(s,i),(0,J.I)(s,a)),t,(0,N.P)(t))}}},$={kernelName:a.$zE,inputsToSave:["x"],gradFunc:u.gradFunc};var tt=n(35181);const et={kernelName:a.$dB,saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,a=(0,M.Y6)(i,e[0].shape)[0],r=s.map((t=>t[a]));return(0,tt.l)(t,r,a).map((t=>()=>t))}};var nt=n(56927),st=n(91565);const it={kernelName:a.p2J,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:a,strides:r,pad:o,dataFormat:l}=n;return M.vA(R.Dh(a),(()=>"Error in gradient of conv2D: dilation rates greater than 1 "+"are not yet supported in gradients. Got dilations '".concat(a,"'"))),{x:()=>(0,st.v)(s.shape,t,i,r,o,l),filter:()=>(0,nt.H)(s,t,i.shape,r,o,l)}}};var at=n(14969);const rt={kernelName:a.jfg,inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:a,pad:r,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>(0,at.X)(t,i,a,r,o,1,l),filter:()=>(0,nt.H)(t,s,i.shape,a,r,o,l)}}};const ot=(0,L.op)({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=(0,k.t)(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let o=e;4===o.rank&&(o=(0,k.t)(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),M.vA(5===r.rank,(()=>"Error in conv3dDerFilter: input must be rank 5, but got shape "+"".concat(r.shape,"."))),M.vA(5===o.rank,(()=>"Error in conv3dDerFilter: dy must be rank 5, but got shape "+"".concat(o.shape,"."))),M.vA(5===n.length,(()=>"Error in conv3dDerFilter: filterShape must be length 5, but got "+"".concat(n,"."))),M.vA(r.shape[4]===n[3],(()=>"Error in conv3dDerFilter: depth of input ".concat(r.shape[4],") must ")+"match input depth in filter (".concat(n[3],"."))),M.vA(o.shape[4]===n[4],(()=>"Error in conv3dDerFilter: depth of dy (".concat(o.shape[4],") must ")+"match output depth for filter (".concat(n[4],").")));const l={x:r,dy:o},u={strides:s,pad:i,filterShape:n};return E.T2.runKernel(a.iGz,l,u)}});var lt=n(46410);const ut={kernelName:a.A1h,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:a}=n;M.vA((0,R.Dh)(s),(()=>"Error in gradient of conv3D: dilation rates greater than 1 are "+"not yet supported in gradients. Got dilations '".concat(s,"'")));const[r,o]=e;return{x:()=>(0,lt.c)(r.shape,t,o,i,a),filter:()=>ot(r,t,o.shape,i,a)}}};var ct=n(49519);const ht={kernelName:a.Mn0,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)((0,h.H)((0,ct.F)((0,r.w)(n,"float32"))),t)}}};var pt=n(8143);const dt={kernelName:a.MnK,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)((0,pt.L)((0,r.w)(n,"float32")),t)}}};var ft=n(19235),mt=n(97167),gt=n(2154);const yt={kernelName:a.nY8,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:a,reverse:r}=n;return{x:()=>{const e=(0,ft.Em)([i],s.rank);let n=(0,mt.r)(t,i,a,!r);return null!=e&&(n=(0,gt.m)(n,e)),n}}}};var bt=n(17399),kt=n(10261);const wt={kernelName:a.tGH,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:a,dimRoundingMode:r}=n,o=null==s?[1,1]:s;M.vA(R.Dh(o),(()=>"Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations "+"'".concat(o,"'")));const[l,u]=e;return M.vA(4===l.rank,(()=>"Error in gradient of depthwiseConv2dNative: input must be "+"rank 4, but got rank ".concat(l.rank,"."))),M.vA(4===u.rank,(()=>"Error in gradient of depthwiseConv2dNative: filter must be "+"rank 4, but got rank ".concat(u.rank,"."))),M.vA(l.shape[3]===u.shape[2],(()=>"Error in gradient of depthwiseConv2d: number of input "+"channels (".concat(l.shape[3],") must match the inChannels dimension ")+"in filter ".concat(u.shape[2],"."))),M.vA(R.G0(i,o),(()=>"Error in gradient of depthwiseConv2d: Either strides or "+"dilations must be  1. Got strides ".concat(i," and dilations ")+"'".concat(o,"'."))),R.s_("depthwiseConv2d",a,r),{x:()=>(0,kt.l)(l.shape,t,u,i,a,o,r),filter:()=>(0,bt.x)(l,t,u.shape,i,a,o,r)}}},vt={kernelName:a.jxD,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},o={x:s,filter:i,dy:t};return{x:()=>E.T2.runKernel(a.bP9,r,n),filter:()=>E.T2.runKernel(a.pk0,o,n)}}},It={kernelName:a.Pah,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>E.T2.runKernel(a.rsH,s)}}};var Nt=n(25462);const St={kernelName:a._s9,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=(0,o.l)((0,Nt.o)((0,h.H)((0,f.E)(n))),2/Math.sqrt(Math.PI));return{x:()=>(0,o.l)(t,s)}}},xt={kernelName:a.ox3,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,n)}}},Tt={kernelName:a.ybN,inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>(0,k.t)(t,n.shape)}}},zt={kernelName:a.ybj,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,Nt.o)(n))}}},At={kernelName:a.ZgB,gradFunc:t=>({x:()=>(0,N.P)(t)})},Ft={kernelName:a.ElG,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,b.assertAndGetBroadcastShape)(n.shape,s.shape);return{a:()=>{const e=(0,c.y)(t,(0,r.w)(s,"float32")),a=(0,b.getReductionAxes)(n.shape,i);return a.length>0?(0,k.t)((0,w.c)(e,a),n.shape):e},b:()=>{let e=(0,o.l)(t,(0,r.w)(n,"float32"));const a=(0,b.getReductionAxes)(s.shape,i);a.length>0&&(e=(0,k.t)((0,w.c)(e,a),s.shape));const l=(0,f.E)(s);return(0,h.H)((0,c.y)(e,(0,r.w)(l,"float32")))}}}};var Dt=n(26777),Ct=n(43017);const Et={kernelName:a.i5R,inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,a,r,l]=e,u=null==l?(0,p.d)(1):l,c=(0,b.getReductionAxes)(a.shape,i.shape),h=[];if(1===a.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const d=(0,m.j)(i,a),f=(0,o.l)(t,u),g=(0,Dt.Z)((0,z.W)(r,(0,p.d)(s))),y=(0,o.l)((0,o.l)((0,o.l)(g,g),g),(0,p.d)(-.5));return{x:()=>1===a.rank?(0,k.t)((0,o.l)((0,o.l)(t,(0,Ct.V)((0,k.t)(g,[1,1,1,a.shape[0]]),h)),u),i.shape):(0,k.t)((0,o.l)((0,o.l)(t,g),u),i.shape),mean:()=>{let t=(0,o.l)((0,o.l)(g,(0,p.d)(-1)),f);return 1===a.rank&&(t=(0,w.c)(t,c)),(0,k.t)(t,a.shape)},variance:()=>{let t=(0,o.l)((0,o.l)(y,d),f);return 1===a.rank&&(t=(0,w.c)(t,c)),(0,k.t)(t,a.shape)},scale:()=>{const e=(0,o.l)(d,g);let n=(0,o.l)(t,e);return 1===a.rank&&(n=(0,w.c)(n,c)),(0,k.t)(n,a.shape)},offset:()=>{let e=t;return 1===a.rank&&(e=(0,w.c)(e,c)),(0,k.t)(e,a.shape)}}}};var _t=n(901),Mt=n(65149);const Rt={kernelName:a.mxL,inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:a,batchDims:r}=n,o=(0,M.Y6)(a,s.shape)[0],l=(t,e,n)=>()=>{const s=t.shape,i=e.size,r=s.slice(0,o),l=r.length,u=s.slice(a,s.length).slice(1),c=u.length,h=Lt(0,l),p=Lt(l+1,l+1+c),d=Ot([r,[i],u]),f=(0,k.t)(n,d),m=(0,k.t)(e,[i]),g=Ot([[l],h,p]),y=(0,gt.m)(f,g);let b=(0,Mt.z)(y,m,t.shape[o]);const w=(0,ft.gx)(g);return b=(0,gt.m)(b,w),b};if(1===r){const e=s.shape[0],n=s.split(e,0);return{x:()=>{const e=(0,_t.t)(n.map(((e,n)=>l(e,i.slice(n,1),t.slice(n,1))())));return e.reshape(s.shape)},indices:()=>i}}return{x:l(s,i,t),indices:()=>i}}};function Lt(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Ot(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const Wt={kernelName:a.lLS,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>(0,N.P)(n),b:()=>(0,N.P)(s)}}},Bt={kernelName:a.lzr,gradFunc:t=>({x:()=>(0,r.w)(t,"float32")})},Pt={kernelName:a.gIW,gradFunc:t=>({x:()=>(0,N.P)(t)})},Ut={kernelName:a.E3$,gradFunc:t=>({x:()=>(0,N.P)(t)})},Ht={kernelName:a.iPs,gradFunc:t=>({x:()=>(0,N.P)(t)})};var Vt=n(22759);const jt={kernelName:a.X0$,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,a=(0,Vt.r)(s,0);return{x:()=>(0,X._)(a,t,(0,o.l)(t,i))}}},Gt={kernelName:a.Cg$,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,z.W)(n,1))}}},qt={kernelName:a.tG8,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,r.w)(n,"float32"))}}},Zt={kernelName:a.zfU,inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=(0,Nt.o)(s);return(0,m.j)(t,(0,o.l)((0,w.c)(t,i,!0),e))}}}};const Kt=(0,L.op)({localResponseNormalizationBackprop_:function(t,e,n){const s={x:t,y:e,dy:n},i={depthRadius:arguments.length>3&&void 0!==arguments[3]?arguments[3]:5,bias:arguments.length>4&&void 0!==arguments[4]?arguments[4]:1,alpha:arguments.length>5&&void 0!==arguments[5]?arguments[5]:1,beta:arguments.length>6&&void 0!==arguments[6]?arguments[6]:.5};return E.T2.runKernel(a.ToN,s,i)}}),Jt={kernelName:a.jM4,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:a,bias:r,alpha:o,beta:l}=n;return{x:()=>Kt(s,i,t,a,r,o,l)}}};var Yt=n(37457);function Xt(t,e,n,s){return e.rank<n.rank&&(e=(0,k.t)(e,ft.SM(e.shape,s))),t.rank<n.rank&&(t=(0,k.t)(t,ft.SM(t.shape,s))),{x:()=>(0,o.l)(t,(0,r.w)((0,Yt.L)(n,e),t.dtype))}}const Qt={kernelName:a.VAI,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,a=e[0],r=Xt(t,e[1],a,M.Y6(i,a.shape));return{x:()=>r.x()}}};var $t=n(58320);const te={kernelName:a.LDN,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>(0,o.l)(t,(0,r.w)((0,K.D)(n,s),"float32")),b:()=>(0,o.l)(t,(0,r.w)((0,$t.M)(n,s),"float32"))}}};const ee=(0,L.op)({maxPool3dGrad_:function(t,e,n,s,i,r,o){const l=(0,_.YT)(t,"dy","maxPool3dGrad"),u=(0,_.YT)(e,"input","maxPool3dGrad"),c=(0,_.YT)(n,"output","maxPool3dGrad");let h=l,p=u,d=c,f=!1;4===u.rank&&(f=!0,h=(0,k.t)(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=(0,k.t)(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]]),d=(0,k.t)(c,[1,c.shape[0],c.shape[1],c.shape[2],c.shape[3]])),M.vA(5===h.rank,(()=>"Error in maxPool3dGrad: dy must be rank 5 but got rank "+"".concat(h.rank,"."))),M.vA(5===p.rank,(()=>"Error in maxPool3dGrad: input must be rank 5 but got rank "+"".concat(p.rank,"."))),M.vA(5===d.rank,(()=>"Error in maxPool3dGrad: output must be rank 5 but got rank "+"".concat(d.rank,"."))),(0,R.s_)("maxPool3dGrad",r,o);const m={dy:h,input:p,output:d},g={filterSize:s,strides:i,pad:r,dimRoundingMode:o},y=E.T2.runKernel(a.cHb,m,g);return f?(0,k.t)(y,[y.shape[1],y.shape[2],y.shape[3],y.shape[4]]):y}}),ne={kernelName:a.ySp,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:a,strides:r,pad:o,dimRoundingMode:l}=n;return{x:()=>ee(t,s,i,a,r,o,l)}}};const se=(0,L.op)({maxPoolGrad_:function(t,e,n,s,i,r,o){const l=(0,_.YT)(t,"dy","maxPoolGrad"),u=(0,_.YT)(e,"input","maxPoolGrad"),c=(0,_.YT)(n,"output","maxPoolGrad");M.vA(u.rank===l.rank,(()=>"Rank of input (".concat(u.rank,") does not match rank of dy ")+"(".concat(l.rank,")"))),M.vA(4===l.rank,(()=>"Error in maxPoolGrad: dy must be rank 4 but got rank "+"".concat(l.rank,"."))),M.vA(4===u.rank,(()=>"Error in maxPoolGrad: input must be rank 4 but got rank "+"".concat(u.rank,"."))),R.s_("maxPoolGrad",r,o);const h={dy:l,input:u,output:c},p={filterSize:s,strides:i,pad:r,dimRoundingMode:o};return E.T2.runKernel(a.RXX,h,p)}}),ie={kernelName:a.t3d,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:a,strides:r,pad:o}=n;return{x:()=>se(t,s,i,a,r,o)}}};var ae=n(44616);const re={kernelName:a.g5A,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,a=M.Y6(i,s.shape),r=(0,ft.lb)(s.shape,a)[1],l=M.Ze(r);return{x:()=>{const e=s.shape.slice();a.forEach((t=>{e[t]=1}));const n=(0,k.t)(t,e);return(0,c.y)((0,o.l)(n,(0,ae.S)(s.shape,"float32")),l)}}}},oe={kernelName:a.lNG,inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[a,r]=e,o=Xt(t,r,a,M.Y6(i,a.shape));return{x:()=>o.x()}}},le={kernelName:a.LG0,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>(0,o.l)(t,(0,r.w)((0,J.I)(n,s),"float32")),b:()=>(0,o.l)(t,(0,r.w)((0,Vt.r)(n,s),"float32"))}}};var ue=n(10463);const ce={kernelName:a.x7F,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,a=i.map((t=>t[0]));return{x:()=>(0,ue.d)(t,a,s.shape)}}};var he=n(48587);const pe={kernelName:a.BLA,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,b.assertAndGetBroadcastShape)(n.shape,s.shape);return{a:()=>{const e=(0,b.getReductionAxes)(n.shape,i);return e.length>0?(0,k.t)((0,w.c)(t,e),n.shape):t},b:()=>{const e=(0,o.l)(t,(0,h.H)((0,he.R)((0,c.y)(n,s)))),a=(0,b.getReductionAxes)(s.shape,i);return a.length>0?(0,k.t)((0,w.c)(e,a),s.shape):e}}}},de={kernelName:a.xu7,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,b.assertAndGetBroadcastShape)(n.shape,s.shape);return{a:()=>{const e=(0,o.l)(t,(0,r.w)(s,"float32")),a=(0,b.getReductionAxes)(n.shape,i);return a.length>0?(0,k.t)((0,w.c)(e,a),n.shape):e},b:()=>{const e=(0,o.l)(t,(0,r.w)(n,"float32")),a=(0,b.getReductionAxes)(s.shape,i);return a.length>0?(0,k.t)((0,w.c)(e,a),s.shape):e}}}},fe={kernelName:a.l0G,gradFunc:t=>({x:()=>(0,h.H)(t)})};var me=n(74218);const ge={kernelName:a.urI,inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>(0,me.U)(n.shape,"float32")}}},ye={kernelName:a.LWX,gradFunc:t=>({x:()=>(0,N.P)(t)})};var be=n(3984);const ke={kernelName:a.mM$,saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return(0,be.K)(t,s).map((t=>()=>t))}},we={kernelName:a.ODT,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,a=i.map((t=>t[0]));return{x:()=>(0,ue.d)(t,a,s.shape)}}};var ve=n(75911),Ie=n(77587);const Ne={kernelName:a.pyJ,inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,a=n,l=s,u=b.assertAndGetBroadcastShape(a.shape,l.shape);return{a:()=>{const e=(0,r.w)(l,"float32");let n=(0,o.l)(t,(0,o.l)(e,(0,Ie.n)(a,(0,m.j)(e,(0,p.d)(1)))));const s=b.getReductionAxes(a.shape,u);return s.length>0&&(n=(0,w.c)(n,s)),(0,k.t)(n,a.shape)},b:()=>{const e=(0,Vt.r)(a,0),n=(0,X._)(e,(0,ve.R)(a),(0,N.P)(a));let s=(0,o.l)(t,(0,o.l)(i,n));const r=b.getReductionAxes(l.shape,u);return r.length>0&&(s=(0,w.c)(s,r)),(0,k.t)(s,l.shape)}}}},Se={kernelName:a.Ncv,inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,Vt.r)(n,0);return{x:()=>(0,X._)(i,t,(0,o.l)(t,s)),alpha:()=>{let e=(0,X._)(i,(0,N.P)(t),(0,o.l)(t,n));const a=(0,b.getReductionAxes)(s.shape,t.shape);return a.length>0&&(e=(0,w.c)(e,a)),(0,k.t)(e,s.shape)}}}};var xe=n(63739);function Te(t,e,n){const s=t.shape.length,i=s-n.length,a=ft.Em(n,s);let r=t;null!=a&&(r=(0,gt.m)(t,a));const l=r.shape.slice(),u=l.splice(s-n.length,n.length).reduce(((t,e)=>t*e),1);l.push(u);let c=function(t,e,n){const s=t.shape.slice();s[n]=1;const i=(0,k.t)(e,s),a=(0,xe.L)(t,n,!0,!1),r=(0,xe.L)(t,n,!0,!0),l=(0,o.l)(a,r);return(0,o.l)(i,l)}(r.reshape(l),e,i);if(c=c.reshape(r.shape),null!=a){const t=ft.gx(a);c=(0,gt.m)(c,t)}return c}const ze={kernelName:a.kdj,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;let a=[];return a=void 0===i||null===i?s.shape.map(((t,e)=>e)):"number"===typeof i?[i]:i,{x:()=>Te(s,t,a)}}},Ae={kernelName:a.sDr,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=b.assertAndGetBroadcastShape(n.shape,s.shape);return{a:()=>{const e=(0,c.y)(t,(0,r.w)(s,"float32")),a=b.getReductionAxes(n.shape,i);return a.length>0?(0,k.t)((0,w.c)(e,a),n.shape):e},b:()=>{let e=(0,o.l)(t,(0,r.w)(n,"float32"));const a=b.getReductionAxes(s.shape,i);a.length>0&&(e=(0,k.t)((0,w.c)(e,a),s.shape));const l=(0,f.E)(s);return(0,h.H)((0,c.y)(e,(0,r.w)(l,"float32")))}}}},Fe={kernelName:a.huO,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,h.H)((0,f.E)(n)))}}},De={kernelName:a.P_L,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=(0,o.l)((0,J.I)(n,6),(0,l.P)(n));return{x:()=>(0,o.l)(t,(0,r.w)(s,"float32"))}}},Ce={kernelName:a.fUj,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,r.w)((0,l.P)(n),"float32"))}}},Ee={kernelName:a.R23,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,k.t)(t,n.shape)}}},_e={kernelName:a.hgw,inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>E.T2.runKernel(a.FCQ,i,n)}}},Me={kernelName:a.jOE,inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>E.T2.runKernel(a.XQy,i,n)}}};var Re=n(47419);const Le={kernelName:a.D7i,gradFunc:(t,e,n)=>{const{dims:s}=n,i=(0,M.Y6)(s,t.shape);return{x:()=>(0,Re.B)(t,i)}}},Oe={kernelName:a.hVg,gradFunc:t=>({x:()=>(0,N.P)(t)})},We={kernelName:a.TOR,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,h.H)((0,c.y)(t,(0,o.l)((0,Ie.n)(n,1.5),2)))}}};var Be=n(15344);const Pe={kernelName:a.l6P,inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>(0,r.w)((0,N.P)(n),"float32"),t:()=>(0,o.l)(t,(0,r.w)(n,t.dtype)),e:()=>(0,o.l)(t,(0,r.w)((0,Be.N)(n),t.dtype))}}};var Ue=n(3765);const He={kernelName:a.u$b,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=(0,Vt.r)(n,(0,p.d)(0)),s=(0,p.d)(Ue.j),i=(0,p.d)(Ue.X),a=(0,o.l)(t,i),l=(0,o.l)((0,o.l)(t,s),(0,Nt.o)((0,r.w)(n,"float32")));return(0,X._)(e,a,l)}}}},Ve={kernelName:a.vI1,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,o.l)(n,(0,m.j)((0,p.d)(1),n)))}}},je={kernelName:a.YVe,gradFunc:t=>({x:()=>(0,N.P)(t)})};var Ge=n(35894);const qe={kernelName:a.hql,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)((0,Ge.g)((0,r.w)(n,"float32")),t)}}};var Ze=n(71164);const Ke={kernelName:a.J3C,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)((0,Ze.y)((0,r.w)(n,"float32")),t)}}};var Je=n(85048),Ye=n(34642);const Xe={kernelName:a.JiE,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:a}=n,r=s.shape,[o,l]=(0,Ye.parseSliceParams)(s,i,a),u=[];for(let c=0;c<t.rank;c++)u.push([o[c],r[c]-o[c]-l[c]]);return{x:()=>(0,Je.e)(t,u)}}},Qe={kernelName:a.rFG,outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,a=(0,o.l)(t,s);return{logits:()=>(0,m.j)(a,(0,o.l)((0,w.c)(a,[i],true),s))}}};var $e=n(74213);const tn={kernelName:a.Fin,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,$e.r)(n))}}};var en=n(2557);const nn={kernelName:a.A8B,gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>(0,en.G)(t,s,i)}}};var sn=n(94429);const an={kernelName:a.Blb,gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>(0,sn.x)(t,s)}}},rn={kernelName:a.dFH,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,o.l)((0,d.R)((0,r.w)(n,"float32")),2))}}},on={kernelName:a.M6A,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)(t,(0,o.l)((0,r.w)(n,"float32"),2))}}},ln={kernelName:a.Ddj,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=(0,p.d)(2);return{a:()=>(0,o.l)(t,(0,o.l)(i,(0,m.j)(n,s))),b:()=>(0,o.l)(t,(0,o.l)(i,(0,m.j)(s,n)))}}},un={kernelName:a.pnw,gradFunc:t=>({x:()=>(0,N.P)(t)})},cn={kernelName:a.PbM,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=b.assertAndGetBroadcastShape(n.shape,s.shape);return{a:()=>{let e=t;const s=b.getReductionAxes(n.shape,i);return s.length>0&&(e=(0,w.c)(e,s)),(0,k.t)(e,n.shape)},b:()=>{let e=t;const n=b.getReductionAxes(s.shape,i);return n.length>0&&(e=(0,w.c)(e,n)),(0,k.t)((0,h.H)(e),s.shape)}}}},hn={kernelName:a.WuN,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:a}=n;(0,M.Y6)(a,s.shape).forEach((t=>{i[t]=1}));const r=(0,k.t)(t,i),l=(0,o.l)(r,(0,ae.S)(s.shape,"float32"));return{x:()=>l}}},pn={kernelName:a.oFs,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,c.y)(t,(0,f.E)((0,Ge.g)(n)))}}},dn={kernelName:a.iuW,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>(0,o.l)((0,m.j)((0,p.d)(1),(0,f.E)(n)),t)}}},fn={kernelName:a.FAs,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=(0,N.P)(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=(0,z.W)(e,(0,ue.d)(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let a=0;a<i[1];++a)e=(0,z.W)(e,(0,ue.d)(t,[n*s.shape[0],a*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let a=0;a<i[1];++a)for(let r=0;r<i[2];++r)e=(0,z.W)(e,(0,ue.d)(t,[n*s.shape[0],a*s.shape[1],r*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error("Gradient for tile operation is not implemented for rank-"+"".concat(s.rank," tensors yet."));for(let n=0;n<i[0];++n)for(let a=0;a<i[1];++a)for(let r=0;r<i[2];++r)for(let o=0;o<i[3];++o)e=(0,z.W)(e,(0,ue.d)(t,[n*s.shape[0],a*s.shape[1],r*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},mn={kernelName:a.wx0,gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,a=ft.gx(i);return{x:()=>(0,gt.m)(t,a)}}},gn={kernelName:a.dXR,gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>(0,_t.t)(t,i)}}};var yn=n(74023),bn=n(56178),kn=n(82131);const wn={kernelName:a.pPe,inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=(0,kn.P)(e,(0,N.P)(e)),s=(0,bn.k)(t,n);let i=(0,K.D)(e,(0,p.d)(0,"int32"));const a=s.rank-i.rank;for(let o=0;o<a;++o)i=(0,yn.U)(i,o+1);i=(0,Y.n)(i,(0,ae.S)(s.shape,"bool"));const r=(0,N.P)(s);return(0,X._)(i,s,r)}(t,n)}}};const vn={kernelName:a.xJ3,gradFunc:t=>({x:()=>(0,N.P)(t)})};var In=n(10843);const Nn=[u,g,y,v,I,S,x,T,A,F,D,C,W,P,H,j,G,q,Z,Q,$,et,rt,it,ut,ht,dt,yt,wt,vt,Ae,It,St,xt,Tt,zt,Ft,At,Et,Rt,Wt,Bt,Pt,Ut,Ht,jt,Gt,qt,Zt,Jt,Qt,Qt,te,ne,ie,re,oe,le,ce,pe,de,fe,ge,ye,ke,we,we,Ne,Se,ze,Fe,De,Ce,Ee,_e,Me,Le,Oe,We,Pe,He,Ve,je,qe,Ke,Xe,Qe,tn,nn,nn,an,an,rn,ln,on,un,cn,hn,pn,dn,fn,mn,gn,wn,vn];for(const Hm of Nn)(0,In.kr)(Hm);n(57513);class Sn extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Sn.prototype)}}class xn extends Error{constructor(t){super(t),Object.setPrototypeOf(this,xn.prototype)}}class Tn extends Error{constructor(t){super(t),Object.setPrototypeOf(this,Tn.prototype)}}class zn extends Error{constructor(t){super(t),Object.setPrototypeOf(this,zn.prototype)}}class An extends Error{constructor(t){super(t),Object.setPrototypeOf(this,An.prototype)}}Error;class Fn{constructor(t){this.maxEntries=t||100,this.cache=new Map}get(t){let e;return this.cache.has(t)&&(e=this.cache.get(t),this.cache.delete(t),this.cache.set(t,e)),e}put(t,e){if(this.cache.has(t))this.cache.delete(t);else if(this.cache.size>=this.maxEntries){const t=this.cache.keys().next().value;this.cache.delete(t)}this.cache.set(t,e)}getMaxEntries(){return this.maxEntries}setMaxEntries(t){if(t<0)throw new Error("The maxEntries of LRU caches must be at least 0, but got ".concat(t,"."));if(this.maxEntries>t)for(let e=0;e<this.maxEntries-t;e++){const t=this.cache.keys().next().value;this.cache.delete(t)}this.maxEntries=t}}function Dn(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function Cn(t,e){if(!t)throw new An(e)}function En(t,e){let n=0;for(const s of t)s===e&&n++;return n}function _n(t){return 1===t.length?t[0]:t}function Mn(t){return Array.isArray(t)?t:[t]}function Rn(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function Ln(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let On={};function Wn(t){if(null===t||void 0===t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function Bn(t){if(null!=t&&"object"===typeof t)if(Array.isArray(t))t.forEach((t=>Bn(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"===typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!==typeof e.value?Bn(e):t[n]=e.value)}}}function Pn(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"object",i=arguments.length>4&&void 0!==arguments[4]&&arguments[4];if("string"===typeof t){const i=t;let a;if(i in n)a=n[i];else if(i in On)a=On[i];else if(a=e[i],null==a)throw new Tn("Unknown ".concat(s,": ").concat(t,". ")+"This may be due to one of the following reasons:\n"+"1. The ".concat(s," is defined in Python, in which ")+"case it needs to be ported to TensorFlow.js or your JavaScript code.\n"+"2. The custom ".concat(s," is defined in JavaScript, ")+"but is not registered properly with tf.serialization.registerClass().");return a}{const a=t;if(null==a.className||null==a.config)throw new Tn("".concat(s,": Improper config format: ")+"".concat(JSON.stringify(a),".\n")+"'className' and 'config' must set.");const r=a.className;let o,l;if(r in n?[o,l]=n[r]:r in On?[o,l]=On.className:r in e&&([o,l]=e[r]),null==o)throw new Tn("Unknown ".concat(s,": ").concat(r,". ")+"This may be due to one of the following reasons:\n"+"1. The ".concat(s," is defined in Python, in which ")+"case it needs to be ported to TensorFlow.js or your JavaScript code.\n"+"2. The custom ".concat(s," is defined in JavaScript, ")+"but is not registered properly with tf.serialization.registerClass().");if(null!=l){const t={};for(const n of Object.keys(On))t[n]=On[n];for(const i of Object.keys(n))t[i]=n[i];a.config.customObjects=t;const e=Object.assign({},On);for(const i of Object.keys(n))On[i]=n[i];Bn(a.config);const s=l(o,a.config,n,i);return On=Object.assign({},e),s}{const t=Object.assign({},On);for(const s of Object.keys(n))On[s]=n[s];const e=new o(a.config);return On=Object.assign({},t),e}}}function Un(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function Hn(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function Vn(t){if(null==t)throw new Tn("Invalid value in obj: ".concat(JSON.stringify(t)));for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function jn(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new Tn("".concat(n," is not a valid ").concat(e,".  Valid values are ").concat(t," or null/undefined."))}function Gn(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:0,s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1/0;return Cn(n>=0),Cn(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function qn(t,e){Array.isArray(t)?(i.util.assert(t.length>0,(()=>"".concat(e," is unexpectedly an empty array."))),t.forEach(((t,n)=>qn(t,"element ".concat(n+1," of ").concat(e))))):i.util.assert(Number.isInteger(t)&&t>0,(()=>"Expected ".concat(e," to be a positive integer, but got ")+"".concat(Zn(t),".")))}function Zn(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>Zn(t))).join(",")+"]":"string"===typeof t?'"'.concat(t,'"'):"".concat(t)}function Kn(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}let Jn=0;function Yn(){return Jn++}const Xn={};function Qn(){let t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return t in Xn||(Xn[t]=0),Xn[t]+=1,t+Xn[t].toString()}const $n=["channelsFirst","channelsLast"],ts=["nearest","bilinear"],es=["valid","same","causal"],ns=["max","avg"],ss=["sum","mul","concat","ave"],is=new Map;function as(t){jn($n,"DataFormat",t)}function rs(t){jn(es,"PaddingMode",t)}function os(t){jn(ns,"PoolMode",t)}const ls=[];function us(t,e){ls.push(t);try{const t=e();return ls.pop(),t}catch(n){throw ls.pop(),n}}function cs(t){if(!ds(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===ls.length?"":ls.join("/")+"/")+t}function hs(t){if(!ds(t))throw new Error("Not a valid tensor name: '"+t+"'");is.has(t)||is.set(t,0);const e=is.get(t);if(is.set(t,is.get(t)+1),e>0){const n="".concat(t,"_").concat(e);return is.set(n,1),n}return t}const ps=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function ds(t){return!!t.match(ps)}function fs(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function ms(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s<e&&(e=s)}return e}function gs(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s>e&&(e=s)}return e}function ys(t,e){if(e<t)throw new Tn("end (".concat(e,") < begin (").concat(t,") is forbidden."));const n=[];for(let s=t;s<e;++s)n.push(s);return n}let bs;function ks(){return null==bs&&(bs=(0,i.backend)().epsilon()),bs}function ws(t,e){return i.cast(t,e)}function vs(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:-1;const n=t.shape.slice();return e<0&&(e=n.length+e+1),n.splice(e,0,1),i.reshape(t,n)}function Is(t,e,n){return(0,i.tidy)((()=>{switch(t.rank){case 1:return i.slice1d(t,e,n);case 2:return i.slice2d(t,[e,0],[n,t.shape[1]]);case 3:return i.slice3d(t,[e,0,0],[n,t.shape[1],t.shape[2]]);case 4:return i.slice4d(t,[e,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3]]);case 5:return i.slice(t,[e,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return i.slice(t,[e,0,0,0,0,0],[n,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new Tn("sliceAlongFirstAxis() received an unsupported tensor rank: "+"".concat(t.rank))}}))}function Ns(t,e,n){return(0,i.tidy)((()=>{switch(t.rank){case 1:return i.slice1d(t,e,n);case 2:return i.slice2d(t,[0,e],[t.shape[0],n]);case 3:return i.slice3d(t,[0,0,e],[t.shape[0],t.shape[1],n]);case 4:return i.slice4d(t,[0,0,0,e],[t.shape[0],t.shape[1],t.shape[2],n]);default:throw new Tn("sliceAlongLastAxis() received an unsupported tensor rank: "+"".concat(t.rank))}}))}function Ss(t,e,n,s){return(0,i.tidy)((()=>{switch(t.rank){case 1:return i.slice1d(t,e,n);case 2:switch(s){case 1:return Is(t,e,n);case 2:return Ns(t,e,n);default:throw new Tn("The axis is not within the rank of the tensor "+"".concat(s))}case 3:switch(s){case 1:return Is(t,e,n);case 2:return i.slice3d(t,[0,e,0],[t.shape[0],n,t.shape[2]]);case 3:return Ns(t,e,n);default:throw new Tn("The axis is not within the rank of the tensor "+"".concat(s))}case 4:switch(s){case 1:return Is(t,e,n);case 2:return i.slice4d(t,[0,e,0,0],[t.shape[0],n,t.shape[2],t.shape[3]]);case 3:return i.slice4d(t,[0,0,e,0],[t.shape[0],t.shape[1],n,t.shape[3]]);case 4:return Ns(t,e,n);default:throw new Tn("The axis is not within the rank of the tensor "+"".concat(s))}default:throw new Tn("sliceAlongLastAxis() received an unsupported tensor rank: "+"".concat(t.rank))}}))}function xs(t){let e,n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:-1;return n<0&&(e=t[0].rank,n=0!==e?e:0),n===t[0].rank&&(n=-1),i.concat(t,n)}function Ts(t,e){switch(t.rank){case 1:return i.concat1d([t,e]);case 2:return i.concat2d([t,e],0);case 3:return i.concat3d([t,e],0);case 4:return i.concat4d([t,e],0);default:throw new Tn("concatAlongFirstAxis() received an unsupported "+"tensor rank: ".concat(t.rank))}}function zs(t,e){if(Array.isArray(e)||(e=[e]),t.rank!==e.length)throw new Tn("The length of input n (".concat(e.length,") does not match ")+"the number of dimensions in input x (".concat(t.rank,")"));return i.tile(t,e)}function As(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:0,n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:1,s=arguments.length>3?arguments[3]:void 0,a=arguments.length>4?arguments[4]:void 0;return i.randomNormal(t,e,n,s,a)}function Fs(t,e,n,s){if(t.rank<2||e.rank<2)throw new zn("dot requires both inputs to be rank >= 2"+" but got x shape = ".concat(t.shape," and y shape = ").concat(e.shape));if(e.rank>=3){if(t.shape.slice(-1)[0]!==e.shape.slice(-2)[0])throw new zn("If rank y >= 3, then the second last dim"+" of y must equal the last dim of x but got x shape = ".concat(t.shape," and ")+" y shape = ".concat(e.shape))}if(2===t.rank&&2===e.rank){const a=!1,r=!1;return i.fused.matMul({a:t,b:e,transposeA:a,transposeB:r,bias:s?Es(t.rank,s,"channelsLast"):null,activation:n})}{const a=t.shape.slice(),r=a.pop();t=i.reshape(t,[-1,r]);const o=e.shape.slice(),l=o.pop(),u=o.pop(),c=[...o,l],h=Array.from({length:e.rank},((t,n)=>0===n?e.rank-2:n<=e.rank-2?n-1:n));e=i.reshape(i.transpose(e,h),[u,-1]);const p=[...a,...c],d=!1,f=!1;return i.reshape(i.fused.matMul({a:t,b:e,transposeA:d,transposeB:f,bias:s?Es(t.rank,s,"channelsLast"):null,activation:n}),p)}}function Ds(t,e,n){return(0,i.tidy)((()=>(e=Array.isArray(e)?(0,i.tensor1d)(e,"int32"):i.cast(e,"int32"),i.gather(t,e,n))))}function Cs(t){return i.mul(t,t)}function Es(t,e,n){const s=e.shape;if(1!==e.rank&&e.rank!==t)throw new Tn("Unexpected bias dimensions: ".concat(e.rank)+"; expected it to be 1 or ".concat(t));if(5===t){if("channelsFirst"===n)return 1===s.length?i.reshape(e,[1,s[0],1,1,1]):i.reshape(e,[1,s[3],s[0],s[1],s[2]]);if("channelsLast"===n)return 1===s.length?i.reshape(e,[1,1,1,1,s[0]]):i.reshape(e,[1].concat(s))}else if(4===t){if("channelsFirst"===n)return 1===s.length?i.reshape(e,[1,s[0],1,1]):i.reshape(e,[1,s[2],s[0],s[1]]);if("channelsLast"===n)return 1===s.length?i.reshape(e,[1,1,1,s[0]]):i.reshape(e,[1].concat(s))}else if(3===t){if("channelsFirst"===n)return 1===s.length?i.reshape(e,[1,s[0],1]):i.reshape(e,[1,s[1],s[0]]);if("channelsLast"===n)return 1===s.length?i.reshape(e,[1,1,s[0]]):i.reshape(e,[1].concat(s))}else if(t<3)return e;throw new Tn("Unsupported input rank by biasAdd: ".concat(e.rank))}function _s(t,e,n){return(0,i.tidy)((()=>(null==n&&(n="channelsLast"),as(n),i.add(t,Es(t.rank,e,n)))))}function Ms(t,e,n,s){return(0,i.tidy)((()=>i.dropout(t,e,n,s)))}function Rs(t,e){return arguments.length>2&&void 0!==arguments[2]&&arguments[2]?t():e()}const Ls=["fanIn","fanOut","fanAvg"],Os=["normal","uniform","truncatedNormal"];class Ws extends i.serialization.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class Bs extends Ws{apply(t,e){return(0,i.zeros)(t,e)}}Bs.className="Zeros",i.serialization.registerClass(Bs);class Ps extends Ws{apply(t,e){return(0,i.ones)(t,e)}}Ps.className="Ones",i.serialization.registerClass(Ps);class Us extends Ws{constructor(t){if(super(),"object"!==typeof t)throw new Tn("Expected argument of type ConstantConfig but got ".concat(t));if(void 0===t.value)throw new Tn("config must have value set but got ".concat(t));this.value=t.value}apply(t,e){return(0,i.tidy)((()=>(0,i.mul)((0,i.scalar)(this.value),(0,i.ones)(t,e))))}getConfig(){return{value:this.value}}}Us.className="Constant",i.serialization.registerClass(Us);class Hs extends Ws{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return(0,i.randomUniform)(t,this.minval,this.maxval,e,this.seed)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}Hs.className="RandomUniform",i.serialization.registerClass(Hs);class Vs extends Ws{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new zn("randomNormal does not support dType ".concat(e,"."));return As(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Vs.className="RandomNormal",i.serialization.registerClass(Vs);class js extends Ws{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new zn("truncatedNormal does not support dType ".concat(e,"."));return(0,i.truncatedNormal)(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}js.className="TruncatedNormal",i.serialization.registerClass(js);class Gs extends Ws{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return(0,i.tidy)((()=>{if(2!==t.length||t[0]!==t[1])throw new Tn("Identity matrix initializer can only be used for 2D square matrices.");return(0,i.mul)(this.gain,(0,i.eye)(t[0]))}))}getConfig(){return{gain:this.gain}}}Gs.className="Identity",i.serialization.registerClass(Gs);class qs extends Ws{constructor(t){if(super(),t.scale<0)throw new Tn("scale must be a positive float. Got: ".concat(t.scale));var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,jn(Ls,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){jn(Os,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const n=function(t){let e,n,s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"channelsLast";if(as(s),2===t.length)e=t[0],n=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===s){const s=fs(t,2);e=t[1]*s,n=t[0]*s}else if("channelsLast"===s){const s=fs(t,0,t.length-2);e=t[t.length-2]*s,n=t[t.length-1]*s}}else{const s=fs(t);e=Math.sqrt(s),n=Math.sqrt(s)}return[e,n]}(t),s=n[0],a=n[1];let r=this.scale;if("fanIn"===this.mode?r/=Math.max(1,s):"fanOut"===this.mode?r/=Math.max(1,a):r/=Math.max(1,(s+a)/2),"normal"===this.distribution){const n=Math.sqrt(r);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new zn("".concat(this.getClassName()," does not support dType ").concat(e,"."));return(0,i.truncatedNormal)(t,0,n,e,this.seed)}{const n=Math.sqrt(3*r);return(0,i.randomUniform)(t,-n,n,e,this.seed)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}qs.className="VarianceScaling",i.serialization.registerClass(qs);class Zs extends qs{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Zs.className="GlorotUniform",i.serialization.registerClass(Zs);class Ks extends qs{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Ks.className="GlorotNormal",i.serialization.registerClass(Ks);class Js extends qs{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Js.className="HeNormal",i.serialization.registerClass(Js);class Ys extends qs{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Ys.className="HeUniform",i.serialization.registerClass(Ys);class Xs extends qs{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Xs.className="LeCunNormal",i.serialization.registerClass(Xs);class Qs extends qs{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return qs.className}}Qs.className="LeCunUniform",i.serialization.registerClass(Qs);class $s extends Ws{constructor(t){super(),this.DEFAULT_GAIN=1,this.ELEMENTS_WARN_SLOW=2e3,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed}apply(t,e){return(0,i.tidy)((()=>{if(t.length<2)throw new zn("Shape must be at least 2D.");if("int32"!==e&&"float32"!==e&&void 0!==e)throw new TypeError("Unsupported data type ".concat(e,"."));const n=i.util.sizeFromShape(t.slice(0,-1)),s=t[t.length-1],a=n*s;a>this.ELEMENTS_WARN_SLOW&&console.warn("Orthogonal initializer is being called on a matrix with more "+"than ".concat(this.ELEMENTS_WARN_SLOW," (").concat(a,") elements: ")+"Slowness may result.");const r=As([Math.max(s,n),Math.min(s,n)],0,1,e,this.seed),o=i.linalg.qr(r,!1);let l=o[0];const u=o[1].flatten().stridedSlice([0],[Math.min(s,n)*Math.min(s,n)],[Math.min(s,n)+1]);return l=(0,i.mul)(l,u.sign()),n<s&&(l=l.transpose()),(0,i.mul)((0,i.scalar)(this.gain),l.reshape(t))}))}getConfig(){return{gain:this.gain,seed:this.seed}}}$s.className="Orthogonal",i.serialization.registerClass($s);const ti={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function ei(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return Pn(t,i.serialization.SerializationMap.getMap().classNameMap,e,"initializer")}function ni(t){return Wn(t)}function si(t){if("string"===typeof t){const e=t in ti?ti[t]:t;if("GlorotNormal"===e)return new Ks;if("GlorotUniform"===e)return new Zs;if("HeNormal"===e)return new Js;if("HeUniform"===e)return new Ys;if("LeCunNormal"===e)return new Xs;if("LeCunUniform"===e)return new Qs;{const t={};return t.className=e,t.config={},ei(t)}}return t instanceof Ws?t:ei(t)}function ii(t){return Array.isArray(t)&&Array.isArray(t[0])}function ai(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function ri(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new Tn("Expected Tensor length to be 1; got ".concat(t.length));e=t[0]}else e=t;return e}function oi(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return t[0];throw new Tn("Expected exactly 1 Shape; got ".concat(t.length))}return t}function li(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}const ui="Variable";class ci{constructor(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"float32",n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:ui,s=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:null;this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=Yn(),n=null==n?ui:n,this.originalName=cs(n),this.name=hs(this.originalName),this.trainable_=s,this.constraint=a,this.val=i.variable(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error("LayersVariable ".concat(this.name," is already disposed."))}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function hi(t){return t.map((t=>t.read()))}function pi(t){t.forEach((t=>{t[0].write(t[1])}))}class di{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class fi{constructor(t,e,n,s,i,a,r){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=r,this.id=Yn(),null!=a&&(this.originalName=cs(a),this.name=hs(this.originalName)),this.rank=e.length}}let mi=0;class gi{constructor(t,e){this.callArgs=e,this.id=mi++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const n of t.inboundLayers)null!=n&&n.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let yi=0;class bi extends i.serialization.Serializable{constructor(){let t=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=yi++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=Rn(t)+"_"+Qn(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new xn("The layer has never been called "+"and thus has no defined ".concat(e,"."));if(this.inboundNodes.length<=t)throw new Tn("Asked to get ".concat(e," at node ").concat(t,", ")+"but the layer has only ".concat(this.inboundNodes.length," inbound nodes."));return this.inboundNodes[t]}getInputAt(t){return _n(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return _n(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new Sn("Layer ".concat(this.name)+' has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use `getInputAt(nodeIndex)` instead.');if(0===this.inboundNodes.length)throw new Sn("Layer ".concat(this.name)+" is not connected, no input to return.");return _n(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new Sn("Layer ".concat(this.name)+" has no inbound nodes.");if(this.inboundNodes.length>1)throw new Sn("Layer ".concat(this.name)+' has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use `getOutputAt(nodeIndex)` instead.');return _n(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){const e=Mn(t);if(null==this.inputSpec||0===this.inputSpec.length)return;const n=Mn(this.inputSpec);if(e.length!==n.length)throw new Tn("Layer ".concat(this.name," expects ").concat(n.length," inputs, ")+"but it received ".concat(e.length," input tensors. ")+"Input received: ".concat(t));for(let s=0;s<e.length;s++){const t=e[s],i=n[s];if(null==i)continue;const a=t.rank;if(null!=i.ndim&&a!==i.ndim)throw new Tn("Input ".concat(s," is incompatible with layer ").concat(this.name,": ")+"expected ndim=".concat(i.ndim,", found ndim=").concat(a));if(null!=i.maxNDim&&a>i.maxNDim)throw new Tn("Input ".concat(s," is incompatible with layer ").concat(this.name)+": expected max_ndim=".concat(i.maxNDim,", found ndim=").concat(a));if(null!=i.minNDim&&a<i.minNDim)throw new Tn("Input ".concat(s," is incompatible with layer ").concat(this.name)+": expected min_ndim=".concat(i.minNDim,", found ndim=").concat(a,"."));if(null!=i.dtype&&t.dtype!==i.dtype)throw new Tn("Input ".concat(s," is incompatible with layer ").concat(this.name," ")+": expected dtype=".concat(i.dtype,", found dtype=").concat(t.dtype,"."));if(i.axes){const e=t.shape;for(const t in i.axes){const n=Number(t),a=i.axes[t],r=n>=0?e[n]:e[e.length+n];if(null!=a&&-1===[a,null].indexOf(r))throw new Tn("Input ".concat(s," is incompatible with layer ")+"".concat(this.name,": expected axis ").concat(n," of input shape to ")+"have value ".concat(a," but got shape ").concat(e,"."))}}if(null!=i.shape)for(let e=0;e<i.shape.length;++e){const n=i.shape[e],a=t.shape[e];if(null!=n&&null!=a&&n!==a)throw new Tn("Input ".concat(s," is incompatible with layer ")+"".concat(this.name,": expected shape=").concat(i.shape,", ")+"found shape=".concat(t.shape,"."))}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=Mn(t),s=function(t){let e=!0;for(const n of Mn(t))if(!(n instanceof fi)){e=!1;break}return e}(t),i=function(t){let e=!0;for(const n of Mn(t))if(n instanceof fi){e=!1;break}return e}(t);if(s===i)throw new Tn("Arguments to apply() must be all SymbolicTensors or all Tensors");return us(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of Mn(t))e.push(n.shape);this.build(_n(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);this.supportsMasking&&this.setMaskMetadata(t,s);const i=Mn(s),a=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),a.push(t);if(s=_n(a),null!=this.activityRegularizer)throw new zn("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=Mn(t);const e=[];for(const n of t)e.push(n.shape);return _n(e)}(t),s=this.computeOutputShape(n);let i;const a="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new fi(a,n,this,Mn(t),e,this.name,s))):new fi(a,s,this,Mn(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new zn("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn("The rank of the input tensor provided (shape: "+"".concat(JSON.stringify(t),") does not match that of the ")+"batchInputShape (".concat(JSON.stringify(this.batchInputShape),") ")+"of the layer ".concat(this.name));else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn("The shape of the input tensor "+"(".concat(JSON.stringify(t),") does not ")+"match the expectation of layer ".concat(this.name,": ")+"".concat(JSON.stringify(this.batchInputShape)))}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new Sn("The layer ".concat(this.name," has never been called and thus has no ")+"defined output shape.");const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new Sn("The layer ".concat(this.name," has multiple inbound nodes with different ")+'output shapes. Hence the notion of "output shape" is ill-defined for the layer.')}countParams(){if(!this.built)throw new xn("You tried to call countParams() on ".concat(this.name,", ")+"but the layer is not built yet. Build it first by calling build(batchInputShape).");return li(this.weights)}build(t){this.built=!0}getWeights(){return hi(arguments.length>0&&void 0!==arguments[0]&&arguments[0]?this.trainableWeights:this.weights)}setWeights(t){(0,i.tidy)((()=>{const e=this.weights;if(e.length!==t.length)throw new Tn('You called setWeights(weights) on layer "'.concat(this.name,'" ')+"with a weight list of length ".concat(t.length,", ")+"but the layer was expecting ".concat(e.length," weights. ")+"Provided weights: ".concat(t,"..."));if(0===e.length)return;const n=[],s=hi(e);for(let a=0;a<s.length;++a){const r=s[a],o=e[a],l=t[a];if(!i.util.arraysEqual(r.shape,l.shape))throw new Tn("Layer weight shape ".concat(r.shape," ")+"not compatible with provided weight shape ".concat(l.shape));n.push([o,l])}pi(n)}))}addWeight(t,e,n,s,i,a,r,o){if(-1!==this._addedWeightNames.indexOf(t))throw new Tn("Duplicate weight name ".concat(t," for layer ").concat(this.name));this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=null!=o?o():si("zeros"));const l=s.apply(e,n),u=new ci(l,n,t,a,r);return l.dispose(),null!=i&&this.addLoss((()=>i.apply(u.read()))),null==a&&(a=!0),a?this._trainableWeights.push(u):this._nonTrainableWeights.push(u),u}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=Mn(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError("Layer ".concat(this.name," does not support masking, ")+"but was passed an inputMask.");e.forEach((t=>{if(null!=t)throw new TypeError("Layer ".concat(this.name," does not support masking, ")+"but was passed an inputMask.")}))}return null}return e}setMaskMetadata(t,e,n){if(!this.supportsMasking)return;const s=this.computeMask(t,n),i=Mn(e),a=Mn(s);if(i.length!==a.length)throw new Error("".concat(this.name," outputs ").concat(i.length," tensors ")+"but ".concat(i.length," masks for those tensors"));for(let r=0;r<i.length;r++)i[r].kerasMask=a[r]}addInboundNode(t,e,n,s,i,a){let r=arguments.length>6&&void 0!==arguments[6]?arguments[6]:null;const o=Mn(t);e=Mn(e),n=Mn(n),s=Mn(s),i=ai(i),a=ai(a);const l=[],u=[],c=[];for(const h of o)l.push(h.sourceLayer),u.push(h.nodeIndex),c.push(h.tensorIndex);new gi({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:c,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:a},r);for(let h=0;h<e.length;h++)e[h].sourceLayer=this,e[h].nodeIndex=this.inboundNodes.length-1,e[h].tensorIndex=h}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error("Layer '".concat(this.name,"' is already disposed."))}dispose(){if(!this.built)throw new Error("Cannot dispose Layer ".concat(this.name," because it has not been ")+"built yet.");if(null===this._refCount)throw new Error("Cannot dispose Layer ".concat(this.name," because it has not been used ")+"yet.");this.assertNotDisposed();let t=0;return 0===--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function ki(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=ki(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class wi extends bi{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:Qn("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new Tn("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new Tn("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new Tn("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new fi(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new gi({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new Tn("Cannot pass any input to an "+"InputLayer's apply() method. InputLayer name: ".concat(this.name))}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function vi(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new Tn("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new wi({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}wi.className="InputLayer",i.serialization.registerClass(wi);class Ii{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof Ii)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,e,n){if(null!=this.id2Value[t.id])throw new Tn("Duplicate key: name=".concat(t.name,", id=").concat(t.id));return this.id2Value[t.id]=function(t,e){if(null==t.dtype||t.dtype===e.dtype)return e;try{return(0,i.cast)(e,t.dtype)}catch(n){throw new Tn("The dtype of the feed (".concat(e.dtype,") can not be cast to the dtype ")+"of the key '".concat(t.name,"' (").concat(t.dtype,")."))}}(t,e),this.name2Id[t.name]=t.id,null!=n&&(this.id2Mask[t.id]=n),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof fi){if(null==this.id2Value[t.id])throw new Tn("Nonexistent key: ".concat(t.name));return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new Tn("Feed dict has no SymbolicTensor name: ".concat(t));return this.id2Value[e]}}getMask(t){if(t instanceof fi){if(null==this.id2Value[t.id])throw new Tn("Nonexistent key: ".concat(t.name));return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new Tn("Feed dict has no SymbolicTensor name: ".concat(t));return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&(0,i.dispose)(this.id2Mask)}}const Ni=new Fn,Si=new Fn;function xi(t,e,n,s){const a=null!=n&&n.training,r=Array.isArray(t),o=r?t:[t],l=o.map((t=>t.name)),u=[],c=e.names();for(const i of l)-1!==c.indexOf(i)?u.push(e.getValue(i)):u.push(null);null!=s&&(s.maxNumTensors=-1/0,s.minNumTensors=1/0);const h=l.join(",")+"|"+e.names().sort().join(",");let p,d=Ni.get(h);if(null==d){const t=function(t,e){i.util.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let n=[],s={};if(1===t.length){const i=zi(t[0],e);n=i.sorted,s=i.recipientMap}else{const i=new Set;for(const a of t){const{sorted:t,recipientMap:r}=zi(a,e);for(const e of t)i.has(e.name)||(n.push(e),i.add(e.name));for(const e in r)null==s[e]&&(s[e]=new Set),r[e].forEach((t=>s[e].add(t)))}}return{sorted:n,recipientCounts:Ti(s)}}(o,e);d=t.sorted,p=t.recipientCounts,Ni.put(h,d),Si.put(h,p)}p={},a||Object.assign(p,Si.get(h));const f=new Ii(e);for(let m=0;m<d.length;++m){if(null!=s){const t=(0,i.memory)().numTensors;t>s.maxNumTensors&&(s.maxNumTensors=t),t<s.minNumTensors&&(s.minNumTensors=t)}const t=d[m],r=t.sourceLayer;if(r instanceof wi)continue;const o=[],c=[],h=[];let g=!1;for(const n of t.inputs){const t=f.getValue(n),s=f.getMask(n);o.push(t),c.push(s),null!=s&&(g=!0),a||(p[n.name]--,0!==p[n.name]||e.hasKey(n)||-1!==l.indexOf(n.name)||t.isDisposed||!0===n.sourceLayer.stateful||h.push(t))}g&&((n=n||{}).mask=c[0]);const y=Mn(r.apply(o,n));let b=null;r.supportsMasking&&(b=r.computeMask(o,c));const k=Ai(t),w=Array.isArray(k)?k:[k];for(let e=0;e<w.length;++e){f.hasKey(w[e])||f.add(w[e],y[e],Array.isArray(b)?b[0]:b);const t=l.indexOf(w[e].name);-1!==t&&(u[t]=y[e])}a||(0,i.dispose)(h)}return f.disposeMasks(),r?u:u[0]}function Ti(t){const e={};for(const n in t)e[n]=t[n].size;return e}function zi(t,e){const n=new Set,s=[],i={};for(const o of e.names())n.add(o);const a=[],r=[];for(a.push(t);a.length>0;){const t=a[a.length-1];if(n.has(t.name)){a.pop();continue}const e=r[r.length-1]===a.length-1;if(0===t.inputs.length||e)a.pop(),s.push(t),n.add(t.name),e&&r.pop();else{r.push(a.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||a.push(e)}}return{sorted:s,recipientMap:i}}function Ai(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}function Fi(t,e){return(0,i.tidy)((()=>i.sqrt(i.sum(i.mul(t,t),e,!0))))}(0,i.env)().registerFlag("TOPOLOGICAL_SORT_CACHE_MAX_ENTRIES",(()=>100),(function(t){null!=Ni&&Ni.setMaxEntries(t),null!=Si&&Si.setMaxEntries(t)}));class Di extends i.serialization.Serializable{getConfig(){return{}}}class Ci extends Di{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,i.tidy)((()=>{const e=Fi(t,this.axis),n=i.clipByValue(e,0,this.maxValue);return i.mul(t,i.div(n,i.add(ks(),e)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Ci.className="MaxNorm",i.serialization.registerClass(Ci);class Ei extends Di{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,i.tidy)((()=>i.div(t,i.add(ks(),Fi(t,this.axis)))))}getConfig(){return{axis:this.axis}}}Ei.className="UnitNorm",i.serialization.registerClass(Ei);class _i extends Di{apply(t){return i.relu(t)}}_i.className="NonNeg",i.serialization.registerClass(_i);class Mi extends Di{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return(0,i.tidy)((()=>{const e=Fi(t,this.axis),n=i.add(i.mul(this.rate,i.clipByValue(e,this.minValue,this.maxValue)),i.mul(1-this.rate,e));return i.mul(t,i.div(n,i.add(ks(),e)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Mi.className="MinMaxNorm",i.serialization.registerClass(Mi);const Ri={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function Li(t){return Wn(t)}function Oi(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return Pn(t,i.serialization.SerializationMap.getMap().classNameMap,e,"constraint")}function Wi(t){if(null==t)return null;if("string"===typeof t){return Oi({className:t in Ri?Ri[t]:t,config:{}})}return t instanceof Di?t:Oi(t)}async function Bi(t){if(null==t)return;const e=[],n=[],s=[];for(const i in t){const a=t[i];if("number"!==typeof a){const t=a;e.push(t.data()),n.push(i),s.push(t)}}if(e.length>0){const a=await Promise.all(e);for(let e=0;e<a.length;++e)t[n[e]]=a[e][0];(0,i.dispose)(s)}}function Pi(t){if(null!=t)for(const e in t){const n=t[e];"number"!==typeof n&&n.dispose()}}var Ui;!function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(Ui||(Ui={}));class Hi{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class Vi{constructor(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:10;null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class ji extends Hi{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,e){null==e&&(e={});const n=null==e.size?0:e.size;this.seen+=n;for(const s in e){const t=e[s];if("number"===typeof t)this.totals.hasOwnProperty(s)||(this.totals[s]=0),this.totals[s]=this.totals[s]+t*n;else{let e;s in this.totals?e=this.totals[s]:this.totals[s]=0;const a=(0,i.tidy)((()=>(0,i.add)(this.totals[s],(0,i.mul)(t,n))));this.totals[s]=a,null!=e&&e.dispose()}}}async onEpochEnd(t,e){if(null!=e)for(const n of this.params.metrics)null!=this.totals[n]&&("number"===typeof this.totals[n]?e[n]=this.totals[n]/this.seen:(0,i.tidy)((()=>{const t=(0,i.mul)((0,i.div)(1,this.seen),this.totals[n]);e[n]=t,this.totals[n].dispose(),(0,i.keep)(e[n])})))}}class Gi extends Hi{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const n in e)null==this.history[n]&&(this.history[n]=[]),this.history[n].push(e[n])}async syncData(){const t=[],e=[],n=[];for(const i in this.history){const s=this.history[i];for(let a=0;a<s.length;++a)if("number"!==typeof s[a]){const r=s[a];t.push(r.data()),e.push(i),n.push(a)}}const s=await Promise.all(t);for(let i=0;i<s.length;++i){this.history[e[i]][n[i]].dispose(),this.history[e[i]][n[i]]=s[i][0]}}}class qi extends Hi{constructor(t,e){if(super(),this.currentEpoch=0,this.nowFunc=t.nowFunc,this.nextFrameFunc=t.nextFrameFunc||i.nextFrame,this.yieldEvery=e||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");i.util.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,e,n){let s,a=null!=n?n():i.util.now();return function(){const r=null!=n?n():i.util.now();return r-a<e||(a=r,s=t(...arguments)),s}}(this.maybeWait.bind(this),this.yieldEvery,this.nowFunc)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,e,n){const s=[];null!=this.yield&&(await Bi(n),s.push(this.yield(t,e,n))),s.push(this.nextFrameFunc()),await Promise.all(s)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await Bi(e),await this.epochBegin(t,e))}async onEpochEnd(t,e){const n=[];null!=this.epochEnd&&(await Bi(e),n.push(this.epochEnd(t,e))),"epoch"===this.yieldEvery&&n.push(this.nextFrameFunc()),await Promise.all(n)}async onBatchBegin(t,e){null!=this.batchBegin&&(await Bi(e),await this.batchBegin(t,e))}async onBatchEnd(t,e){const n=[];null!=this.batchEnd&&(await Bi(e),n.push(this.batchEnd(t,e))),"batch"===this.yieldEvery?n.push(this.nextFrameFunc()):i.util.isNumber(this.yieldEvery)&&n.push(this.maybeWait(this.currentEpoch,t,e)),await Promise.all(n)}async onTrainBegin(t){null!=this.trainBegin&&(await Bi(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await Bi(t),await this.trainEnd(t))}}function Zi(t,e){if(null==t&&(t={}),t instanceof Hi)return[t];if(Array.isArray(t)&&t[0]instanceof Hi)return t;return Mn(t).map((t=>new qi(t,e)))}class Ki{constructor(){}static registerCallbackConstructor(t,e){i.util.assert(t>=0&&Number.isInteger(t),(()=>"Verbosity level is expected to be an integer >= 0, "+"but got ".concat(t))),Ki.checkForDuplicate(e),null==Ki.constructors[t]&&(Ki.constructors[t]=[]),Ki.constructors[t].push(e)}static checkForDuplicate(t){for(const e in Ki.constructors){Ki.constructors[+e].forEach((e=>{if(e===t)throw new Tn("Duplicate callback constructor.")}))}}static clear(){Ki.constructors={}}static createCallbacks(t){const e=[];for(const n in Ki.constructors){const s=+n;t>=s&&e.push(...Ki.constructors[s])}return e.map((t=>new t))}}function Ji(t,e,n,s,i,a,r,o,l){const u=new Gi,c=[new ji,...Ki.createCallbacks(e)];null!=t&&c.push(...t),c.push(u);const h=new Vi(c);return h.setParams({epochs:n,initialEpoch:s,samples:i,steps:a,batchSize:r,verbose:e,doValidation:o,metrics:l}),{callbackList:h,history:u}}function Yi(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return Pn(t,i.serialization.SerializationMap.getMap().classNameMap,e,"layer",n)}function Xi(t,e){return(0,i.tidy)((()=>{"float32"!==t.dtype&&(t=i.cast(t,"float32"));const n=i.sum(Cs(t),e,!0),s=i.fill(n.shape,ks()),a=i.sqrt(i.maximum(n,s));return i.div(t,a)}))}function Qi(t,e){return(0,i.tidy)((()=>i.mean(Cs(i.sub(e,t)),-1)))}function $i(t,e){return(0,i.tidy)((()=>i.mean(i.abs(i.sub(e,t)),-1)))}function ta(t,e){return(0,i.tidy)((()=>{const n=i.sub(t,e),s=i.clipByValue(i.abs(t),ks(),Number.MAX_VALUE),a=i.abs(i.div(n,s));return i.mul(100,i.mean(a,-1))}))}function ea(t,e){return(0,i.tidy)((()=>{const n=i.clipByValue(e,ks(),Number.MAX_VALUE),s=i.log(i.add(1,n)),a=i.clipByValue(t,ks(),Number.MAX_VALUE),r=i.log(i.add(1,a));return i.mean(Cs(i.sub(s,r)),-1)}))}function na(t,e){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return(0,i.tidy)((()=>{if(n)e=i.softmax(e);else{const t=i.sum(e,e.shape.length-1,!0);e=i.div(e,t)}return e=i.clipByValue(e,ks(),1-ks()),i.neg(i.sum(i.mul(i.cast(t,"float32"),i.log(e)),e.shape.length-1))}))}function sa(t,e){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return(0,i.tidy)((()=>{const s=i.cast(i.floor(function(t){const e=[fs(t.shape)];return i.reshape(t,e)}(t)),"int32"),a=(e=i.clipByValue(e,ks(),1-ks())).shape;return na(i.reshape(i.oneHot(s,a[a.length-1]),a),e,n)}))}function ia(t,e){return(0,i.tidy)((()=>{let n;return n=i.clipByValue(e,ks(),1-ks()),n=i.log(i.div(n,i.sub(1,n))),i.mean(function(t,e){if(!i.util.arraysEqual(t.shape,e.shape))throw new Tn("logits and labels must have the same shape, but got shapes "+"".concat(JSON.stringify(t.shape)," and ").concat(JSON.stringify(e.shape)));return(0,i.tidy)((()=>{const n=i.relu(e),s=i.neg(i.abs(e));return i.add(i.sub(n,i.mul(e,t)),i.log1p(i.exp(s)))}))}(t,n),-1)}))}function aa(t,e){return(0,i.tidy)((()=>{const n=i.clipByValue(t,ks(),1),s=i.clipByValue(e,ks(),1);return i.sum(i.mul(t,i.log(i.div(n,s))),-1)}))}function ra(t,e){return(0,i.tidy)((()=>{const n=Xi(t,-1),s=Xi(e,-1),a=i.mul(n,s);return i.neg(i.sum(a,-1))}))}Ki.constructors={};const oa={meanSquaredError:Qi,meanAbsoluteError:$i,meanAbsolutePercentageError:ta,meanSquaredLogarithmicError:ea,squaredHinge:function(t,e){return(0,i.tidy)((()=>{const n=i.maximum(0,i.sub(1,i.mul(t,e)));return i.mean(Cs(n),-1)}))},hinge:function(t,e){return(0,i.tidy)((()=>{const n=i.maximum(0,i.sub(1,i.mul(t,e)));return i.mean(n,-1)}))},categoricalHinge:function(t,e){return(0,i.tidy)((()=>{const n=i.sum(i.mul(t,e),-1),s=i.max(i.mul(i.sub(1,t),e),-1);return i.maximum(0,i.add(1,i.sub(s,n)))}))},logcosh:function(t,e){return(0,i.tidy)((()=>{const n=Math.log(2),s=i.sub(e,t),a=i.sub(i.add(s,i.softplus(i.mul(-2,s))),n);return i.mean(a,-1)}))},categoricalCrossentropy:na,sparseCategoricalCrossentropy:sa,binaryCrossentropy:ia,kullbackLeiblerDivergence:aa,poisson:function(t,e){return(0,i.tidy)((()=>{const n=i.log(i.add(ks(),e));return i.mean(i.sub(e,i.mul(t,n)),-1)}))},cosineProximity:ra};function la(t){if("string"===typeof t){if(t in oa)return oa[t];let e="Unknown loss ".concat(t);throw t.toLowerCase().includes("softmaxcrossentropy")&&(e="Unknown loss ".concat(t,". ")+'Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy'),new Tn(e)}return t}function ua(t,e){return(0,i.tidy)((()=>{const n=i.mul(.5,i.onesLike(e)),s=ws(i.greater(e,n),t.dtype);return i.mean(i.equal(t,s),-1)}))}function ca(t,e){return(0,i.tidy)((()=>ws(i.equal(i.argMax(t,-1),i.argMax(e,-1)),"float32")))}function ha(t,e){return(0,i.tidy)((()=>i.cast(i.sum(i.logicalAnd(i.equal(t,1),i.equal(e,1))),"float32")))}function pa(t,e){return ia(t,e)}function da(t,e){return t.rank===e.rank&&(t=i.squeeze(t,[t.rank-1])),(e=i.argMax(e,-1)).dtype!==t.dtype&&(e=i.cast(e,t.dtype)),i.cast(i.equal(t,e),"float32")}const fa=na,ma=sa,ga={binaryAccuracy:ua,categoricalAccuracy:ca,precision:function(t,e){return(0,i.tidy)((()=>{const n=ha(t,e),s=function(t,e){return(0,i.tidy)((()=>i.cast(i.sum(i.logicalAnd(i.equal(t,0),i.equal(e,1))),"float32")))}(t,e),a=i.add(n,s);return i.cast(i.where(i.greater(a,0),i.div(n,a),0),"float32")}))},categoricalCrossentropy:fa,sparseCategoricalCrossentropy:ma,mse:Qi,MSE:Qi,mae:$i,MAE:$i,mape:ta,MAPE:ta,cosine:ra};function ya(t){if("string"===typeof t&&t in ga)return ga[t];if("string"!==typeof t&&null!=t)return t;throw new Tn("Unknown metric ".concat(t))}function ba(t){if(Cn(null!==t,"Unknown LossOrMetricFn ".concat(t)),"string"===typeof t)return t;{let e;for(const n of Object.keys(oa))if(oa[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(ga))if(ga[n]===t){e=n;break}return void 0!==e?e:t.name}}const ka=1048576;function wa(t,e){let n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if(null==t||"object"!==typeof t||Object.getPrototypeOf(t)!==Object.prototype||!va(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>ka&&console.warn('User-defined metadata of model "'.concat(e,'" is too large in ')+"size (length=".concat(n.length," when serialized). It is not ")+"recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= "+"".concat(ka,"."))}}function va(t){if(null===t)return!0;if("object"===typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!==typeof n)return!1;if(!va(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!va(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function Ia(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:console.log;const i=function(t){let e=!0;const n=[],s=[];for(const i in t.nodesByDepth)n.push(t.nodesByDepth[i]);for(const i of n){if(i.length>1||1===i.length&&i[0].inboundLayers.length>1){e=!1;break}s.push(...i)}if(e)for(const i of t.layers){let t=!1;for(const n of i.inboundNodes)if(-1!==s.indexOf(n)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),a=["Layer (type)","Input Shape","Output shape","Param #"];let r;if(i?(e=e||90,n=n||[.32,.61,.89,1]):(e=e||115,n=n||[.24,.48,.7,.8,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){a.push("Receives inputs"),r=[];for(const e in t.nodesByDepth)r.push(...t.nodesByDepth[e])}s("_".repeat(e)),Na(a,n,s),s("=".repeat(e));const o=t.layers;for(let c=0;c<o.length;++c)i?Sa(o[c],n,s):xa(o[c],n,r,s),s((c===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?li(t.collectedTrainableWeights):li(t.trainableWeights);return e}(t),u=li(t.nonTrainableWeights);s("Total params: ".concat(l+u)),s("Trainable params: ".concat(l)),s("Non-trainable params: ".concat(u)),s("_".repeat(e))}function Na(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:console.log,s="";for(let i=0;i<t.length;++i)i>0&&(s=s.slice(0,s.length-1)+" "),s+=t[i],s=s.slice(0,e[i]),s+=" ".repeat(e[i]-s.length);n(s)}function Sa(t,e,n){let s,i;try{i=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(o){i="multiple"}try{s=JSON.stringify(t.outputShape)}catch(o){s="multiple"}const a=t.name,r=t.getClassName();Na(["".concat(a," (").concat(r,")"),i,s,t.countParams().toString()],e,n)}function xa(t,e,n,s){let i,a;try{a=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(c){a="multiple"}try{i=JSON.stringify(t.outputShape)}catch(c){i="multiple"}const r=[];for(const h of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(h)))for(let t=0;t<h.inboundLayers.length;++t){const e=h.inboundLayers[t].name,n=h.nodeIndices[t],s=h.tensorIndices[t];r.push("".concat(e,"[").concat(n,"][").concat(s,"]"))}const o=t.name,l=t.getClassName(),u=0===r.length?"":r[0];Na(["".concat(o," (").concat(l,")"),a,i,t.countParams().toString(),u],e,s);for(let h=1;h<r.length;++h)Na(["","","","",r[h]],e,s)}function Ta(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"===typeof n}function za(t,e){if(null===t)return null;if("string"===typeof t)return Ln(t);if("number"===typeof t||"boolean"===typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Ta(e,i,s)?n.push(s):n.push(za(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"===typeof s)e[n]=s;else{const t=Ln(n);e[t]=za(s,t)}}return e}}function Aa(t,e){if(null===t||void 0===t)return null;if("string"===typeof t)return Rn(t);if("number"===typeof t||"boolean"===typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];Ta(e,i,s)?n.push(s):n.push(Aa(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=Rn(n);e[i]="name"!==n&&"className"!==n||"string"!==typeof s?Aa(s,n):s}return e}}const Fa="4.22.0";class Da extends bi{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=Qn(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],Hn(this.inputs).length!==this.inputs.length)throw new Tn("The list of inputs passed to the model is redundant. All inputs should only appear once. Found: "+"".concat(this.inputs.map((t=>t.name))));Hn(this.outputs).length!==this.outputs.length&&console.warn("The list of outputs passed to the model is redundant. All outputs should only appear once. Found: "+"".concat(this.outputs.map((t=>t.name)))),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const y of this.outputs){const t=y.sourceLayer,e=y.nodeIndex,n=y.tensorIndex;this.outputLayers.push(t),this.outputLayersNodeIndices.push(e),this.outputLayersTensorIndices.push(n)}for(const y of this.inputs){const t=y.sourceLayer,e=y.nodeIndex,n=y.tensorIndex;Cn(0===e,"input layer has >1 nodes"),Cn(0===n,"input layer has >1 tensors"),this.inputLayers.push(t),this.inputLayersNodeIndices.push(e),this.inputLayersTensorIndices.push(n)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let y=0;y<this.inputLayers.length;y++){const e=this.inputLayers[y];if(!(e instanceof wi))throw new TypeError("Input layers to a LayersModel must be InputLayer objects. "+"Received inputs: ".concat(t.inputs,". ")+"Input ".concat(y," (0-based) originates ")+"from layer type ".concat(e.getClassName(),"."));this.inputNames.push(e.name),this.feedInputShapes.push(e.batchInputShape),this.feedInputNames.push(e.name)}for(const y of this.outputLayers)this.outputNames.push(y.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},a={},r=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new xn("The tensor ".concat(t.name,' at layer "').concat(s.name,'" ')+"is part of a cycle.");if(-1!==e.indexOf(u))return;this.containerNodes.add(Da.nodeKey(s,i)),s.id in a||(a[s.id]=Object.keys(a).length),-1===n.indexOf(u)&&n.push(u);const c=u.inboundLayers.length;for(let a=0;a<c;a++){const t=u.inputTensors[a],s=u.inboundLayers[a],i=u.nodeIndices[a],r=u.tensorIndices[a];o(t,e,n,s,i,r)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);r.push(u)},l=[],u=[];for(const y of this.outputs)o(y,l,u);const c=r.slice().reverse();for(const y of c){n[y.id]=y,y.id in e||(e[y.id]=0);let t=e[y.id];const a=null==s[y.outboundLayer.id]?0:s[y.outboundLayer.id];t=Math.max(t,a),s[y.outboundLayer.id]=t,i[y.outboundLayer.id]=y.outboundLayer,e[y.id]=t;for(let s=0;s<y.inboundLayers.length;s++){const i=y.inboundLayers[s],a=y.nodeIndices[s],r=i.inboundNodes[a],o=null==e[r.id]?0:e[r.id];e[r.id]=Math.max(t+1,o),n[r.id]=r}}const h={};for(const y in e){const t=e[y];t in h||(h[t]=[]),h[t].push(n[y])}const p={};for(const y in s){const t=s[y];t in p||(p[t]=[]),p[t].push(i[y])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(Un);this.layers=[];for(const y of d){const t=p[y];t.sort(((t,e)=>{const n=a[t.id],s=a[e.id];return n<s?-1:n>s?1:0}));for(const e of t)e instanceof Da&&this.internalContainerRefs.push(e),this.layers.push(e)}this.layersByDepth=p,d=Object.keys(h).map((t=>parseInt(t,10))).sort(Un);const f=this.inputs.slice(),m=[];for(const y of d)for(const t of h[y]){const e=t.outboundLayer;if(null!=e){for(const n of t.inputTensors)if(-1===f.indexOf(n))throw new xn("Graph disconnected: cannot obtain value for tensor ".concat(n)+' at layer "'.concat(e.name,'". ')+"The following previous layers were accessed without "+"issue: ".concat(m));for(const e of t.outputTensors)f.push(e);m.push(e.name)}}this.nodesByDepth=h;const g=this.layers.map((t=>t.name));for(const y of g){const t=g.filter((t=>t===y)).length;if(1!==t)throw new xn('The name "'.concat(y,'" is used ').concat(t," times ")+"in the model. All layer names should be unique. Layer names: "+JSON.stringify(g))}this.outboundNodes=[],this.inboundNodes=[],new gi({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error("Container '".concat(this.name,"' is already disposed."))}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0===--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new Tn("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t){let e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];const n={};let s=0;const i=(t=>{const e=Object.keys(t);if(0===e.length)return!1;const n=e[0].split("/");return!isNaN(parseInt(n[n.length-1],10))})(t);i&&this.parseWeights(t);for(const r of this.layers)for(const[t,e]of r.weights.entries()){const a=i?"".concat(e.name.split("/").slice(0,-1).join("/")+"/").concat(t):e.originalName;if(null!=n[a])throw new Tn("Duplicate weight name: ".concat(a));n[a]=e,s++}const a=[];for(const r in t){let s=r;if(null==n[r]){const t=r.split("/");s=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[s])a.push([n[s],t[r]]);else if(e)throw new Tn("Provided weight data has no target variable: ".concat(r));delete n[s]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new Tn("".concat(t.length," of ").concat(s," weights are not set: ")+"".concat(t))}pi(a)}parseWeights(t){for(const e in Object.keys(t)){const n=e.split("/"),s=["vars","layer_checkpoint_dependencies"],i=n.map((t=>t.startsWith("_")?t.slice(1):t)).filter((t=>!s.includes(t))).join("/");i!==e&&(t[i]=t[e],delete t[e])}}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers ".concat(Fa),e.backend="TensorFlow.js",e}toJSON(t){let e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];const n=Aa(this.updatedConfig());return e?JSON.stringify(n):n}call(t,e){return(0,i.tidy)((()=>{t=Mn(t);const n=new Ii;for(let e=0;e<this.inputs.length;++e)n.add(this.inputs[e],t[e]);return xi(this.outputs,n,e)}))}computeMask(t,e){return(0,i.tidy)((()=>{let n;return t=Mn(t),n=null==e?Dn(null,t.length):Mn(e),this.runInternalGraph(t,n)[1]}))}computeOutputShape(t){const e=ai(t);if(e.length!==this.inputLayers.length)throw new Tn("Invalid inputShape argument ".concat(t,": ")+"model has ".concat(this.inputLayers.length," tensor inputs."));const n={};for(let r=0;r<e.length;r++){const t=this.inputLayers[r],s=e[r];n[t.name+"_0_0"]=s}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Un);if(s.length>1)for(const r of s){const t=this.nodesByDepth[r];for(const e of t){const t=e.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(t.id))continue;const s=[];for(let r=0;r<e.inboundLayers.length;r++){const t=e.inboundLayers[r],i=e.nodeIndices[r],a=e.tensorIndices[r],o=n["".concat(t.name,"_").concat(i,"_").concat(a)];s.push(o)}const i=ai(t.computeOutputShape(_n(s))),a=t.inboundNodes.indexOf(e);for(let e=0;e<i.length;e++){n["".concat(t.name,"_").concat(a,"_").concat(e)]=i[e]}}}const i=[],a=[];for(let r=0;r<this.outputLayers.length;r++){const t=this.outputLayers[r],e=this.outputLayersNodeIndices[r],n=this.outputLayersTensorIndices[r],s="".concat(t.name,"_").concat(e,"_").concat(n);a.push(s)}for(let r=0;r<a.length;r++){const t=a[r];Cn(t in n),i.push(n[t])}return _n(i)}runInternalGraph(t,e){null==e&&(e=Dn(null,t.length));const n={};for(let o=0;o<this.inputs.length;++o){const s=this.inputs[o],i=t[o],a=e[o];n[s.id]=[i,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Un);for(const o of s){const t=this.nodesByDepth[o];for(const e of t){const t=e.outboundLayer,s=e.inputTensors,i=e.outputTensors,a=new Array;for(const e of s)e.id in n&&a.push(n[e.id]);if(a.length===s.length){let s,r,o,l,u={};if(null!=e.callArgs&&(u=e.callArgs),1===a.length){const[e,n]=a[0];null==u.mask&&(u.mask=n),o=Mn(t.call(e,u)),l=Mn(t.computeMask(e,n)),s=[e],r=[n]}else s=a.map((t=>t[0])),r=a.map((t=>t[1])),null==u.mask&&(u.mask=r),o=Mn(t.call(s,u)),l=Mn(t.computeMask(s,r));if(t.activityRegularizer)throw new zn("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],a=l[t];n[e.id]=[s,a]}}}}const i=[],a=[],r=[];for(const o of this.outputs){Cn(o.id in n,"Could not compute output ".concat(o.name," : ").concat(o.id));const[t,e]=n[o.id];r.push(t.shape),i.push(t),a.push(e)}return[i,a,r]}buildNodeConversionMap(t){const e={};let n;for(const s of this.layers){n=s instanceof Da?1:0;for(let t=0;t<s.inboundNodes.length;t++){const i=Da.nodeKey(s,t);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e)return this.findLayer(e);if(null==t)throw new Tn("Provide either a layer name or layer index");if("number"===typeof t)return this.findLayer(t);for(const n of this.layers)if(n.name===t)return n;throw new Tn("No such layer: ".concat(t))}findLayer(t){if(this.layers.length<=t)throw new Tn("Was asked to retrieve layer at index ".concat(t,", but model only ")+"has ".concat(this.layers.length," layer(s)."));return this.layers[t]}calculateLosses(){return(0,i.tidy)((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=Da.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const r of this.layers){const t=r.getClassName(),s=r.getConfig(),i=[];for(let n=0;n<r.inboundNodes.length;n++){const t=r.inboundNodes[n],s=Da.nodeKey(r,n);let o={};if(this.containerNodes.has(s)){if(t.callArgs)try{JSON.stringify(t.callArgs),o=t.callArgs}catch(a){console.warn("Layer ".concat(r.name," was passed ")+"non-serializable keyword arguments: "+"".concat(t.callArgs,". They will not be included ")+"in the serialized model (and thus will be missing at deserialization time)."),o={}}if(t.inboundLayers.length>0){const n=[];for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],r=t.tensorIndices[s];let l=e[Da.nodeKey(i,a)];null==l&&(l=0),n.push([i.name,l,r,o])}i.push(n)}}}const o={};o.name=r.name,o.className=t,o.config=s,o.inboundNodes=i,n.push(o)}t.layers=n;const s=[];for(let r=0;r<this.inputLayers.length;r++){const t=this.inputLayers[r],n=this.inputLayersNodeIndices[r],i=Da.nodeKey(t,n);if(!this.containerNodes.has(i))continue;let a=e[i];null!==a&&void 0!==a||(a=0);const o=this.inputLayersTensorIndices[r];s.push([t.name,a,o])}t.inputLayers=s;const i=[];for(let r=0;r<this.outputLayers.length;r++){const t=this.outputLayers[r],n=this.outputLayersNodeIndices[r],s=Da.nodeKey(t,n);if(!this.containerNodes.has(s))continue;let a=e[s];null!==a&&void 0!==a||(a=0);const o=this.outputLayersTensorIndices[r];i.push([t.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e){let n=arguments.length>3&&void 0!==arguments[3]&&arguments[3];const s={},i={};function a(t,e){t.name in i?i[t.name].push(e):i[t.name]=[e]}function r(t,e){const n=[];let i;for(const r of e){const o=r[0],l=r[1],u=r[2];if(i=null==r[3]?{}:r[3],!(o in s))return void a(t,e);const c=s[o];if(c.inboundNodes.length<=l)return void a(t,e);const h=c.inboundNodes[l];n.push(h.outputTensors[u])}n.length>0&&t.apply(_n(n),i)}function o(t){const i=t.name,r=Yi(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(n),s[i]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new Tn("Corrupted configuration, expected array for nodeData: ".concat(t));a(r,t)}))}const l=e.name,u=e.layers;for(const f of u)o(f);for(;!Vn(i);)for(const t of u){const e=s[t.name];if(e.name in i){const t=i[e.name];delete i[e.name];for(const n of t)r(e,n)}}const c=[],h=[],p=e.inputLayers;for(const f of p){const t=f[0],e=f[1],n=f[2];Cn(t in s);const i=s[t].inboundNodes[e].outputTensors;c.push(i[n])}const d=e.outputLayers;for(const f of d){const t=f[0],e=f[1],n=f[2];Cn(t in s);const i=s[t].inboundNodes[e].outputTensors;h.push(i[n])}return new t({inputs:c,outputs:h,name:l})}get stateful(){if(this._stateful)throw new Tn("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){(0,i.tidy)((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function Ca(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"===typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error("Provided ".concat(n," is an array of ").concat(t.length," ")+"element(s), but the model has ".concat(s," outputs. ")+"Make sure a set of weights is provided for each model output.");return t}if("object"===typeof t&&Object.keys(t).length>0&&"object"===typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error("The model has multiple (".concat(s,") outputs, ")+"so ".concat(n," must be either an array with ")+"".concat(s," elements or an object with ").concat(e," keys. ")+"Provided ".concat(n," not understood: ").concat(JSON.stringify(t)))}function Ea(t,e){return Ca(t,e,"classWeight")}async function _a(t,e,n,s){if(null!=e||null!=s)throw new Error("Support sampleWeight is not implemented yet");if(null!=n){const e=(0,i.tidy)((()=>{if(1===t.shape.length)return(0,i.clone)(t);if(2===t.shape.length){if(t.shape[1]>1){const e=1;return(0,i.argMax)(t,e)}if(1===t.shape[1])return(0,i.reshape)(t,[t.shape[0]]);throw new Error("Encountered unexpected last-dimension size (".concat(t.shape[1],") ")+"during handling of class weights. The size is expected to be >= 1.")}throw new Error("Unexpected rank of target (y) tensor (".concat(t.rank,") during ")+"handling of class weights. The rank is expected to be 1 or 2.")})),s=Array.from(await e.data());(0,i.dispose)(e);const a=[];return s.forEach((t=>{if(null==n[t])throw new Error("classWeight must contain all classes in the training data. "+"The class ".concat(t," exists in the data but not in ")+"classWeight");a.push(n[t])})),(0,i.tensor1d)(a,"float32")}return null}function Ma(t,e){return(0,i.mul)(t,e)}function Ra(t,e){let n,s;const a=e;n=a.xs,s=a.ys,i.util.assert(null!=n&&null!=s,(()=>"A Dataset iterator for fitDataset() is expected to generate objects of the form `{xs: xVal, ys: yVal}`, where the two values may be `tf.Tensor`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates "+"".concat(e)));const r=La("input",t.inputNames,n),o=La("output",t.outputNames,s),l=r[0].shape[0];i.util.assert(r.length===t.inputs.length,(()=>"LayersModel has ".concat(t.inputs.length," inputs, but the dataset ")+"provides ".concat(r.length," inputs.  (Expected input keys: ")+"".concat(JSON.stringify(t.inputNames),")"))),i.util.assert(o.length===t.outputs.length,(()=>"LayersModel has ".concat(t.outputs.length," outputs, but the dataset ")+"provides ".concat(o.length," outputs.  (Expected output keys: ")+"".concat(JSON.stringify(t.outputNames),")")));for(let u=0;u<r.length;u++)i.util.assert(r[u].shape[0]===l,(()=>"Batch size mismatch: input "+"".concat(t.inputNames[u]," has ").concat(r[u].shape[0],"; ")+"expected  ".concat(l," based on input ").concat(t.inputNames[0],".")));for(let u=0;u<o.length;u++)i.util.assert(o[u].shape[0]===l,(()=>"Batch size mismatch: output "+"".concat(t.outputNames[u]," has ").concat(o[u].shape[0],"; ")+"expected  ".concat(l," based on input ").concat(t.inputNames[0],".")));return{xs:r,ys:o}}function La(t,e,n){if(n instanceof i.Tensor)return[n];if(Array.isArray(n))return i.util.assert(n.length===e.length,(()=>"Received an array of ".concat(n.length," Tensors, but expected ").concat(e.length," to match the ").concat(t," keys ").concat(e,"."))),n;{const s=[];for(const i of e){if(null==n[i])throw new Tn("The feature data generated by the dataset lacks the required "+"".concat(t," key '").concat(i,"'."));s.push(n[i])}return s}}async function Oa(t,e,n){const s=null!=n.batchesPerEpoch;if(i.util.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),i.util.assert(null!=n,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),i.util.assert(null!=n.epochs&&n.epochs>0&&Number.isInteger(n.epochs),(()=>"For fitDataset(), config.epochs is expected to be a positive "+"integer, but got ".concat(n.epochs))),i.util.assert(!s||n.batchesPerEpoch>0&&Number.isInteger(n.batchesPerEpoch),(()=>"For fitDataset(), config.batchesPerEpoch is expected to be a "+"positive integer if specified, but got ".concat(n.batchesPerEpoch))),i.util.assert(null==n.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const a=null!=n.validationData;let r,o;if(a)if(Wa(n.validationData))i.util.assert(null==n.validationBatches||n.validationBatches>0&&Number.isInteger(n.validationBatches),(()=>"For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, "+"but got ".concat(n.validationBatches)));else{const t=function(t){if(3===t.length)throw new zn("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(n.validationData);r=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let c;c=a?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const h=Zi(n.callbacks,n.yieldEvery),p=null==n.verbose?1:n.verbose,{callbackList:d,history:f}=Ji(h,p,n.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(e,n),null,a,c);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let m=null==n.initialEpoch?0:n.initialEpoch,g=await e.iterator();for(;m<n.epochs;){const c={};await d.onEpochBegin(m);let h=0,p=0;for(s||(g=await e.iterator());!s||h<n.batchesPerEpoch;){const e=await g.next();if(s&&e.done){console.warn("You provided `batchesPerEpoch` as "+"".concat(n.batchesPerEpoch,", ")+"but your dataset iterator ran out of data after "+"".concat(h," batches; ")+"interrupting training. Make sure that your dataset can generate at least `batchesPerEpoch * epochs` batches (in this case, "+"".concat(n.batchesPerEpoch*n.epochs," batches). ")+"You may need to use the repeat() function when building your dataset.");break}if(null!=e.value){const{xs:s,ys:a}=Ra(t,e.value),r={};r.batch=p,r.size=s[0].shape[0],await d.onBatchBegin(p,r);const o=[];if(null!=n.classWeight){const e=Ea(n.classWeight,t.outputNames);for(let t=0;t<e.length;++t)o.push(await _a(a[t],null,e[t]))}const c=s.concat(a).concat(o),f=l(c);i.dispose(c);for(let t=0;t<u.length;++t){const e=u[t],n=f[t];r[e]=n,i.keep(n)}await d.onBatchEnd(p,r),Pi(r),p++,h++}if(s?h>=n.batchesPerEpoch:e.done){if(a){let e;e=Wa(n.validationData)?Mn(await t.evaluateDataset(n.validationData,{batches:n.validationBatches})):Mn(t.evaluate(r,o,{batchSize:null==n.validationBatchSize?32:n.validationBatchSize,verbose:0}));for(let n=0;n<t.metricsNames.length;++n)c["val_".concat(t.metricsNames[n])]=e[n]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(m,c),m++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function Wa(t){return"function"===typeof t.iterator}function Ba(t){i.util.assert(t>0&&Number.isInteger(t),(()=>"batchSize is required to be a positive integer, but got ".concat(t)))}function Pa(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>Is(t,e,n-e))):Is(t,e,n-e)}function Ua(t,e){return i.tidy((()=>null==t?null:Array.isArray(t)?t.map((t=>Ua(t,e))):Ds(t,"int32"===e.dtype?e:i.cast(e,"int32"))))}function Ha(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}function Va(t){const e=[];t instanceof i.Tensor&&(t=[t]);for(let n=0;n<t.length;++n){const s=t[n];if(1===s.rank)e.push(vs(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");e.push(s)}}return e}function ja(t,e){if(null==t)return;const n=[];if(e instanceof i.Tensor)n.push(e.id);else if(Array.isArray(e))e.forEach((t=>n.push(t.id)));else if(null!=e)for(const i in e){const t=e[i];n.push(t.id)}const s=[];if(t instanceof i.Tensor)-1===n.indexOf(t.id)&&s.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===n.indexOf(t.id)&&s.push(t)}));else if(null!=t)for(const i in t){const e=t[i];-1===n.indexOf(e.id)&&s.push(e)}s.forEach((t=>{t.isDisposed||t.dispose()}))}function Ga(t){return Array.isArray(t)}function qa(t){return!function(t){return t instanceof i.Tensor}(t)&&!Ga(t)}function Za(t,e,n){let s,i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:"";if(null==e||0===e.length){if(null!=t){let e=!1;if(Ga(t)&&t.length>0)e=!0;else if(qa(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new Tn("Error when checking model ".concat(a," expected no data, ")+"but got ".concat(t))}return[]}if(null==t)return e.map((t=>null));if(qa(t)){s=[];for(const n of e){if(null==t[n])throw new Tn('No data provided for "'.concat(n,'". Need data for each key in: ')+"".concat(e));s.push(t[n])}}else if(Ga(t)){if(t.length!==e.length)throw new Tn("Error when checking model ".concat(a,": the Array of ")+"Tensors that you are passing to your model is not the size the "+"model expected. Expected to see ".concat(e.length," Tensor(s), but ")+"instead got the following list of Tensor(s): ".concat(t));s=t}else{if(e.length>1)throw new Tn("The model ".concat(a," expects ").concat(e.length," Tensor(s), ")+"but only received one Tensor. Found: Tensor with shape ".concat(t.shape));s=[t]}if(s=Va(s),null!=n)for(let r=0;r<e.length;++r){if(null==n[r])continue;const t=s[r];if(t.shape.length!==n[r].length)throw new Tn("Error when checking ".concat(a,": expected ").concat(e[r]," ")+"to have ".concat(n[r].length," dimension(s). but got array with ")+"shape ".concat(t.shape));for(let e=0;e<n[r].length;++e){if(0===e&&!i)continue;const s=t.shape[e],o=n[r][e];if(null!=o&&o>=0&&s!==o)throw new Tn("".concat(a," expected a batch of elements where each ")+"example has shape [".concat(n[r].slice(1,n[r].length),"] ")+"(i.e.,tensor shape [*,".concat(n[r].slice(1,n[r].length),"])")+" but the ".concat(a," received an input with ").concat(t.shape[0])+" examples, each with shape [".concat(t.shape.slice(1,t.shape.length),"]")+" (tensor shape [".concat(t.shape,"])"))}}return s}function Ka(t,e,n){let s,i=!(arguments.length>3&&void 0!==arguments[3])||arguments[3],a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:"";if(Array.isArray(t)){if(t.length!==e.length)throw new Tn("Error when checking model ".concat(a,": the Array of ")+"Tensors that you are passing to your model is not the size the "+"the model expected. Expected to see ".concat(e.length," Tensor(s),")+" but instead got ".concat(t.length," Tensors(s)."));s=t}else{if(e.length>1)throw new Tn("The model expects ".concat(e.length," ").concat(a," Tensors, ")+"but only received one Tensor. Found: array with shape "+"".concat(JSON.stringify(t.shape),"."));s=[t]}if(null!=n)for(let r=0;r<e.length;++r){if(null==n[r])continue;const t=s[r];if(t.shape.length!==n[r].length)throw new Tn("Error when checking ".concat(a,": expected ").concat(e[r]," ")+"to have ".concat(n[r].length," dimension(s), but got array with ")+"shape ".concat(JSON.stringify(t.shape)));for(let s=0;s<n[r].length;++s){if(0===s&&!i)continue;const o=t.shape[s],l=n[r][s];if(null!=l&&l!==o)throw new Tn("Error when checking ".concat(a,": expected ")+"".concat(e[r]," to have shape ").concat(JSON.stringify(n[r])," but ")+"got array with shape ".concat(JSON.stringify(t.shape),"."))}}}class Ja extends Da{constructor(t){super(t),this.isTraining=!1}summary(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:console.log;if(!this.built)throw new Tn("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");Ia(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"===typeof t.optimizer)this.optimizer_=function(t){const e={Adagrad:()=>i.train.adagrad(.01),Adadelta:()=>i.train.adadelta(1,.95,ks()),Adam:()=>i.train.adam(.001,.9,.999,ks()),Adamax:()=>i.train.adamax(.002,.9,.999,ks(),0),RMSProp:()=>i.train.rmsprop(.001,.9,0,ks()),SGD:()=>i.train.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new Tn("Unknown Optimizer ".concat(t))}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof i.Optimizer))throw new Tn("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let e=[];if(Array.isArray(t.loss)||"string"===typeof t.loss||"function"===typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new Tn("When passing an Array as loss, it should have one entry per "+"model output. The model has ".concat(this.outputs.length," output(s), ")+"but you passed loss=".concat(t.loss,"."));const n=t.loss;e=n.map((t=>la(t)))}else{const n=la(t.loss);this.outputs.forEach((t=>{e.push(n)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new Tn('Unknown entry in loss dictionary: "'.concat(e,'". ')+"Only expected the following keys: ".concat(this.outputNames));for(const n of this.outputNames)null==t.loss[n]&&console.warn('Output "'.concat(n,'" is missing from loss dictionary. We assume ')+"this was done on purpose, and we will not be expecting data "+"to be passed to ".concat(n," during training")),e.push(la(t.loss[n]))}this.lossFunctions=e,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let i=0;i<this.outputs.length;++i){const t=this.internalOutputShapes[i],e=this.outputNames[i];this.feedOutputNames.push(e),this.feedOutputShapes.push(t),this.feedLossFns.push(this.lossFunctions[i])}const n=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],us("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const s=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"===typeof t||"function"===typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!==typeof t)throw new TypeError("Type of metrics argument not understood. Expected an string,"+"function, Array, or Object, found: ".concat(t));n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),a=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};us("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==n.indexOf(t))continue;(e=>{let n,s,i;for(const r of e){if("string"===typeof r&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(r)){const e=this.internalOutputShapes[t];let a;1===e[e.length-1]||this.lossFunctions[t]===ia?-1!==["accuracy","acc"].indexOf(r)?s=ua:-1!==["crossentropy","ce"].indexOf(r)&&(s=pa):this.lossFunctions[t]===sa?-1!==["accuracy","acc"].indexOf(r)?s=da:-1!==["crossentropy","ce"].indexOf(r)&&(s=ma):-1!==["accuracy","acc"].indexOf(r)?s=ca:-1!==["crossentropy","ce"].indexOf(r)&&(s=fa),-1!==["accuracy","acc"].indexOf(r)?a="acc":-1!==["crossentropy","ce"].indexOf(r)&&(a="ce"),i=s,n=""+a}else{const t=ya(r);i=t,n=""+ba(r)}let e;us(n,(()=>{e=i})),a(t,n,e)}})(s[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const s=null==n.batchSize?32:n.batchSize;Ba(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const t=i[0].concat(i[1]);this.makeTestFunction();const e=this.testFunction;return _n(this.testLoop(e,t,s,n.verbose,n.steps))}finally{ja(i[0],t),ja(i[1],e)}}async evaluateDataset(t,e){return this.makeTestFunction(),async function(t,e,n){const s=null!=(n=n||{}).batches,a=t.testFunction;let r=[];if(n.verbose>0)throw new zn("Verbose mode is not implemented yet.");i.util.assert(!s||n.batches>0&&Number.isInteger(n.batches),(()=>"Test loop expects `batches` to be a positive integer, but "+"received ".concat(JSON.stringify(n.batches))));const o="function"===typeof e.next?e:await e.iterator();let l=0,u=0;for(;!s||u<n.batches;){const e=await o.next();if(r=i.tidy((()=>{if(e.value){const{xs:n,ys:s}=Ra(t,e.value),o=n.concat(s),c=i.tidy((()=>a(o)));if(i.dispose(o),0===u)for(let t=0;t<c.length;++t)r.push((0,i.scalar)(0));const h=o[0].shape[0];for(let t=0;t<c.length;++t){const e=c[t],n=r[t];r[t]=i.tidy((()=>i.add(r[t],i.mul(h,e)))),u>0&&i.dispose(n)}i.dispose(c),l+=h,++u}return r})),e.done){s&&console.warn("Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least `batches` "+"batches (in this case, ".concat(n.batches," batches). ")+"You may need to use the repeat() function when building your dataset.");break}}for(let c=0;c<r.length;++c){const t=r[c];r[c]=i.div(r[c],l),i.dispose(t)}return _n(r)}(this,t,e)}checkNumSamples(t,e,n){let s,i=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"steps";if(null!=n){if(s=null,null!=e)throw new Tn("If ".concat(i," is set, batchSize must be null or undefined.")+"Got batchSize = ".concat(e))}else{if(null==t)throw new Tn("Either the input data should have a defined shape, or "+"".concat(i," shoud be specified."));s=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return s}execute(t,e){if(Array.isArray(e)&&0===e.length)throw new Tn("`outputs` is an empty Array, which is not allowed.");const n=Array.isArray(e),s=n?e:[e],a=this.retrieveSymbolicTensors(s),r=new Ii;if(t instanceof i.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new Tn("The number of inputs provided (".concat(t.length,") ")+"does not match the number of inputs of this model "+"(".concat(this.inputs.length,")."));for(let e=0;e<this.inputs.length;++e)r.add(this.inputs[e],t[e])}else for(const i of this.inputs){const e=t[i.name];if(null==e)throw new Tn("No value is provided for the model's input ".concat(i.name));r.add(i,e)}const o=xi(a,r);return n?o:o[0]}retrieveSymbolicTensors(t){const e=Dn(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],a=i.map((t=>t.name));for(let s=0;s<t.length;++s){const r=a.indexOf(t[s]);if(-1!==r&&(e[s]=i[r],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new Tn("Cannot find SymbolicTensors for output name(s): "+"".concat(JSON.stringify(n)))}return e}predictLoop(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:32,n=arguments.length>2&&void 0!==arguments[2]&&arguments[2];return i.tidy((()=>{const s=this.checkNumSamples(t);if(n)throw new zn("Verbose predictLoop() is not implemented yet.");const a=Ha(s,e),r=this.outputs.map((t=>[]));for(let e=0;e<a.length;++e){i.tidy((()=>{const n=a[e][0],s=a[e][1],i=Pa(t,n,s),r=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)r.push({key:this.inputs[t],value:i[t]});else r.push({key:this.inputs[0],value:i});const o=new Ii(r);return xi(this.outputs,o)})).forEach(((t,e)=>r[e].push(t)))}return _n(r.map((t=>i.concat(t,0))))}))}predict(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};const n=Va(t);Ka(n,this.inputNames,this.feedInputShapes,!1);try{const t=null==e.batchSize?32:e.batchSize;return Ba(t),this.predictLoop(n,t)}finally{ja(n,t)}}predictOnBatch(t){Ka(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,e){let n=arguments.length>3?arguments[3]:void 0;if(null==this.optimizer_)throw new xn("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const s=[];for(let i=0;i<this.feedOutputShapes.length;++i){const t=this.feedOutputShapes[i];this.feedLossFns[i]===sa?s.push(t.slice(0,t.length-1).concat([1])):s.push(t)}if(function(t,e){const n=Hn(t.map((t=>t.shape[0])));n.sort();const s=Hn(e.map((t=>t.shape[0])));if(s.sort(),n.length>1)throw new Tn("All input Tensors (x) should have the same number of samples. Got array shapes: "+"".concat(JSON.stringify(t.map((t=>t.shape)))));if(s.length>1)throw new Tn("All target Tensors (y) should have the same number of samples. Got array shapes: "+"".concat(JSON.stringify(e.map((t=>t.shape)))));if(n.length>0&&s.length>0&&!i.util.arraysEqual(n,s))throw new Tn("Input Tensors should have the same number of samples as target "+"Tensors. Found ".concat(n[0]," input sample(s) and ").concat(s[0]," target ")+"sample(s).")}(t=Za(t,this.feedInputNames,this.feedInputShapes,!1,"input"),e=Za(e,this.feedOutputNames,s,!1,"target")),function(t,e,n){const s=[Qi,ia,na];for(let i=0;i<t.length;++i){const a=t[i],r=e[i],o=n[i];if(null!=r){if(r===na&&1===a.shape[a.shape.length-1])throw new Tn("You are passing a target array of shape ".concat(a.shape," while using ")+"a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].");if(-1!==s.indexOf(r)){const t=a.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new Tn("A target Tensor with shape ".concat(a.shape," was passed for an ")+"output of shape ".concat(o,", while using a loss function that ")+"expects targets to have the same shape as the output.")}}}}}(e,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=n&&n>0&&t[0].shape[0]%n!==0)throw new Tn("In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size "+"".concat(n,". Found: ").concat(t[0].shape[0]," sample(s)."));return[t,e]}async standardizeUserData(t,e,n,s){let i=!(arguments.length>4&&void 0!==arguments[4])||arguments[4],a=arguments.length>5?arguments[5]:void 0;const[r,o]=this.standardizeUserDataXY(t,e,i,a);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=Ea(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await _a(o[e],null,t[e]))}return[r,o,l]}testLoop(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:0,a=arguments.length>4?arguments[4]:void 0;return i.tidy((()=>{const r=this.checkNumSamples(e,n,a,"steps"),o=[];if(s>0)throw new zn("Verbose mode is not implemented yet.");if(null!=a)throw new zn("steps mode in testLoop() is not implemented yet");{const s=Ha(r,n),a=(0,i.tensor1d)(ys(0,r));for(let n=0;n<s.length;++n){const r=s[n][0],l=s[n][1],u=Is(a,r,l-r),c=Ua(e,u),h=t(c);if(0===n)for(let t=0;t<h.length;++t)o.push((0,i.scalar)(0));for(let t=0;t<h.length;++t){const e=h[t];o[t]=i.add(o[t],i.mul(l-r,e))}}for(let t=0;t<o.length;++t)o[t]=i.div(o[t],r)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(En(t,s)>1){const e=En(t.slice(0,n),s);i+="_".concat(e)}e.push(i)}return e}makeTrainFunction(){return t=>{const e=[],n=t.slice(0,this.inputs.length),s=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),r=[],o=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:n[e]});const o=new Ii(t),l=xi(this.outputs,o,{training:!0});let u;for(let n=0;n<this.lossFunctions.length;++n){let t=(0,this.lossFunctions[n])(s[n],l[n]);null!=a[n]&&(t=Ma(t,a[n]));const r=i.mean(t);e.push(r),u=0===n?t:i.add(u,t)}for(let n=0;n<this.metricsTensors.length;++n){let t;if(this.outputs.length>1&&n<this.outputs.length)t=e[n];else{const e=this.metricsTensors[n][0],a=this.metricsTensors[n][1];t=i.mean(e(s[a],l[a]))}i.keep(t),r.push(t)}return u=i.mean(u),this.calculateLosses().forEach((t=>{u=i.add(u,t)})),u}),!0,o)].concat(r)}}makeTestFunction(){this.testFunction=t=>i.tidy((()=>{const e=[];let n;const s=t.slice(0,this.inputs.length),a=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=[];for(let t=0;t<this.inputs.length;++t)r.push({key:this.inputs[t],value:s[t]});const o=new Ii(r),l=xi(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const s=this.lossFunctions[t],r=i.mean(s(a[t],l[t]));n=0===t?r:i.add(n,r),e.push(n)}for(let t=0;t<this.metricsTensors.length;++t){const n=this.metricsTensors[t][0],s=this.metricsTensors[t][1],r=i.mean(n(a[s],l[s]));e.push(r)}return e}))}async fit(t,e){let n,s,a,r,o,l,u,c,h,p=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};if(this.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");this.isTraining=!0;try{const i=null==p.batchSize?32:p.batchSize;Ba(i);const d=!1,f=await this.standardizeUserData(t,e,p.sampleWeight,p.classWeight,d,i);n=f[0],s=f[1],h=f[2];let m,g=!1;if(null!=p.validationData&&p.validationData.length>0){if(g=!0,2!==p.validationData.length)throw 3===p.validationData.length?new zn("validationData including sample weights is not supported yet."):new Tn("When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; "+"".concat(p.validationData," is invalid."));o=p.validationData[0],l=p.validationData[1];const t=!0,e=await this.standardizeUserData(o,l,null,null,t,i);u=e[0],c=e[1],m=u.concat(c)}else if(null!=p.validationSplit&&p.validationSplit>0&&p.validationSplit<1){g=!0;const t=Math.floor(n[0].shape[0]*(1-p.validationSplit)),e=n[0].shape[0];u=Pa(n,t,e),a=n,n=Pa(n,0,t),c=Pa(s,t,e),r=s,s=Pa(s,0,t),m=u.concat(c)}else null!=p.validationSteps&&(g=!0);const y=n.concat(s).concat(h);this.checkTrainableWeightsConsistency();const b=this.makeTrainFunction(),k=this.getDedupedMetricsNames();let w,v;g?(this.makeTestFunction(),w=this.testFunction,v=k.slice().concat(k.map((t=>"val_"+t)))):(w=null,m=[],v=k.slice());const I=Zi(p.callbacks,p.yieldEvery);return await this.fitLoop(b,y,k,i,p.epochs,p.verbose,I,w,m,p.shuffle,v,p.initialEpoch,null,null)}finally{this.isTraining=!1,ja(n,t),ja(s,e),ja(a,t),ja(r,e),ja(u,o),ja(c,l),null!=h&&i.dispose(h)}}async fitLoop(t,e,n,s,a,r,o,l,u,c,h,p,d,f){null==s&&(s=32),null==a&&(a=1),null==c&&(c=!0),null==p&&(p=0);let m=!1;if(null!=l&&null!=u&&(m=!0),null!=f&&(m=!0,null==d))throw new Tn("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const g=this.checkNumSamples(e,s,d,"steps_per_epoch");let y;null!=g&&(y=ys(0,g)),null==r&&(r=1);const{callbackList:b,history:k}=Ji(o,r,a,p,g,d,s,m,h);b.setModel(this),this.history=k,await b.onTrainBegin(),this.stopTraining_=!1;for(let w=p;w<a;++w){await b.onEpochBegin(w);const a={};if(null!=d)throw new zn("stepsPerEpoch mode is not implemented yet.");{if("batch"===c)throw new zn("batch shuffling is not implemneted yet");c&&i.util.shuffle(y);const r=(0,i.tensor1d)(y),o=Ha(g,s);for(let c=0;c<o.length;++c){const h={};if(await b.onBatchBegin(c,h),i.tidy((()=>{const p=o[c][0],d=o[c][1],f=Is(r,p,d-p);h.batch=c,h.size=d-p;const g=Ua(e,f),y=t(g);for(let t=0;t<n.length;++t){const e=n[t],s=y[t];h[e]=s,i.keep(s)}if(c===o.length-1&&m){const t=this.testLoop(l,u,s);for(let e=0;e<n.length;++e){const s=n[e],r=t[e];i.keep(r),a["val_"+s]=r}}})),await b.onBatchEnd(c,h),Pi(h),this.stopTraining_)break}r.dispose()}if(await b.onEpochEnd(w,a),this.stopTraining_)break}return await b.onTrainEnd(),await this.history.syncData(),this.history}async fitDataset(t,e){return Oa(this,t,e)}async trainOnBatch(t,e){const n=await this.standardizeUserData(t,e),s=n[0],a=n[1],r=this.makeTrainFunction()(s.concat(a)),o=[];for(const i of r){const t=await i.data();o.push(t[0])}return i.dispose(r),ja(n[0],t),ja(n[1],e),_n(o)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let a=0;a<s.length;++a)n&&!s[a].trainable||e.push({name:s[a].originalName,tensor:i[a]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const e=i.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=e-i.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"===typeof this.loss)t=Rn(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!==typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>Rn(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!==typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=Rn(n[s])}}return t}getMetricIdentifiers(){if("string"===typeof this.metrics||"function"===typeof this.metrics)return[Rn(ba(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>Rn(ba(t))));{const t={};for(const e in this.metrics)t[e]=Rn(ba(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Yi(za(t.optimizer_config));let n,s;if("string"===typeof t.loss)n=Ln(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>Ln(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=Ln(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>Ln(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=Ln(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,e){if("string"===typeof t){const e=i.io.getSaveHandlers(t);if(0===e.length)throw new Tn("Cannot find any save handlers for URL '".concat(t,"'"));if(e.length>1)throw new Tn("Found more than one (".concat(e.length,") save handlers for ")+"URL '".concat(t,"'"));t=e[0]}if(null==t.save)throw new Tn("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const n=await i.io.encodeWeights(this.getNamedWeights(e)),s={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v".concat(Fa),convertedBy:null};if(null!=e&&e.includeOptimizer&&null!=this.optimizer){s.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:e,specs:a}=await i.io.encodeWeights(await this.optimizer.getWeights(),t);n.specs.push(...a),n.data=i.io.concatenateArrayBuffers([n.data,e])}if(null!=this.userDefinedMetadata){const t=!0;wa(this.userDefinedMetadata,this.name,t),s.userDefinedMetadata=this.userDefinedMetadata}return s.weightData=n.data,s.weightSpecs=n.specs,t.save(s)}setUserDefinedMetadata(t){wa(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Ja.className="Model",i.serialization.registerClass(Ja);class Ya extends Ja{}async function Xa(t,e){if(null==e&&(e={}),"string"===typeof t){const n=i.io.getLoadHandlers(t,e);if(0===n.length)n.push(i.io.browserHTTPRequest(t,e));else if(n.length>1)throw new Tn("Found more than one (".concat(n.length,") load handlers for ")+"URL '".concat(t,"'"));t=n[0]}return async function(t,e,n){null==n&&(n={});if(null==t.load)throw new Tn("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const s=await t.load();let a=s.modelTopology;null!=a.model_config&&(a=a.model_config);const r=null==n.strict||n.strict,o=null!=s.weightData&&null!=s.weightSpecs&&r,l=Yi(za(a),e,o),u=s.trainingConfig;null!=u&&l.loadTrainingConfig(u);null!=s.userDefinedMetadata&&l.setUserDefinedMetadata(s.userDefinedMetadata);if(null!=s.weightData){if(null==s.weightSpecs)throw new Tn("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const n=i.io.decodeWeights(t,e),s={},a=[];return e.forEach((t=>{"optimizer"===t.group?a.push({name:t.name,tensor:n[t.name]}):s[t.name]=n[t.name]})),{modelWeights:s,optimizerWeights:a}}(s.weightData,s.weightSpecs);l.loadWeights(t,r),null!=l.optimizer&&e.length>0&&await l.optimizer.setWeights(e),(0,i.dispose)(t),(0,i.dispose)(e.map((t=>t.tensor)))}return l}(t,void 0,e)}Ya.className="Functional",i.serialization.registerClass(Ya);class Qa extends Ja{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:Qn("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new Tn("Negative dimension size caused by adding layer "+"".concat(t.name," with input shape [")+"".concat(t.inboundNodes[0].inputTensors[0].shape,"]"))}add(t){const e=t instanceof Qa||t instanceof Ja;let n;if(e){if(n=t,1!==n.outputs.length)throw new Tn("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new Tn("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new Tn("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=vi({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new Tn("A layer added to a Sequential model must not already be "+"connected somewhere else. LayersModel received layer ".concat(t.name," ")+"which has ".concat(t.inboundNodes.length," pre-existing inbound ")+"connections.");if(1!==t.inboundNodes[0].outputTensors.length)throw new Tn("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=ki(this.outputs[0])}this.inboundNodes=[],new gi({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:Dn(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(oi(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Ja({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:console.log;this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};if(!this.built)throw new xn("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new xn("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};if(!this.built)throw new xn("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new xn("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,e){let n,s=arguments.length>3&&void 0!==arguments[3]&&arguments[3],a={};if(e instanceof Array){if(null==e[0].className||"Merge"===e[0].className)throw new Tn("Legacy serialization format not supported yet.");n=e}else i.util.assert(null!=e.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),n=e.layers,delete e.layers,a=e;const r=new t(a);if(!(r instanceof Qa))throw new zn("Sequential.fromConfig called on non-Sequential input: ".concat(r));for(const i of n){const t=Yi(i,void 0,s);s&&t.setFastWeightInitDuringBuild(!0),r.add(t)}return r}set stopTraining(t){if(null==this.model)throw new Tn("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new Tn("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function $a(t){return new Qa(t)}function tr(t){return vi(t)}Qa.className="Sequential",i.serialization.registerClass(Qa);class er extends i.serialization.Serializable{getConfig(){return{}}}class nr extends er{apply(t){return function(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:1;if(1!==e)throw new zn("Support for alpha values other than 1 (".concat(e,") is not implemented ")+"yet.");return i.elu(t)}(t,arguments.length>1&&void 0!==arguments[1]?arguments[1]:1)}}nr.className="elu",i.serialization.registerClass(nr);class sr extends er{apply(t){return i.selu(t)}}sr.className="selu",i.serialization.registerClass(sr);class ir extends er{apply(t){return i.relu(t)}}ir.className="relu",i.serialization.registerClass(ir);class ar extends er{apply(t){return(0,i.tidy)((()=>i.minimum(6,i.relu(t))))}}ar.className="relu6",i.serialization.registerClass(ar);class rr extends er{apply(t){return t}}rr.className="linear",i.serialization.registerClass(rr);class or extends er{apply(t){return i.sigmoid(t)}}or.className="sigmoid",i.serialization.registerClass(or);class lr extends er{apply(t){return function(t){return(0,i.tidy)((()=>{const e=i.add(.5,i.mul(.2,t));return i.clipByValue(e,0,1)}))}(t)}}lr.className="hardSigmoid",i.serialization.registerClass(lr);class ur extends er{apply(t){return i.softplus(t)}}ur.className="softplus",i.serialization.registerClass(ur);class cr extends er{apply(t){return function(t){return(0,i.tidy)((()=>i.div(t,i.add(i.abs(t),1))))}(t)}}cr.className="softsign",i.serialization.registerClass(cr);class hr extends er{apply(t){return i.tanh(t)}}hr.className="tanh",i.serialization.registerClass(hr);class pr extends er{apply(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:-1;return i.softmax(t,e)}}pr.className="softmax",i.serialization.registerClass(pr);class dr extends er{apply(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:-1;return i.logSoftmax(t,e)}}dr.className="logSoftmax",i.serialization.registerClass(dr);class fr extends er{apply(t){return(0,i.tidy)((()=>i.tidy((()=>{const e=Math.sqrt(2),n=i.mul(.5,i.add(1,i.erf(i.div(t,e))));return i.mul(t,n)}))))}}fr.className="gelu",i.serialization.registerClass(fr);class mr extends er{apply(t){return(0,i.tidy)((()=>i.mul(.5,i.mul(t,i.add(1,i.tanh(i.mul(i.sqrt(i.div(2,Math.PI)),i.add(t,i.mul(.044715,i.pow(t,3))))))))))}}mr.className="gelu_new",i.serialization.registerClass(mr);class gr extends er{apply(t){return(0,i.tidy)((()=>i.mul(t,i.tanh(i.softplus(t)))))}}gr.className="mish",i.serialization.registerClass(gr);class yr extends er{apply(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:1;return(0,i.tidy)((()=>i.mul(i.sigmoid(i.mul(t,e)),t)))}}function br(t){return t.getClassName()}function kr(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return Pn(t,i.serialization.SerializationMap.getMap().classNameMap,e,"activation")}function wr(t){if(null==t){const t={className:"linear",config:{}};return kr(t)}if("string"===typeof t){const e={};return e.className=t,e.config={},kr(e)}return t instanceof er?t:kr(t)}function vr(t){if(null!=t&&"object"!==typeof t)throw new Error("Argument to L1L2 regularizer's constructor is expected to be an "+"object, but received: ".concat(t))}yr.className="swish",i.serialization.registerClass(yr);class Ir extends i.serialization.Serializable{}class Nr extends Ir{constructor(t){super(),vr(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return(0,i.tidy)((()=>{let e=(0,i.zeros)([1]);return this.hasL1&&(e=(0,i.add)(e,(0,i.sum)(i.mul(this.l1,(0,i.abs)(t))))),this.hasL2&&(e=(0,i.add)(e,(0,i.sum)(i.mul(this.l2,Cs(t))))),i.reshape(e,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}Nr.className="L1L2",i.serialization.registerClass(Nr);const Sr={l1l2:"L1L2"};function xr(t){return Wn(t)}function Tr(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return Pn(t,i.serialization.SerializationMap.getMap().classNameMap,e,"regularizer")}function zr(t){if(null==t)return null;if("string"===typeof t){return Tr({className:t in Sr?Sr[t]:t,config:{}})}return t instanceof Ir?t:Tr(t)}class Ar extends bi{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=ri(t);let n=(0,i.relu)(t);return null!=this.maxValue&&(n=(0,i.clipByValue)(n,0,this.maxValue)),n}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}Ar.className="ReLU",i.serialization.registerClass(Ar);class Fr extends bi{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=ri(t);return(0,i.leakyRelu)(n,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}Fr.className="LeakyReLU",i.serialization.registerClass(Fr);class Dr extends bi{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=si(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=zr(t.alphaRegularizer),this.alphaConstraint=Wi(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!==typeof t.sharedAxes)throw new Tn("Expected sharedAxes to be a number or an array of numbers, "+"but got ".concat(t.sharedAxes));this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=oi(t)).slice(1);if(null!=this.sharedAxes)for(const s of this.sharedAxes)e[s-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let s=1;s<t.length;++s)n[s]=t[s];this.inputSpec=[new di({ndim:t.length,axes:n})],this.built=!0}call(t,e){return t=ri(t),(0,i.prelu)(t,this.alpha.read())}getConfig(){const t={alphaInitializer:ni(this.alphaInitializer),alphaRegularizer:xr(this.alphaRegularizer),alphaConstraint:Li(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}Dr.className="PReLU",i.serialization.registerClass(Dr);class Cr extends bi{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new zn("Non-default alpha value (".concat(t.alpha,") is not supported by the ")+"ELU layer yet.");this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const n=ri(t);return(0,i.elu)(n)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}Cr.className="ELU",i.serialization.registerClass(Cr);class Er extends bi{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const n=ri(t);return(0,i.mul)(n,(0,i.cast)((0,i.greater)(n,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}Er.className="ThresholdedReLU",i.serialization.registerClass(Er);class _r extends bi{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new pr).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){return(0,i.tidy)((()=>{let n=ri(t);const s=e.mask;if(null!=s){const t=(0,i.mul)((0,i.sub)((0,i.ones)(n.shape),(0,i.cast)(s,n.dtype)),(0,i.scalar)(-1e9));n=(0,i.add)(n,t)}return this.axis instanceof Array?this.axis.length>1?(0,i.exp)((0,i.sub)(n,(0,i.logSumExp)(n,this.axis,!0))):this.softmax(n,this.axis[0]):this.softmax(n,this.axis)}))}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Mr(t,e,n){if("number"===typeof t)return Dn(t,e);if(t.length!==e)throw new Tn("The ".concat(n," argument must be an integer or tuple of ").concat(e," integers.")+" Received: ".concat(t.length," elements."));for(let i=0;i<e;++i){const a=t[i];if((s=a)!==parseInt(s.toString(),10))throw new Tn("The ".concat(n," argument must be an integer or tuple of ").concat(e)+" integers. Received: ".concat(JSON.stringify(t)," including a")+" non-integer number ".concat(a))}return t;var s}function Rr(t,e,n,s){if(null==t)return t;let i;return i="same"===n?t:t-(e+(e-1)*((arguments.length>4&&void 0!==arguments[4]?arguments[4]:1)-1))+1,Math.floor((i+s-1)/s)}function Lr(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+gs([n-e,0]);else{if("same"!==s)throw new Tn("Unsupport padding mode: ".concat(s,"."));t*=e}return t}function Or(t,e){return(0,i.tidy)((()=>(as(e),"channelsFirst"===e?i.transpose(t,[0,2,3,1]):t)))}function Wr(t,e){return(0,i.tidy)((()=>(as(e),"channelsFirst"===e?i.transpose(t,[0,2,3,4,1]):t)))}function Br(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:1,a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:"valid",r=arguments.length>5?arguments[5]:void 0,o=arguments.length>6&&void 0!==arguments[6]?arguments[6]:1;return(0,i.tidy)((()=>{if(null==r&&(r="channelsLast"),as(r),3!==t.shape.length)throw new Tn("The input of a conv1dWithBias operation should be 3, but is "+"".concat(t.shape.length," instead."));if(3!==e.shape.length)throw new Tn("The kernel for a conv1dWithBias operation should be 3, but is "+"".concat(e.shape.length," instead"));if(null!=n&&1!==n.shape.length)throw new Tn("The bias for a conv1dWithBias operation should be 1, but is "+"".concat(n.shape.length," instead"));if("channelsFirst"===r&&(t=i.transpose(t,[0,2,1])),"causal"===a)throw new zn("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let l=i.conv1d(t,e,s,"same"===a?"same":"valid","NWC",o);return null!=n&&(l=_s(l,n)),l}))}function Pr(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:[1,1],a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:"valid",r=arguments.length>5?arguments[5]:void 0,o=arguments.length>6?arguments[6]:void 0,l=arguments.length>7&&void 0!==arguments[7]?arguments[7]:null;return(0,i.tidy)((()=>{if(null==r&&(r="channelsLast"),as(r),3!==t.rank&&4!==t.rank)throw new Tn("conv2dWithBiasActivation expects input to be of rank 3 or 4, "+"but received ".concat(t.rank,"."));if(3!==e.rank&&4!==e.rank)throw new Tn("conv2dWithBiasActivation expects kernel to be of rank 3 or 4, "+"but received ".concat(t.rank,"."));let u=Or(t,r);if("causal"===a)throw new zn("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=i.fused.conv2d({x:u,filter:e,strides:s,pad:"same"===a?"same":"valid",dilations:o,dataFormat:"NHWC",bias:n,activation:l}),"channelsFirst"===r&&(u=i.transpose(u,[0,3,1,2])),u}))}function Ur(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:[1,1,1],a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:"valid",r=arguments.length>5?arguments[5]:void 0,o=arguments.length>6?arguments[6]:void 0;return(0,i.tidy)((()=>{if(null==r&&(r="channelsLast"),as(r),4!==t.rank&&5!==t.rank)throw new Tn("conv3dWithBias expects input to be of rank 4 or 5, but received "+"".concat(t.rank,"."));if(4!==e.rank&&5!==e.rank)throw new Tn("conv3dWithBias expects kernel to be of rank 4 or 5, but received "+"".concat(t.rank,"."));let l=Wr(t,r);if("causal"===a)throw new zn("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return l=i.conv3d(l,e,s,"same"===a?"same":"valid","NDHWC",o),null!=n&&(l=_s(l,n)),"channelsFirst"===r&&(l=i.transpose(l,[0,4,1,2,3])),l}))}_r.className="Softmax",i.serialization.registerClass(_r);class Hr extends bi{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Hr.verifyArgs(e),this.rank=t,qn(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new zn("Convolution layer for rank other than 1, 2, or 3 (".concat(this.rank,") is ")+"not implemented yet.");if(this.kernelSize=Mr(e.kernelSize,t,"kernelSize"),this.strides=Mr(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,rs(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,as(this.dataFormat),this.activation=wr(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=si(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Wi(e.biasConstraint),this.biasRegularizer=zr(e.biasRegularizer),this.activityRegularizer=zr(e.activityRegularizer),this.dilationRate=Mr(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new Tn("dilationRate must be a number or an array of a single number for 1D convolution, but received "+"".concat(JSON.stringify(this.dilationRate)));if(2===this.rank){if("number"===typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new Tn("dilationRate must be a number or array of two numbers for 2D "+"convolution, but received ".concat(JSON.stringify(this.dilationRate)))}else if(3===this.rank)if("number"===typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new Tn("dilationRate must be a number or array of three numbers for 3D "+"convolution, but received ".concat(JSON.stringify(this.dilationRate)))}static verifyArgs(t){if(Cn("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!==typeof t.kernelSize&&!Gn(t.kernelSize,"number",1,3))throw new Tn("BaseConv expects config.kernelSize to be number or number[] with "+"length 1, 2, or 3, but received ".concat(JSON.stringify(t.kernelSize),"."))}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:br(this.activation),useBias:this.useBias,biasInitializer:ni(this.biasInitializer),biasRegularizer:xr(this.biasRegularizer),activityRegularizer:xr(this.activityRegularizer),biasConstraint:Li(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class Vr extends Hr{constructor(t,e){super(t,e),this.kernel=null,Vr.verifyArgs(e),this.filters=e.filters,qn(this.filters,"filters"),this.kernelInitializer=si(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Wi(e.kernelConstraint),this.kernelRegularizer=zr(e.kernelRegularizer)}build(t){t=oi(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Tn("The channel dimension of the input should be defined. "+"Found ".concat(t[e]));const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,e){return(0,i.tidy)((()=>{let e;t=ri(t);const n=null==this.bias?null:this.bias.read(),s=Kn(this.activation.getClassName());if(null!=s&&2===this.rank)e=Pr(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate,s);else{if(1===this.rank)e=Br(t,this.kernel.read(),n,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=Pr(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new zn("convolutions greater than 3D are not implemented yet.");e=Ur(t,this.kernel.read(),n,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=oi(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let i=0;i<n.length;++i){const t=Rr(n[i],this.kernelSize[i],this.padding,this.strides[i],"number"===typeof this.dilationRate?this.dilationRate:this.dilationRate[i]);e.push(t)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:ni(this.kernelInitializer),kernelRegularizer:xr(this.kernelRegularizer),kernelConstraint:Li(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!==typeof t.filters||t.filters<1)throw new Tn("Convolution layer expected config.filters to be a 'number' > 0 "+"but got ".concat(JSON.stringify(t.filters)))}}class jr extends Vr{constructor(t){super(2,t),jr.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!==typeof t.kernelSize&&!Gn(t.kernelSize,"number",1,2))throw new Tn("Conv2D expects config.kernelSize to be number or number[] with "+"length 1 or 2, but received ".concat(JSON.stringify(t.kernelSize),"."))}}jr.className="Conv2D",i.serialization.registerClass(jr);class Gr extends Vr{constructor(t){super(3,t),Gr.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!==typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new Tn("Conv3D expects config.kernelSize to be number or"+" [number, number, number], but received ".concat(JSON.stringify(t.kernelSize),"."))}}Gr.className="Conv3D",i.serialization.registerClass(Gr);class qr extends jr{constructor(t){if(super(t),this.inputSpec=[new di({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new Tn("Conv2DTranspose currently supports only padding modes 'same' "+"and 'valid', but received padding mode ".concat(this.padding))}build(t){if(4!==(t=oi(t)).length)throw new Tn("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Tn("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new di({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,e){return i.tidy((()=>{let e=ri(t);if(4!==e.shape.length)throw new Tn("Conv2DTranspose.call() expects input tensor to be rank-4, but "+"received a tensor of rank-".concat(e.shape.length));const n=e.shape,s=n[0];let a,r;"channelsFirst"===this.dataFormat?(a=2,r=3):(a=1,r=2);const o=n[a],l=n[r],u=this.kernelSize[0],c=this.kernelSize[1],h=this.strides[0],p=this.strides[1],d=[s,Lr(o,h,u,this.padding),Lr(l,p,c,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=i.transpose(e,[0,2,3,1]));let f=i.conv2dTranspose(e,this.kernel.read(),d,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=i.transpose(f,[0,3,1,2])),null!=this.bias&&(f=_s(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f}))}computeOutputShape(t){const e=(t=oi(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const a=this.kernelSize[0],r=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=Lr(e[s],o,a,this.padding),e[i]=Lr(e[i],l,r,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}qr.className="Conv2DTranspose",i.serialization.registerClass(qr);class Zr extends Gr{constructor(t){if(super(t),this.inputSpec=[new di({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new Tn("Conv3DTranspose currently supports only padding modes 'same' "+"and 'valid', but received padding mode ".concat(this.padding))}build(t){if(5!==(t=oi(t)).length)throw new Tn("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new Tn("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new di({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,e){return i.tidy((()=>{let e=ri(t);if(5!==e.shape.length)throw new Tn("Conv3DTranspose.call() expects input tensor to be rank-4, but "+"received a tensor of rank-".concat(e.shape.length));const n=e.shape,s=n[0];let a,r,o;"channelsFirst"===this.dataFormat?(o=2,a=3,r=4):(o=1,a=2,r=3);const l=n[o],u=n[a],c=n[r],h=this.kernelSize[0],p=this.kernelSize[1],d=this.kernelSize[2],f=this.strides[0],m=this.strides[1],g=this.strides[2],y=[s,Lr(l,f,h,this.padding),Lr(u,m,p,this.padding),Lr(c,g,d,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=i.transpose(e,[0,2,3,4,1]));let b=i.conv3dTranspose(e,this.kernel.read(),y,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(b=i.transpose(b,[0,4,1,2,3])),null!==this.bias&&(b=_s(b,this.bias.read(),this.dataFormat)),null!==this.activation&&(b=this.activation.apply(b)),b}))}computeOutputShape(t){const e=(t=oi(t)).slice();let n,s,i,a;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,a=4):(n=4,s=1,i=2,a=3);const r=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],c=this.strides[1],h=this.strides[2];return e[n]=this.filters,e[s]=Lr(e[s],u,r,this.padding),e[i]=Lr(e[i],c,o,this.padding),e[a]=Lr(e[a],h,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Zr.className="Conv3DTranspose",i.serialization.registerClass(Zr);class Kr extends Vr{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new Tn("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new Tn("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new Tn("SeparableConv".concat(this.rank,"D supports only padding modes: ")+"'same' and 'valid', but received ".concat(JSON.stringify(e.padding)));this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=si(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=zr(e.depthwiseRegularizer),this.depthwiseConstraint=Wi(e.depthwiseConstraint),this.pointwiseInitializer=si(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=zr(e.pointwiseRegularizer),this.pointwiseConstraint=Wi(e.pointwiseConstraint)}build(t){if((t=oi(t)).length<this.rank+2)throw new Tn("Inputs to SeparableConv".concat(this.rank,"D should have rank ")+"".concat(this.rank+2,", but received input shape: ")+"".concat(JSON.stringify(t)));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new Tn("The channel dimension of the inputs should be defined, "+"but found ".concat(JSON.stringify(t[e])));const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let r=0;r<this.rank;++r)i.push(1);i.push(n*this.depthMultiplier,this.filters);const a=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,a,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,a,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,a,this.biasConstraint):this.bias=null,this.inputSpec=[new di({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,e){return(0,i.tidy)((()=>{let e;if(t=ri(t),1===this.rank)throw new zn("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=i.transpose(t,[0,2,3,1])),e=i.separableConv2d(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=_s(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=i.transpose(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=ni(this.depthwiseInitializer),t.pointwiseInitializer=ni(this.pointwiseInitializer),t.depthwiseRegularizer=xr(this.depthwiseRegularizer),t.pointwiseRegularizer=xr(this.pointwiseRegularizer),t.depthwiseConstraint=Li(this.depthwiseConstraint),t.pointwiseConstraint=Li(this.pointwiseConstraint),t}}Kr.className="SeparableConv";class Jr extends Kr{constructor(t){super(2,t)}}Jr.className="SeparableConv2D",i.serialization.registerClass(Jr);class Yr extends Vr{constructor(t){super(1,t),Yr.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!==typeof t.kernelSize&&!Gn(t.kernelSize,"number",1,1))throw new Tn("Conv1D expects config.kernelSize to be number or number[] with "+"length 1, but received ".concat(JSON.stringify(t.kernelSize),"."))}}Yr.className="Conv1D",i.serialization.registerClass(Yr);class Xr extends bi{constructor(t){super(t),"number"===typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"===typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return(0,i.tidy)((()=>{if(t=ri(t),"channelsLast"===this.dataFormat){const e=Ss(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return Ss(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=Ss(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return Ss(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Xr.className="Cropping2D",i.serialization.registerClass(Xr);class Qr extends bi{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,as(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,jn(ts,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,e){return i.tidy((()=>{let e=ri(t);const n=e.shape;if("channelsFirst"===this.dataFormat){e=i.transpose(e,[0,2,3,1]);const t=this.size[0]*n[2],s=this.size[1]*n[3],a="nearest"===this.interpolation?i.image.resizeNearestNeighbor(e,[t,s]):i.image.resizeBilinear(e,[t,s]);return i.transpose(a,[0,3,1,2])}{const t=this.size[0]*n[1],s=this.size[1]*n[2];return"nearest"===this.interpolation?i.image.resizeNearestNeighbor(e,[t,s]):i.image.resizeBilinear(e,[t,s])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}}Qr.className="UpSampling2D",i.serialization.registerClass(Qr);class $r extends Hr{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=si(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Wi(t.depthwiseConstraint),this.depthwiseRegularizer=zr(t.depthwiseRegularizer)}build(t){if((t=oi(t)).length<4)throw new Tn("Inputs to DepthwiseConv2D should have rank 4. "+"Received input shape: ".concat(JSON.stringify(t),"."));const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new Tn("The channel dimension of the inputs to DepthwiseConv2D should "+"be defined, but is not (".concat(t[e],")."));const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.tidy)((()=>{let e=function(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:[1,1],s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"valid",a=arguments.length>4?arguments[4]:void 0,r=arguments.length>5?arguments[5]:void 0;return(0,i.tidy)((()=>{null==a&&(a="channelsLast"),as(a);let o=Or(t,a);if(4!==t.rank)throw new Tn("Input for depthwiseConv2d is required to be 4-D, but is instead "+"".concat(t.rank,"-D"));if(4!==e.rank)throw new Tn("depthwiseKernel is required to be 4-D, but is instead "+"".concat(e.rank,"-D"));return o=i.depthwiseConv2d(o,e,n,"same"===s?"same":"valid","NHWC",r),"channelsFirst"===a&&(o=i.transpose(o,[0,3,1,2])),o}))}(t=ri(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=_s(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=oi(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=Rr(e,this.kernelSize[0],this.padding,this.strides[0]),a=Rr(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,a]:[t[0],i,a,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=ni(this.depthwiseInitializer),t.depthwiseRegularizer=xr(this.depthwiseRegularizer),t.depthwiseConstraint=Li(this.depthwiseRegularizer),t}}function to(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new Tn("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function eo(t,e,n){let s=arguments.length>3&&void 0!==arguments[3]&&arguments[3],a=arguments.length>4?arguments[4]:void 0,r=arguments.length>5?arguments[5]:void 0,o=arguments.length>6&&void 0!==arguments[6]&&arguments[6],l=arguments.length>7&&void 0!==arguments[7]&&arguments[7];return i.tidy((()=>{const u=e.shape.length;if(u<3)throw new Tn("Input should be at least 3D, but is ".concat(u,"D."));const c=[1,0].concat(ys(2,u));if(e=i.transpose(e,c),null!=r)throw new zn("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=a&&(a=i.cast(i.cast(a,"bool"),"float32"),a.rank===u-1&&(a=i.expandDims(a,-1)),a=i.transpose(a,c)),s&&(e=i.reverse(e,0),null!=a&&(a=i.reverse(a,0)));const h=[];let p,d=n;const f=e.shape[0],m=i.unstack(e);let g,y;null!=a&&(g=i.unstack(a));for(let e=0;e<f;++e){const n=m[e],s=i.tidy((()=>t(n,d)));if(null==a)p=s[0],d=s[1];else{const t=i.tidy((()=>{const t=g[e],n=i.sub(i.onesLike(t),t);return{output:i.add(i.mul(s[0],t),i.mul(d[0],n)),newStates:d.map(((e,a)=>i.add(i.mul(s[1][a],t),i.mul(e,n))))}}));p=t.output,d=t.newStates}l&&h.push(p)}if(l){const t=1;y=i.stack(h,t)}return[p,y,d]}))}$r.className="DepthwiseConv2D",i.serialization.registerClass($r);class no extends bi{constructor(t){let e;if(super(t),null==t.cell)throw new Tn("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new co({cells:t.cell}):t.cell,null==e.stateSize)throw new Tn("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new di({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return ys(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){ii(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,e){return i.tidy((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new zn("Constants support is not implemented in RNN yet.");ii(t)&&(t=t[0]);const e=this.stateful?t[0]:null,n=t.slice(2);this.inputSpec[0]=new di({shape:[e,null,...n]});const s=[t[0]].concat(t.slice(2));let a;if(this.cell.build(s),a=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!i.util.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),a))throw new Tn("An initialState was passed that is not compatible with "+"cell.stateSize. Received stateSpec=".concat(this.stateSpec,"; ")+"However cell.stateSize is ".concat(this.cell.stateSize))}else this.stateSpec=a.map((t=>new di({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t){let e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];(0,i.tidy)((()=>{if(!this.stateful)throw new Sn("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape[0];if(null==n)throw new Tn("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>i.zeros([n,t]))):this.states_=[i.zeros([n,this.cell.stateSize])];else if(null==t)i.dispose(this.states_),null!=this.keptStates&&(i.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>i.zeros([n,t]))):this.states_[0]=i.zeros([n,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Tn("Layer ".concat(this.name," expects ").concat(this.states_.length," state(s), ")+"but it received ".concat(t.length," state value(s). Input ")+"received: ".concat(t));!0===e?this.keptStates.push(this.states_.slice()):i.dispose(this.states_);for(let e=0;e<this.states_.length;++e){const s=t[e],a=Array.isArray(this.cell.stateSize)?this.cell.stateSize[e]:this.cell.stateSize,r=[n,a];if(!i.util.arraysEqual(s.shape,r))throw new Tn("State ".concat(e," is incompatible with layer ").concat(this.name,": ")+"expected shape=".concat(r,", received shape=").concat(s.shape));this.states_[e]=s}}this.states_=this.states_.map((t=>i.keep(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=to(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let a=[],r=[];if(null!=n){e.initialState=n,a=a.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new di({shape:t.shape}));r=r.concat(this.stateSpec)}null!=s&&(e.constants=s,a=a.concat(s),this.numConstants=s.length);if(a[0]instanceof fi){const n=[t].concat(a),s=this.inputSpec.concat(r),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return(0,i.tidy)((()=>{const n=null==e?null:e.mask,s=null==e?null:e.training;let i=null==e?null:e.initialState;t=ri(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const a=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==a)throw new Tn("RNN Layer has ".concat(a," state(s) but was passed ")+"".concat(i.length," initial state(s)."));this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const r={training:s},o=eo(((t,e)=>{const n=this.cell.call([t].concat(e),r);return[n[0],n.slice(1)]}),t,i,this.goBackwards,n,null,this.unroll,this.returnSequences),l=o[0],u=o[1],c=o[2];this.stateful&&this.resetStates(c,s);const h=this.returnSequences?u:l;return this.returnState?[h].concat(c):h}))}getInitialState(t){return(0,i.tidy)((()=>{let e=i.zeros(t.shape);return e=i.sum(e,[1,2]),e=vs(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?zs(e,[1,t]):e)):this.cell.stateSize>1?[zs(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===no.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign(Object.assign(Object.assign({},n),t),e)}static fromConfig(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const s=Yi(e.cell,n);return new t(Object.assign(e,{cell:s}))}}no.className="RNN",i.serialization.registerClass(no);class so extends bi{}class io extends so{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,qn(this.units,"units"),this.activation=wr(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=si(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=si(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=si(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=zr(t.kernelRegularizer),this.recurrentRegularizer=zr(t.recurrentRegularizer),this.biasRegularizer=zr(t.biasRegularizer),this.kernelConstraint=Wi(t.kernelConstraint),this.recurrentConstraint=Wi(t.recurrentConstraint),this.biasConstraint=Wi(t.biasConstraint),this.dropout=ms([1,gs([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=ms([1,gs([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=oi(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.tidy)((()=>{if(2!==t.length)throw new Tn("SimpleRNNCell expects 2 input Tensors, got ".concat(t.length,"."));let n=t[1];t=t[0];const s=null!=e.training&&e.training;let a;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ho({ones:()=>i.onesLike(t),rate:this.dropout,training:s,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ho({ones:()=>i.onesLike(n),rate:this.recurrentDropout,training:s,dropoutFunc:this.dropoutFunc}));const r=this.dropoutMask,o=this.recurrentDropoutMask;a=Fs(null!=r?i.mul(t,r):t,this.kernel.read()),null!=this.bias&&(a=_s(a,this.bias.read())),null!=o&&(n=i.mul(n,o));let l=i.add(a,Fs(n,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:br(this.activation),useBias:this.useBias,kernelInitializer:ni(this.kernelInitializer),recurrentInitializer:ni(this.recurrentInitializer),biasInitializer:ni(this.biasInitializer),kernelRegularizer:xr(this.kernelRegularizer),recurrentRegularizer:xr(this.recurrentRegularizer),biasRegularizer:xr(this.biasRegularizer),activityRegularizer:xr(this.activityRegularizer),kernelConstraint:Li(this.kernelConstraint),recurrentConstraint:Li(this.recurrentConstraint),biasConstraint:Li(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign(Object.assign({},t),e)}}io.className="SimpleRNNCell",i.serialization.registerClass(io);class ao extends no{constructor(t){t.cell=new io(t),super(t)}call(t,e){return(0,i.tidy)((()=>{null!=this.cell.dropoutMask&&(i.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,a=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:a})}))}static fromConfig(t,e){return new t(e)}}ao.className="SimpleRNN",i.serialization.registerClass(ao);class ro extends so{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new Tn("GRUCell does not support reset_after parameter set to true.");this.units=t.units,qn(this.units,"units"),this.activation=wr(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=wr(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=si(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=si(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=si(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=zr(t.kernelRegularizer),this.recurrentRegularizer=zr(t.recurrentRegularizer),this.biasRegularizer=zr(t.biasRegularizer),this.kernelConstraint=Wi(t.kernelConstraint),this.recurrentConstraint=Wi(t.recurrentConstraint),this.biasConstraint=Wi(t.biasConstraint),this.dropout=ms([1,gs([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=ms([1,gs([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=oi(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,i.tidy)((()=>{if(2!==t.length)throw new Tn("GRUCell expects 2 input Tensors (inputs, h, c), got "+"".concat(t.length,"."));const n=null!=e.training&&e.training;let s=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ho({ones:()=>i.onesLike(t),rate:this.dropout,training:n,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ho({ones:()=>i.onesLike(s),rate:this.recurrentDropout,training:n,count:3,dropoutFunc:this.dropoutFunc}));const a=this.dropoutMask,r=this.recurrentDropoutMask;let o,l,u;0<this.dropout&&this.dropout<1&&(t=i.mul(t,a[0]));let c=Fs(t,this.kernel.read());this.useBias&&(c=_s(c,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(s=i.mul(s,r[0]));const h=this.recurrentKernel.read(),[p,d]=i.split(h,[2*this.units,this.units],h.rank-1),f=Fs(s,p),[m,g,y]=i.split(c,3,c.rank-1),[b,k]=i.split(f,2,f.rank-1);o=this.recurrentActivation.apply(i.add(m,b)),l=this.recurrentActivation.apply(i.add(g,k));const w=Fs(i.mul(l,s),d);u=this.activation.apply(i.add(y,w));const v=i.add(i.mul(o,s),i.mul(i.add(1,i.neg(o)),u));return[v,v]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:br(this.activation),recurrentActivation:br(this.recurrentActivation),useBias:this.useBias,kernelInitializer:ni(this.kernelInitializer),recurrentInitializer:ni(this.recurrentInitializer),biasInitializer:ni(this.biasInitializer),kernelRegularizer:xr(this.kernelRegularizer),recurrentRegularizer:xr(this.recurrentRegularizer),biasRegularizer:xr(this.biasRegularizer),activityRegularizer:xr(this.activityRegularizer),kernelConstraint:Li(this.kernelConstraint),recurrentConstraint:Li(this.recurrentConstraint),biasConstraint:Li(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign(Object.assign({},t),e)}}ro.className="GRUCell",i.serialization.registerClass(ro);class oo extends no{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new ro(t),super(t)}call(t,e){return(0,i.tidy)((()=>{null!=this.cell.dropoutMask&&(i.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,a=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:a})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}oo.className="GRU",i.serialization.registerClass(oo);class lo extends so{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,qn(this.units,"units"),this.activation=wr(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=wr(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=si(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=si(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=si(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=zr(t.kernelRegularizer),this.recurrentRegularizer=zr(t.recurrentRegularizer),this.biasRegularizer=zr(t.biasRegularizer),this.kernelConstraint=Wi(t.kernelConstraint),this.recurrentConstraint=Wi(t.recurrentConstraint),this.biasConstraint=Wi(t.biasConstraint),this.dropout=ms([1,gs([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=ms([1,gs([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=oi(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends Ws{apply(e,s){const i=t.apply([n]),a=(new Ps).apply([n]),r=t.apply([2*n]);return Ts(Ts(i,a),r)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,i.tidy)((()=>{const n=null!=e.training&&e.training;if(3!==t.length)throw new Tn("LSTMCell expects 3 input Tensors (inputs, h, c), got "+"".concat(t.length,"."));let s=t[1];const a=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ho({ones:()=>i.onesLike(t),rate:this.dropout,training:n,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ho({ones:()=>i.onesLike(s),rate:this.recurrentDropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const r=this.dropoutMask,o=this.recurrentDropoutMask;let l,u,c,h;0<this.dropout&&this.dropout<1&&(t=i.mul(t,r[0]));let p=Fs(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(s=i.mul(s,o[0])),p=i.add(p,Fs(s,this.recurrentKernel.read())),this.useBias&&(p=_s(p,this.bias.read()));const[d,f,m,g]=i.split(p,4,p.rank-1);l=this.recurrentActivation.apply(d),u=this.recurrentActivation.apply(f),c=i.add(i.mul(u,a),i.mul(l,this.activation.apply(m))),h=this.recurrentActivation.apply(g);const y=i.mul(h,this.activation.apply(c));return[y,y,c]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:br(this.activation),recurrentActivation:br(this.recurrentActivation),useBias:this.useBias,kernelInitializer:ni(this.kernelInitializer),recurrentInitializer:ni(this.recurrentInitializer),biasInitializer:ni(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:xr(this.kernelRegularizer),recurrentRegularizer:xr(this.recurrentRegularizer),biasRegularizer:xr(this.biasRegularizer),activityRegularizer:xr(this.activityRegularizer),kernelConstraint:Li(this.kernelConstraint),recurrentConstraint:Li(this.recurrentConstraint),biasConstraint:Li(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign(Object.assign({},t),e)}}lo.className="LSTMCell",i.serialization.registerClass(lo);class uo extends no{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new lo(t),super(t)}call(t,e){return(0,i.tidy)((()=>{null!=this.cell.dropoutMask&&(i.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const n=null==e?null:e.mask,s=null==e?null:e.training,a=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:a})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}uo.className="LSTM",i.serialization.registerClass(uo);class co extends so{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,i.tidy)((()=>{let n=t.slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(n.splice(0,t.stateSize.length)):s.push(n.splice(0,1));s.reverse();const i=[];let a;for(let r=0;r<this.cells.length;++r){const o=this.cells[r];n=s[r],a=0===r?[t[0]].concat(n):[a[0]].concat(n),a=o.call(a,e),i.push(a.slice(1))}n=[];for(const t of i.slice().reverse())n.push(...t);return[a[0]].concat(n)}))}build(t){let e;ii(t)&&(t=t[0]),this.cells.forEach(((n,s)=>{us("RNNCell_".concat(s),(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign(Object.assign({},t),e)}static fromConfig(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const s=[];for(const i of e.cells)s.push(Yi(i,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return hi(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}pi(e)}}function ho(t){const{ones:e,rate:n,training:s=!1,count:a=1,dropoutFunc:r}=t,o=()=>null!=r?r(e(),n):Ms(e(),n),l=()=>Rs(o,e,s);if(!a||a<=1)return i.keep(l().clone());return Array(a).fill(void 0).map(l).map((t=>i.keep(t.clone())))}co.className="StackedRNNCells",i.serialization.registerClass(co);var po=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"===typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n};class fo extends no{constructor(t){if(t.unroll)throw new zn("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new zn("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new di({ndim:5})]}call(t,e){return i.tidy((()=>{if(null!=this.cell.dropoutMask&&(i.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(i.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new Tn("ConvRNN2D cell does not support constants");const n=null==e?null:e.mask,s=null==e?null:e.training,a=null==e?null:e.initialState;return super.call(t,{mask:n,training:s,initialState:a})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return i.tidy((()=>{const{stateSize:e}=this.cell,n=t.shape,s=this.computeSingleOutputShape(n),a=[s[0],...s.slice(2)],r=i.zeros(a);return Array.isArray(e)?Array(e.length).fill(r):[r]}))}resetStates(t){let e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];i.tidy((()=>{if(!this.stateful)throw new Sn("Cannot call resetStates() on an RNN Layer that is not stateful.");const n=this.inputSpec[0].shape,s=this.computeSingleOutputShape(n),a=[s[0],...s.slice(2)];if(null==n[0])throw new Tn("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>i.zeros(a))):this.states_=[i.zeros(a)];else if(null==t)i.dispose(this.states_),null!=this.keptStates&&(i.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>i.zeros(a))):this.states_[0]=i.zeros(a);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new Tn("Layer ".concat(this.name," expects ").concat(this.states_.length," state(s), ")+"but it received ".concat(t.length," state value(s). Input ")+"received: ".concat(t));e?this.keptStates.push(this.states_.slice()):i.dispose(this.states_);for(let e=0;e<this.states_.length;++e){const n=t[e],s=a;if(!i.util.arraysEqual(n.shape,s))throw new Tn("State ".concat(e," is incompatible with layer ").concat(this.name,": ")+"expected shape=".concat(s,", received shape=").concat(n.shape));this.states_[e]=n}}this.states_=this.states_.map((t=>i.keep(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:a,dilationRate:r}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],c=Rr(l,s[0],i,a[0],r[0]),h=Rr(u,s[1],i,a[1],r[1]);return[...t.slice(0,2),...o?[n,c,h]:[c,h,n]]}}fo.className="ConvRNN2D";class mo extends lo{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:a,dilationRate:r}=t;super(Object.assign(Object.assign({},t),{units:e})),this.filters=e,qn(this.filters,"filters"),this.kernelSize=Mr(n,2,"kernelSize"),this.kernelSize.forEach((t=>qn(t,"kernelSize"))),this.strides=Mr(s||1,2,"strides"),this.strides.forEach((t=>qn(t,"strides"))),this.padding=i||"valid",rs(this.padding),this.dataFormat=a||"channelsLast",as(this.dataFormat),this.dilationRate=Mr(r||1,2,"dilationRate"),this.dilationRate.forEach((t=>qn(t,"dilationRate")))}build(t){var e;t=oi(t);const n="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[n])throw new Tn("The channel dimension of the input should be defined. "+"Found ".concat(t[n]));const s=t[n],a=this.kernelSize.concat([s,4*this.filters]);this.kernel=this.addWeight("kernel",a,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const r=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",r,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const n=this.biasInitializer,s=this.filters;t=new((e=class extends Ws{apply(t,e){return xs([n.apply([s]),i.ones([s]),n.apply([2*s])])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return i.tidy((()=>{if(3!==t.length)throw new Tn("ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got "+"".concat(t.length,"."));const n=e.training||!1,s=t[0],a=t[1],r=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ho({ones:()=>i.onesLike(s),rate:this.dropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const o=this.dropoutMask,l=(t,e,n)=>e&&e[n]?i.mul(e[n],t):t;let u=l(s,o,0),c=l(s,o,1),h=l(s,o,2),p=l(s,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ho({ones:()=>i.onesLike(a),rate:this.recurrentDropout,training:n,count:4,dropoutFunc:this.dropoutFunc}));const d=this.recurrentDropoutMask;let f=l(a,d,0),m=l(a,d,1),g=l(a,d,2),y=l(a,d,3);const[b,k,w,v]=i.split(this.kernel.read(),4,3),[I,N,S,x]=this.useBias?i.split(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,b,I,this.padding),c=this.inputConv(c,k,N,this.padding),h=this.inputConv(h,w,S,this.padding),p=this.inputConv(p,v,x,this.padding);const[T,z,A,F]=i.split(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,T),m=this.recurrentConv(m,z),g=this.recurrentConv(g,A),y=this.recurrentConv(y,F);const D=this.recurrentActivation.apply(i.add(u,f)),C=this.recurrentActivation.apply(i.add(c,m)),E=i.add(i.mul(C,r),i.mul(D,this.activation.apply(i.add(h,g)))),_=i.mul(this.recurrentActivation.apply(i.add(p,y)),this.activation.apply(E));return[_,_,E]}))}getConfig(){const t=super.getConfig(),{units:e}=t,n=po(t,["units"]),s={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign(Object.assign({},n),s)}inputConv(t,e,n,s){const a=i.conv2d(t,e,this.strides,s||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return n?_s(a,n,this.dataFormat):a}recurrentConv(t,e){return i.conv2d(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}mo.className="ConvLSTM2DCell",i.serialization.registerClass(mo);class go extends fo{constructor(t){const e=new mo(t);super(Object.assign(Object.assign({},t),{cell:e}))}static fromConfig(t,e){return new t(e)}}go.className="ConvLSTM2D",i.serialization.registerClass(go);class yo extends bi{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let s=0;s<this.noiseShape.length;++s)n.push(null==this.noiseShape[s]?e[s]:this.noiseShape[s]);return n}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(n);return Rs((()=>Ms(n,this.rate,s,this.seed)),(()=>n),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}yo.className="Dropout",i.serialization.registerClass(yo);class bo extends yo{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}bo.className="SpatialDropout1D",i.serialization.registerClass(bo);class ko extends bi{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,qn(this.units,"units"),this.activation=wr(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=si(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=si(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Wi(t.kernelConstraint),this.biasConstraint=Wi(t.biasConstraint),this.kernelRegularizer=zr(t.kernelRegularizer),this.biasRegularizer=zr(t.biasRegularizer),this.activityRegularizer=zr(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=oi(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=oi(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t),s=Kn(this.activation.getClassName());let i;return null!=s?i=Fs(n,this.kernel.read(),s,this.bias?this.bias.read():null):(i=Fs(n,this.kernel.read()),null!=this.bias&&(i=_s(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:br(this.activation),useBias:this.useBias,kernelInitializer:ni(this.kernelInitializer),biasInitializer:ni(this.biasInitializer),kernelRegularizer:xr(this.kernelRegularizer),biasRegularizer:xr(this.biasRegularizer),activityRegularizer:xr(this.activityRegularizer),kernelConstraint:Li(this.kernelConstraint),biasConstraint:Li(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}ko.className="Dense",i.serialization.registerClass(ko);class wo extends bi{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=oi(t);for(const e of t.slice(1))if(null==e)throw new Tn('The shape of the input to "Flatten" is not fully defined '+"(got ".concat(t.slice(1),"). Make sure to pass a complete ")+'"input_shape" or "batch_input_shape" argument to the first layer in your model.');return[t[0],fs(t,1)]}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);let n=ri(t);if("channelsFirst"===this.dataFormat&&n.rank>1){const t=[0];for(let e=2;e<n.rank;++e)t.push(e);t.push(1),n=(0,i.transpose)(n,t)}return function(t){if(t.rank<=1)throw new Tn("batchFlatten requires a minimum rank of 2. Got rank: ".concat(t.rank,"."));const e=[t.shape[0],fs(t.shape,1)];return i.reshape(t,e)}(n)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}wo.className="Flatten",i.serialization.registerClass(wo);class vo extends bi{constructor(t){super(t),this.supportsMasking=!0,this.activation=wr(t.activation)}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t);return this.activation.apply(n)}))}getConfig(){const t={activation:br(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}vo.className="Activation",i.serialization.registerClass(vo);class Io extends bi{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return(0,i.tidy)((()=>{return t=ri(t),e=t,n=this.n,(0,i.tidy)((()=>{if(2!==e.shape.length)throw new Tn("repeat() expects a rank-2 tensor, but received a "+"rank-".concat(e.shape.length," tensor."));return zs(vs(e,1),[1,n,1])}));var e,n}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}Io.className="RepeatVector",i.serialization.registerClass(Io);class No extends bi{constructor(t){super(t),this.targetShape=t.targetShape;for(let e=0;e<this.targetShape.length;++e)this.isUnknown(this.targetShape[e])&&(this.targetShape[e]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,a=null;for(let o=0;o<s.length;++o){const t=s[o];if(this.isUnknown(t)){if(null!==a)throw new Tn("Can only specifiy one unknown dimension.");a=o}else i*=t}const r=fs(t);if(null!==a){if(0===i||r%i!==0)throw new Tn(n);s[a]=r/i}else if(r!==i)throw new Tn(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t),s=n.shape,a=s.slice(0,1).concat(this.fixUnknownDimension(s.slice(1),this.targetShape));return(0,i.reshape)(n,a)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}No.className="Reshape",i.serialization.registerClass(No);class So extends bi{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error("Permute constructor requires `dims` to be an Array, but received "+"".concat(t.dims," instead."));const e=ys(1,t.dims.length+1);if(!i.util.arraysEqual(t.dims.slice().sort(),e))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new di({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=oi(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,e){return(0,i.transpose)(ri(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}So.className="Permute",i.serialization.registerClass(So);class xo extends bi{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const n=ri(t);return(0,i.any)((0,i.notEqual)(n,this.maskValue),-1)}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t),s=(0,i.any)((0,i.notEqual)(n,this.maskValue),-1,!0);return(0,i.mul)(n,(0,i.cast)(s,n.dtype))}))}}xo.className="Masking",i.serialization.registerClass(xo);class To extends bi{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(Mn(t.inputLength))}this.inputDim=t.inputDim,qn(this.inputDim,"inputDim"),this.outputDim=t.outputDim,qn(this.outputDim,"outputDim"),this.embeddingsInitializer=si(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=zr(t.embeddingsRegularizer),this.activityRegularizer=zr(t.activityRegularizer),this.embeddingsConstraint=Wi(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return(0,i.tidy)((()=>this.maskZero?(t=ri(t),(0,i.notEqual)(t,(0,i.zerosLike)(t))):null))}computeOutputShape(t){if(t=oi(t),null==this.inputLength)return[...t,this.outputDim];const e=Mn(this.inputLength);if(e.length!==t.length-1)throw new Tn('"inputLength" is '.concat(this.inputLength,", but received ")+"input shape has shape ".concat(t));{let n=0;for(let s=0;s<e.length;++s){const i=e[s],a=t[s+1];if(null!=i&&null!=a&&i!==a)throw new Tn('"inputLength" is '.concat(this.inputLength,", but received ")+"input shape has shape ".concat(t));null==i&&(e[n]=a),n++}}return[t[0],...e,this.outputDim]}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);let n=ri(t);"int32"!==n.dtype&&(n=ws(n,"int32"));const s=Ds(this.embeddings.read(),(0,i.reshape)(n,[n.size]));return(0,i.reshape)(s,oi(this.computeOutputShape(n.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:ni(this.embeddingsInitializer),embeddingsRegularizer:xr(this.embeddingsRegularizer),activityRegularizer:xr(this.activityRegularizer),embeddingsConstraint:Li(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}To.className="Embedding",i.serialization.registerClass(To);class zo extends bi{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new zn}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],a=e[s];if(null==i||null==a||i<0||a<0)n.push(null);else if(1===i)n.push(a);else if(1===a)n.push(i);else{if(i!==a)throw new Tn("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[oi(t)]),t.length<2)throw new Tn("A merge layer should be called on an Array of at least 2 inputs."+" Got ".concat(t.length," input(s)."));let e=[];for(const i of t)null!=i&&null!==i[0]&&e.push(i[0]);if(e=Hn(e),e.length>1)throw new Tn("Can not merge tensors with different batch sizes. "+"Got tensors with shapes: ".concat(JSON.stringify(t),"."));let n=null==t[0]?null:t[0].slice(1);for(let i=1;i<t.length;++i){const e=null==t[i]?null:t[i].slice(1);n=this.computeElementwiseOpOutputShape(n,e)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===Hn(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return(0,i.tidy)((()=>{if(this.reshapeRequired){const e=[],n=t.map((t=>t.rank));if(-1===n.indexOf(null)){const s=gs(n);for(let n of t){const t=n.rank;for(let e=0;e<s-t;++e)n=vs(n,1);e.push(n)}return this.mergeFunction(e)}{let n=!1;for(const r of t){const t=r.rank;if(null==t){const t=r.shape,s=t[0],a=t.slice(1).concat([s]);let o=i.reshape(r,[s].concat(fs(t.slice(1))));o=i.transpose(o,[1,0]),o=i.reshape(o,a),e.push(o),n=!0}else if(t>1){const s=ys(1,t).concat([0]);e.push(i.transpose(r,s)),n=!0}else e.push(r)}let s=this.mergeFunction(e);const a=s.rank;if(n)if(null==a){const t=s.shape,e=t[t.length-1],n=[e].concat(t.slice(0,t.length-1));s=i.reshape(i.transpose(i.reshape(s,[-1,e]),[1,0]),n)}else if(a>1){const t=[a-1].concat(ys(0,a-1));s=i.transpose(s,t)}return s}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==t[0]?null:t[0].slice(1);for(let s=1;s<t.length;++s){const n=null==t[s]?null:t[s].slice(1);e=this.computeElementwiseOpOutputShape(e,n)}let n=[];for(const s of t)null!=s&&null!==s[0]&&n.push(s[0]);return n=Hn(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,e){return i.tidy((()=>{if(null==e)return null;if(!Array.isArray(e))throw new Tn("`mask` should be an Array");if(!Array.isArray(t))throw new Tn("`inputs` should be an Array");if(e.length!==t.length)throw new Tn("The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths "+"(".concat(t.length," vs ").concat(e.length,")"));if(e.every((t=>null==t)))return null;let n=(e=e.map((t=>null==t?t:i.expandDims(t,0))))[0];for(let t=1;t<e.length-1;++t)n=i.logicalAnd(n,e[t]);return n}))}}class Ao extends zo{constructor(t){super(t)}mergeFunction(t){return(0,i.tidy)((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=i.add(e,t[n]);return e}))}}Ao.className="Add",i.serialization.registerClass(Ao);class Fo extends zo{constructor(t){super(t)}mergeFunction(t){return(0,i.tidy)((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=i.mul(e,t[n]);return e}))}}Fo.className="Multiply",i.serialization.registerClass(Fo);class Do extends zo{constructor(t){super(t)}mergeFunction(t){return(0,i.tidy)((()=>{let e=t[0].clone();for(let n=1;n<t.length;++n)e=i.add(e,t[n]);return i.mul(1/t.length,e)}))}}Do.className="Average",i.serialization.registerClass(Do);class Co extends zo{constructor(t){super(t)}mergeFunction(t){return(0,i.tidy)((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=i.maximum(e,t[n]);return e}))}}Co.className="Maximum",i.serialization.registerClass(Co);class Eo extends zo{constructor(t){super(t)}mergeFunction(t){return(0,i.tidy)((()=>{let e=t[0];for(let n=1;n<t.length;++n)e=i.minimum(e,t[n]);return e}))}}Eo.className="Minimum",i.serialization.registerClass(Eo);class _o extends zo{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new Tn("A `Concatenate` layer should be called on a list of at least 2 inputs");let e=!0;for(const s of t)if(null!=s){e=!1;break}if(e)return;const n=[];for(let s=0;s<t.length;++s){const e=t[s].slice();e.splice(this.axis,1);let a=!1;for(const t of n)if(i.util.arraysEqual(t,e)){a=!0;break}a||n.push(e)}if(n.length>1)throw new Tn("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return(0,i.tidy)((()=>xs(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new Tn("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const i of e.slice(1)){if(null==n[s]||null==i[s]){n[s]=null;break}n[s]+=i[s]}return n}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new Tn("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new Tn("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new Tn("Mismatch in the length of mask (".concat(e.length,") ")+"and the legnth of inputs (".concat(t.length,")"));return i.tidy((()=>{let n=!0;if(e.forEach((t=>{null==t||(n=!1)})),n)return null;const s=[];for(let r=0;r<t.length;++r)null==e[r]?s.push(i.cast(i.onesLike(t[r]),"bool")):e[r].rank<t[r].rank?s.push(i.expandDims(e[r],-1)):s.push(e[r]);const a=i.concat(s,this.axis);return i.all(a,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Mo(t,e){for(;t<0;)t+=e;return t}_o.className="Concatenate",i.serialization.registerClass(_o);class Ro extends zo{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){i.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0],n=t[1];if(e.length>3||n.length>3)throw new zn("Dot layer does not support tensors of 4D or higher rank yet.");const s=this.interpretAxes(e,n);if(e[s[0]]!==n[s[1]])throw new Tn("Dimension incompatibility: "+"".concat(e[s[0]]," !== ").concat(n[s[1]]))}mergeFunction(t){if(2!==t.length)throw new Tn("A `Dot` layer must be called on exactly 2 inputs, "+"but received ".concat(t.length," input(s)."));let e,n=t[0],s=t[1];return e=Array.isArray(this.axes)?this.axes.map(((e,n)=>Mo(e,t[n].shape.length))):[Mo(this.axes,n.shape.length),Mo(this.axes,s.shape.length)],this.normalize&&(n=Xi(n,e[0]),s=Xi(s,e[1])),function(t,e,n){if(t.shape.length>3||e.shape.length>3)throw new zn("batchDot is not implemented for tensors of 4D or higher rank yet");if(i.util.assert(t.shape.length>=2,(()=>"batchDot requires the rank of x to be >= 2, "+"but got ".concat(t.shape.length))),i.util.assert(t.shape.length>=2,(()=>"batchDot requires the rank of y to be >= 2, "+"but got ".concat(e.shape.length))),"number"===typeof n&&(n=[n,n]),"complex64"===t.dtype||"complex64"===e.dtype)throw new zn("batchDot is not implemented for complex64-type Tensors yet.");const s=t.shape.length,a=e.shape.length;null==n&&(n=[s-1,a-2]);const r=n;return i.tidy((()=>{let n,o;if(s>a){n=s-a;const t=[];for(let e=0;e<n;++e)t.push(1);e=i.reshape(e,e.shape.concat(t))}else if(a>s){n=a-s;const e=[];for(let t=0;t<n;++t)e.push(1);t=i.reshape(t,t.shape.concat(e))}else n=0;if(2===t.shape.length&&2===e.shape.length)o=r[0]===r[1]?i.sum(i.mul(t,e),r[0]):i.sum(i.mul(i.transpose(t,[1,0]),e),r[1]);else{const n=r[0]!==t.shape.length-1,s=r[1]===e.shape.length-1;o=i.matMul(t,e,n,s)}if(n>0){let t;t=s>a?s+a-3:s-1;const e=[];for(let s=t;s<t+n;++s)e.push(s);o=i.squeeze(o,e)}return 1===o.shape.length&&(o=i.expandDims(o,1)),o}))}(n,s,e)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[Mo(this.axes,t.length),Mo(this.axes,e.length)],n}computeOutputShape(t){i.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0].slice(),n=t[1].slice();if(e.length>3||n.length>3)throw new zn("Dot layer does not support tensors of 4D or higher rank yet.");const s=this.interpretAxes(e,n);e.splice(s[0],1),n.splice(s[1],1),n.splice(0,1);const a=e.concat(n);return 1===a.length&&a.push(1),a}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}Ro.className="Dot",i.serialization.registerClass(Ro);class Lo extends bi{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t);return Rs((()=>(0,i.add)(As(n.shape,0,this.stddev),n)),(()=>n),e.training||!1)}))}}Lo.className="GaussianNoise",i.serialization.registerClass(Lo);class Oo extends bi{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e);const n=ri(t);if(this.rate>0&&this.rate<1){return Rs((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return(0,i.mul)(n,As(n.shape,1,t))}),(()=>n),e.training||!1)}return n}))}}Oo.className="GaussianDropout",i.serialization.registerClass(Oo);class Wo extends bi{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||ri(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,i.tidy)((()=>{if(this.rate<1&&this.rate>0){const n=this._getNoiseShape(t),s=()=>{const e=ri(t),s=-1.7580993408473766;let a=(0,i.greaterEqual)((0,i.randomUniform)(n),this.rate);a=ws(a,"float32");const r=((1-this.rate)*(1+this.rate*s**2))**-.5,o=-r*s*this.rate,l=(0,i.add)((0,i.mul)(e,a),(0,i.mul)((0,i.add)(a,-1),s));return(0,i.add)((0,i.mul)(l,r),o)};return Rs(s,(()=>ri(t)),e.training||!1)}return t}))}}function Bo(t,e,n,s,a){let r,o=arguments.length>5&&void 0!==arguments[5]?arguments[5]:.001;if(2===t.rank)r=i.batchNorm2d(t,e,n,s,a,o);else if(3===t.rank)r=i.batchNorm3d(t,e,n,s,a,o);else{if(4!==t.rank)throw new zn("batchNormalization is not implemented for array of rank ".concat(t.rank," ")+"yet");r=i.batchNorm4d(t,e,n,s,a,o)}return r}function Po(t,e,n,s){let a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:.001;return i.util.arraysEqual(s.slice().sort(),ys(0,t.rank-1))?function(t,e,n,s){let a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:.001;return(0,i.tidy)((()=>{const r=i.moments(t,s),o=r.mean,l=r.variance;return[Bo(t,o,l,n,e,a),o,l]}))}(t,e,n,s,a):function(t,e,n,s){let a=arguments.length>4&&void 0!==arguments[4]?arguments[4]:.001;return(0,i.tidy)((()=>{const r=i.moments(t,s),o=r.mean,l=r.variance,u=[];for(const e of ys(0,t.rank))-1!==s.indexOf(e)?u.push(1):u.push(t.shape[e]);const c=(0,i.reshape)(o,u),h=(0,i.reshape)(l,u),p=null==e?null:(0,i.reshape)(e,u),d=null==n?null:(0,i.reshape)(n,u);return[Bo(t,c,h,d,p,a),o,l]}))}(t,e,n,s,a)}Wo.className="AlphaDropout",i.serialization.registerClass(Wo);class Uo extends bi{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=si(t.betaInitializer||"zeros"),this.gammaInitializer=si(t.gammaInitializer||"ones"),this.movingMeanInitializer=si(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=si(t.movingVarianceInitializer||"ones"),this.betaConstraint=Wi(t.betaConstraint),this.gammaConstraint=Wi(t.gammaConstraint),this.betaRegularizer=zr(t.betaRegularizer),this.gammaRegularizer=zr(t.gammaRegularizer)}build(t){t=oi(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new Tn("Axis ".concat(e," of input tensor should have a defined dimension but ")+"the layer received an input with shape "+"".concat(JSON.stringify(t),"."));this.inputSpec=[new di({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,e){return(0,i.tidy)((()=>{const n=null!=e.training&&e.training,s=ri(t),a=s.shape,r=a.length,o=ys(0,r),l=this.axis>=0?this.axis:this.axis+r;o.splice(l,1);const u=Dn(1,r);u[l]=a[l];const c=o.slice();c.sort();const h=!i.util.arraysEqual(c,ys(0,r).slice(0,r-1));if(!n)return(()=>{if(h){const t=(0,i.reshape)(this.movingMean.read(),u),e=(0,i.reshape)(this.movingVariance.read(),u),n=this.center?(0,i.reshape)(this.beta.read(),u):null,a=this.scale?(0,i.reshape)(this.gamma.read(),u):null;return Bo(s,t,e,n,a,this.epsilon)}return Bo(s,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,d,f]=Po(s,this.gamma.read(),this.beta.read(),o,this.epsilon),m=(t,e,n)=>{i.tidy((()=>{const s=1-n,a=t.read(),r=i.mul(i.sub(a,e),s);t.write(i.sub(a,r))}))};return(()=>{m(this.movingMean,d,this.momentum),m(this.movingVariance,f,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:ni(this.betaInitializer),gammaInitializer:ni(this.gammaInitializer),movingMeanInitializer:ni(this.movingMeanInitializer),movingVarianceInitializer:ni(this.movingVarianceInitializer),betaRegularizer:xr(this.betaRegularizer),gammaRegularizer:xr(this.gammaRegularizer),betaConstraint:Li(this.betaConstraint),gammaConstraint:Li(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}Uo.className="BatchNormalization",i.serialization.registerClass(Uo);class Ho extends bi{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"===typeof this.axis){if(!Number.isInteger(this.axis))throw new Error("Expected axis to be an integer, but received ".concat(this.axis))}else{if(!Array.isArray(this.axis))throw new Error("Expected axis to be an integer or an array of integers, "+"but received ".concat(JSON.stringify(this.axis)));for(const t of this.axis)if(!Number.isInteger(t))throw new Error("Expected axis to be an array of integers, "+"but received ".concat(JSON.stringify(this.axis)))}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=si(t.betaInitializer||"zeros"),this.gammaInitializer=si(t.gammaInitializer||"ones"),this.betaRegularizer=zr(t.betaRegularizer),this.gammaRegularizer=zr(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=oi(t)).length;"number"===typeof this.axis&&(this.axis=[this.axis]);for(let i=0;i<this.axis.length;++i)this.axis[i]<0&&(this.axis[i]+=e);for(const i of this.axis)if(i<0||i>=e)throw new Error("Invalid axis: ".concat(i));if(this.axis.length!==Hn(this.axis).length)throw new Error("Found duplicate axes in: ".concat(this.axis));const n=this.axis.map((e=>t[e])),s=!0;this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,s):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,s):this.beta=null,this.built=!0}call(t,e){const n=ri(t),s=n.shape,a=s.length;return(0,i.tidy)((()=>{let{mean:t,variance:e}=(0,i.moments)(n,this.axis,!0);const r=Dn(1,a);for(const n of this.axis)r[n]=s[n];const o=t=>null!=t&&t.shape.length!==a?i.reshape(t,r):t;let l=this.scale?o(this.gamma.read()):null,u=this.center?o(this.beta.read()):null;const c=[],h=[];for(let n=0;n<a;++n)-1!==this.axis.indexOf(n)?(c.push(s[n]),h.push(1)):(c.push(1),h.push(s[n]));return t=i.tile(t,c),e=i.tile(e,c),null!=l&&(l=i.tile(l,h)),null!=u&&(u=i.tile(u,h)),Bo(n,t,e,u,l,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:ni(this.betaInitializer),gammaInitializer:ni(this.gammaInitializer),betaRegularizer:xr(this.betaRegularizer),gammaRegularizer:xr(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}Ho.className="LayerNormalization",i.serialization.registerClass(Ho);class Vo extends bi{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"===typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new Tn("ZeroPadding2D expects padding to be a length-2 array, but "+"received a length-".concat(t.padding.length," array."));let e,n;if("number"===typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new Tn("ZeroPadding2D expects height padding to be a length-2 array, "+"but received a length-".concat(t.padding[0].length," array."));if(e=t.padding[0],2!==t.padding[1].length)throw new Tn("ZeroPadding2D expects width padding to be a length-2 array, "+"but received a length-".concat(t.padding[1].length," array."));n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new di({ndim:4})]}computeOutputShape(t){let e,n;return t=oi(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,e){return(0,i.tidy)((()=>{return e=ri(t),n=this.padding,s=this.dataFormat,(0,i.tidy)((()=>{if(4!==e.rank)throw new Tn("temporalPadding expects input tensor to be 4-D, but received a "+"".concat(e.rank,"-D tensor."));if(null==n&&(n=[[1,1],[1,1]]),2!==n.length||2!==n[0].length||2!==n[1].length)throw new Tn("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==s&&(s="channelsLast"),"channelsLast"!==s&&"channelsFirst"!==s)throw new Tn("Unknown data format: ".concat(s,". ")+"Supported data formats are 'channelsLast' and 'channelsFirst.");let t;return t="channelsFirst"===s?[[0,0],[0,0],n[0],n[1]]:[[0,0],n[0],n[1],[0,0]],i.pad(e,t)}));var e,n,s}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function jo(t,e,n,s,a,r){return(0,i.tidy)((()=>{let o;as(a),os(r),rs(s),null==n&&(n=[1,1]),null==s&&(s="valid"),null==a&&(a="channelsLast"),null==r&&(r="max"),t=Or(t,a);const l="same"===s?"same":"valid";return o="max"===r?i.maxPool(t,e,n,l):i.avgPool(t,e,n,l),"channelsFirst"===a&&(o=i.transpose(o,[0,3,1,2])),o}))}function Go(t,e,n,s,a,r){return(0,i.tidy)((()=>{let o;as(a),os(r),rs(s),null==n&&(n=[1,1,1]),null==s&&(s="valid"),null==a&&(a="channelsLast"),null==r&&(r="max"),t=Wr(t,a);const l="same"===s?"same":"valid";return o="max"===r?i.maxPool3d(t,e,n,l):i.avgPool3d(t,e,n,l),"channelsFirst"===a&&(o=i.transpose(o,[0,4,1,2,3])),o}))}Vo.className="ZeroPadding2D",i.serialization.registerClass(Vo);class qo extends bi{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"===typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!==typeof t.poolSize[0])throw new Tn("poolSize for 1D convolutional layer must be a number or an Array of a single number, but received "+"".concat(JSON.stringify(t.poolSize)));this.poolSize=t.poolSize}if(qn(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"===typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!==typeof t.strides[0])throw new Tn("strides for 1D convolutional layer must be a number or an Array of a single number, but received "+"".concat(JSON.stringify(t.strides)));this.strides=t.strides}qn(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,rs(this.padding),this.inputSpec=[new di({ndim:3})]}computeOutputShape(t){const e=Rr((t=oi(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return(0,i.tidy)((()=>{this.invokeCallHook(t,e),t=vs(ri(t),2);const n=this.poolingFunction(ri(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return i.squeeze(n,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class Zo extends qo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),jo(t,e,n,s,i,"max")}}Zo.className="MaxPooling1D",i.serialization.registerClass(Zo);class Ko extends qo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),jo(t,e,n,s,i,"avg")}}Ko.className="AveragePooling1D",i.serialization.registerClass(Ko);class Jo extends bi{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new Tn("If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length "+"".concat(t.strides.length,"."));this.strides=t.strides}else this.strides=[t.strides,t.strides];qn(this.poolSize,"poolSize"),qn(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,as(this.dataFormat),rs(this.padding),this.inputSpec=[new di({ndim:4})]}computeOutputShape(t){t=oi(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=Rr(e,this.poolSize[0],this.padding,this.strides[0]),n=Rr(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,e){return(0,i.tidy)((()=>(this.invokeCallHook(t,e),this.poolingFunction(ri(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Yo extends Jo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),jo(t,e,n,s,i,"max")}}Yo.className="MaxPooling2D",i.serialization.registerClass(Yo);class Xo extends Jo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),jo(t,e,n,s,i,"avg")}}Xo.className="AveragePooling2D",i.serialization.registerClass(Xo);class Qo extends bi{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new Tn("If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length "+"".concat(t.strides.length,"."));this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];qn(this.poolSize,"poolSize"),qn(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,as(this.dataFormat),rs(this.padding),this.inputSpec=[new di({ndim:5})]}computeOutputShape(t){t=oi(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=Rr(e,this.poolSize[0],this.padding,this.strides[0]),n=Rr(n,this.poolSize[1],this.padding,this.strides[1]),s=Rr(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,e){return(0,i.tidy)((()=>(this.invokeCallHook(t,e),this.poolingFunction(ri(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class $o extends Qo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),Go(t,e,n,s,i,"max")}}$o.className="MaxPooling3D",i.serialization.registerClass($o);class tl extends Qo{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return as(i),rs(s),Go(t,e,n,s,i,"avg")}}tl.className="AveragePooling3D",i.serialization.registerClass(tl);class el extends bi{constructor(t){super(t),this.inputSpec=[new di({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new zn}}class nl extends el{constructor(t){super(t||{})}call(t,e){return(0,i.tidy)((()=>{const e=ri(t);return i.mean(e,1)}))}}nl.className="GlobalAveragePooling1D",i.serialization.registerClass(nl);class sl extends el{constructor(t){super(t||{})}call(t,e){return(0,i.tidy)((()=>{const e=ri(t);return i.max(e,1)}))}}sl.className="GlobalMaxPooling1D",i.serialization.registerClass(sl);class il extends bi{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,as(this.dataFormat),this.inputSpec=[new di({ndim:4})]}computeOutputShape(t){return"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new zn}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class al extends il{call(t,e){return(0,i.tidy)((()=>{const e=ri(t);return"channelsLast"===this.dataFormat?i.mean(e,[1,2]):i.mean(e,[2,3])}))}}al.className="GlobalAveragePooling2D",i.serialization.registerClass(al);class rl extends il{call(t,e){return(0,i.tidy)((()=>{const e=ri(t);return"channelsLast"===this.dataFormat?i.max(e,[1,2]):i.max(e,[2,3])}))}}rl.className="GlobalMaxPooling2D",i.serialization.registerClass(rl);class ol extends bi{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const s=Yi(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class ll extends ol{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=oi(t)).length<3)throw new Tn("TimeDistributed layer expects an input shape >= 3D, but received "+"input shape ".concat(JSON.stringify(t)));this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=oi(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,e){return(0,i.tidy)((()=>eo(((t,n)=>[ri(this.layer.call(t,e)),[]]),t=ri(t),[],!1,null,null,!1,!0)[1]))}}ll.className="TimeDistributed",i.serialization.registerClass(ll);class ul extends ol{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Yi(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Yi(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,jn(ss,"BidirectionalMergeMode",i),t.weights)throw new zn("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),this.returnState?(s=i.slice(1),e=i[0]):e=i[0],"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):_n(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=to(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const a=[],r=[];if(null!=n){const t=n.length;if(t%2>0)throw new Tn("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,a.push(...n);const s=n.map((t=>new di({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),r.push(...s)}if(null!=s)throw new zn("Support for constants in Bidirectional layers is not implemented yet.");const o=a[0]instanceof fi;for(const l of a)if(l instanceof fi!==o)throw new Tn("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(a),s=this.inputSpec.concat(r),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,e){return(0,i.tidy)((()=>{const n=e.initialState;let s,a,r,o;if(null==n)s=this.forwardLayer.call(t,e),a=this.backwardLayer.call(t,e);else{const i=n.slice(0,n.length/2),r=n.slice(n.length/2);s=this.forwardLayer.call(t,Object.assign(e,{initialState:i})),a=this.backwardLayer.call(t,Object.assign(e,{initialState:r}))}return this.returnState&&(Array.isArray(s)&&(r=s.slice(1).concat(a.slice(1))),s=s[0],a=a[0]),this.returnSequences&&(a=i.reverse(a,1)),"concat"===this.mergeMode?o=xs([s,a]):"sum"===this.mergeMode?o=i.add(s,a):"ave"===this.mergeMode?o=i.mul(.5,i.add(s,a)):"mul"===this.mergeMode?o=i.mul(s,a):null==this.mergeMode&&(o=[s,a]),this.returnState?null==this.mergeMode?o.concat(r):[o].concat(r):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){us(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),us(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Yi(e.layer);if(delete e.layer,null!=e.numConstants)throw new zn("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}ul.className="Bidirectional",i.serialization.registerClass(ul);class cl extends bi{constructor(t){super(t),this.scale=t.scale,t.offset?this.offset=t.offset:this.offset=0}getConfig(){const t={scale:this.scale,offset:this.offset},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,i.tidy)((()=>("float32"!==(t=ri(t)).dtype&&(t=ws(t,"float32")),(0,i.add)((0,i.mul)(t,this.scale),this.offset))))}}cl.className="Rescaling",i.serialization.registerClass(cl);const{resizeBilinear:hl,cropAndResize:pl}=i.image;class dl extends bi{constructor(t){super(t),this.height=t.height,this.width=t.width}centerCrop(t,e,n,s,a,r,o,l){return(0,i.tidy)((()=>{let u,c=!1;const h=[e/r,n/o,(s+e)/r,(a+n)/o],p=[];3===t.rank?(c=!0,u=(0,i.stack)([t])):u=t;for(let t=0;t<u.shape[0];t++)p.push(h);const d=(0,i.tensor)(p,[p.length,4]),f=(0,i.range)(0,p.length,1,"int32"),m=pl(u,d,f,[s,a],"nearest");return ws(c?ri((0,i.unstack)(m)):m,l)}))}upsize(t,e,n,s){return(0,i.tidy)((()=>ws(hl(t,[e,n]),s)))}call(t,e){return(0,i.tidy)((()=>{const e=ri(t),n=e.dtype,s=e.shape,i=s[s.length-3],a=s[s.length-2];let r=0;i!==this.height&&(r=Math.floor((i-this.height)/2));let o=0;return a!==this.width&&(o=Math.floor((a-this.width)/2),0===o&&(o=1)),r>=0&&o>=0?this.centerCrop(e,r,o,this.height,this.width,i,a,n):this.upsize(t,this.height,this.width,n)}))}getConfig(){const t={height:this.height,width:this.width},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const e=(t=oi(t)).length-3,n=t.length-2;return t[e]=this.height,t[n]=this.width,t}}dl.className="CenterCrop",i.serialization.registerClass(dl);class fl extends bi{constructor(t){super(t),this.numTokens=t.numTokens,t.outputMode?this.outputMode=t.outputMode:this.outputMode="multiHot"}getConfig(){const t={numTokens:this.numTokens,outputMode:this.outputMode},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return null==(t=oi(t))?[this.numTokens]:"oneHot"===this.outputMode&&1!==t[t.length-1]?(t.push(this.numTokens),t):(t[t.length-1]=this.numTokens,t)}call(t,e){return(0,i.tidy)((()=>{let n;if("int32"!==(t=ri(t)).dtype&&(t=ws(t,"int32")),"undefined"!==typeof e.countWeights){if("count"!==this.outputMode)throw new Tn("countWeights is not used when outputMode !== count.\n              Received countWeights=".concat(e.countWeights));n=ri(e.countWeights)}const s=(0,i.max)(t),a=(0,i.min)(t),r=(0,i.greater)(this.numTokens,s).bufferSync().get(0),o=(0,i.greaterEqual)(a,0).bufferSync().get(0);if(!r||!o)throw new Tn("Input values must be between 0 < values <="+" numTokens with numTokens=".concat(this.numTokens));return function(t,e,n,s){let a=ri(t);if("int32"!==a.dtype&&(a=ws(a,"int32")),"int"===e)return a;const r=a.shape;if(0===a.rank&&(a=(0,i.expandDims)(a,-1)),"oneHot"===e&&1!==a.shape[a.shape.length-1]&&(a=(0,i.expandDims)(a,-1)),a.rank>2)throw new Tn("When outputMode is not int, maximum output rank is 2"+" Received outputMode ".concat(e," and input shape ").concat(r)+" which would result in output rank ".concat(a.rank,"."));const o=["multiHot","oneHot"].includes(e),l=a;let u;if(u="undefined"!==typeof s&&"count"===e?(0,i.denseBincount)(l,s,n,o):(0,i.denseBincount)(l,[],n,o),"tfIdf"!==e)return u;if(s)return(0,i.mul)(u,s);throw new Tn("When outputMode is 'tfIdf', weights must be provided.")}(t,this.outputMode,this.numTokens,n)}))}}fl.className="CategoryEncoding",i.serialization.registerClass(fl);const ml=new Set(["bilinear","nearest"]);class gl extends bi{constructor(t){if(super(t),this.height=t.height,this.width=t.width,t.interpolation){if(!ml.has(t.interpolation))throw new Tn("Invalid interpolation parameter: ".concat(t.interpolation," is not implemented"));this.interpolation=t.interpolation}else this.interpolation="bilinear";this.cropToAspectRatio=Boolean(t.cropToAspectRatio)}computeOutputShape(t){const e=(t=oi(t))[2];return[this.height,this.width,e]}getConfig(){const t={height:this.height,width:this.width,interpolation:this.interpolation,cropToAspectRatio:this.cropToAspectRatio},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,i.tidy)((()=>{const e=[this.height,this.width];if("bilinear"===this.interpolation)return i.image.resizeBilinear(t,e,!this.cropToAspectRatio);if("nearest"===this.interpolation)return i.image.resizeNearestNeighbor(t,e,!this.cropToAspectRatio);throw new Error("Interpolation is ".concat(this.interpolation," but only ").concat([...ml]," are supported"))}))}}gl.className="Resizing",i.serialization.registerClass(gl);class yl{constructor(t){this.seed=t}next(){if(void 0!==this.seed)return this.seed++}}yl.className="RandomSeed";class bl extends bi{constructor(t){super(t),this.randomGenerator=new yl(t.seed)}getConfig(){const t={seed:this.randomGenerator.seed},e=super.getConfig();return Object.assign(t,e),t}}bl.className="BaseRandomLayer";const kl=new Set(["bilinear","nearest"]);class wl extends bl{constructor(t){super(t);const{factor:e,interpolation:n="bilinear"}=t;if(this.factor=e,Array.isArray(this.factor)&&2===this.factor.length)this.widthLower=this.factor[0],this.widthUpper=this.factor[1];else{if(Array.isArray(this.factor)||!(this.factor>0))throw new Tn("Invalid factor: ".concat(this.factor,". Must be positive number or tuple of 2 numbers"));this.widthLower=-this.factor,this.widthUpper=this.factor}if(this.widthLower<-1||this.widthUpper<-1)throw new Tn("factor must have values larger than -1. Got: ".concat(this.factor));if(this.widthUpper<this.widthLower)throw new Tn("factor cannot have upper bound less than lower bound.\n        Got upper bound: ".concat(this.widthUpper,".\n        Got lower bound: ").concat(this.widthLower,"\n      "));if(n){if(!kl.has(n))throw new Tn("Invalid interpolation parameter: ".concat(n," is not implemented"));this.interpolation=n}}getConfig(){const t={factor:this.factor,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const e=(t=oi(t))[2];return[this.imgHeight,-1,e]}call(t,e){return(0,i.tidy)((()=>{const e=ri(t);this.imgHeight=e.shape[e.shape.length-3];const n=e.shape[e.shape.length-2];this.widthFactor=(0,i.randomUniform)([1],1+this.widthLower,1+this.widthUpper,"float32",this.randomGenerator.next());let s=this.widthFactor.dataSync()[0]*n;s=Math.round(s);const a=[this.imgHeight,s];switch(this.interpolation){case"bilinear":return i.image.resizeBilinear(t,a);case"nearest":return i.image.resizeNearestNeighbor(t,a);default:throw new Error("Interpolation is ".concat(this.interpolation,"\n          but only ").concat([...kl]," are supported"))}}))}}function vl(t){return new wi(t)}function Il(t){return new Cr(t)}function Nl(t){return new Ar(t)}function Sl(t){return new Fr(t)}function xl(t){return new Dr(t)}function Tl(t){return new _r(t)}function zl(t){return new Er(t)}function Al(t){return new Yr(t)}function Fl(t){return new jr(t)}function Dl(t){return new qr(t)}function Cl(t){return new Gr(t)}function El(t){return new Zr(t)}function _l(t){return new Jr(t)}function Ml(t){return new Xr(t)}function Rl(t){return new Qr(t)}function Ll(t){return new $r(t)}function Ol(t){return new vo(t)}function Wl(t){return new ko(t)}function Bl(t){return new yo(t)}function Pl(t){return new bo(t)}function Ul(t){return new wo(t)}function Hl(t){return new Io(t)}function Vl(t){return new No(t)}function jl(t){return new So(t)}function Gl(t){return new To(t)}function ql(t){return new Ao(t)}function Zl(t){return new Do(t)}function Kl(t){return new _o(t)}function Jl(t){return new Co(t)}function Yl(t){return new Eo(t)}function Xl(t){return new Fo(t)}function Ql(t){return new Ro(t)}function $l(t){return new Uo(t)}function tu(t){return new Ho(t)}function eu(t){return new Vo(t)}function nu(t){return new Ko(t)}function su(t){return nu(t)}function iu(t){return nu(t)}function au(t){return new Xo(t)}function ru(t){return au(t)}function ou(t){return au(t)}function lu(t){return new tl(t)}function uu(t){return lu(t)}function cu(t){return lu(t)}function hu(t){return new nl(t)}function pu(t){return new al(t)}function du(t){return new sl(t)}function fu(t){return new rl(t)}function mu(t){return new Zo(t)}function gu(t){return new Yo(t)}function yu(t){return new $o(t)}function bu(t){return new oo(t)}function ku(t){return new ro(t)}function wu(t){return new uo(t)}function vu(t){return new lo(t)}function Iu(t){return new ao(t)}function Nu(t){return new io(t)}function Su(t){return new go(t)}function xu(t){return new mo(t)}function Tu(t){return new no(t)}function zu(t){return new co(t)}function Au(t){return new ul(t)}function Fu(t){return new ll(t)}wl.className="RandomWidth",i.serialization.registerClass(wl);const Du=du,Cu=fu,Eu=mu,_u=gu;function Mu(t){return new Lo(t)}function Ru(t){return new Oo(t)}function Lu(t){return new Wo(t)}function Ou(t){return new xo(t)}function Wu(t){return new cl(t)}function Bu(t){return new dl(t)}function Pu(t){return new gl(t)}function Uu(t){return new fl(t)}function Hu(t){return new wl(t)}var Vu,ju;(0,i.env)().registerFlag("KEEP_INTERMEDIATE_TENSORS",(()=>!1),(t=>{t&&console.warn("Keep intermediate tensors is ON. This will print the values of all intermediate tensors during model inference. Not all models support this mode. For details, check e2e/benchmarks/ model_config.js. This significantly impacts performance.")})),function(t){t[t.DT_INVALID=0]="DT_INVALID",t[t.DT_FLOAT=1]="DT_FLOAT",t[t.DT_DOUBLE=2]="DT_DOUBLE",t[t.DT_INT32=3]="DT_INT32",t[t.DT_UINT8=4]="DT_UINT8",t[t.DT_INT16=5]="DT_INT16",t[t.DT_INT8=6]="DT_INT8",t[t.DT_STRING=7]="DT_STRING",t[t.DT_COMPLEX64=8]="DT_COMPLEX64",t[t.DT_INT64=9]="DT_INT64",t[t.DT_BOOL=10]="DT_BOOL",t[t.DT_QINT8=11]="DT_QINT8",t[t.DT_QUINT8=12]="DT_QUINT8",t[t.DT_QINT32=13]="DT_QINT32",t[t.DT_BFLOAT16=14]="DT_BFLOAT16",t[t.DT_QINT16=15]="DT_QINT16",t[t.DT_QUINT16=16]="DT_QUINT16",t[t.DT_UINT16=17]="DT_UINT16",t[t.DT_COMPLEX128=18]="DT_COMPLEX128",t[t.DT_HALF=19]="DT_HALF",t[t.DT_RESOURCE=20]="DT_RESOURCE",t[t.DT_VARIANT=21]="DT_VARIANT",t[t.DT_UINT32=22]="DT_UINT32",t[t.DT_UINT64=23]="DT_UINT64",t[t.DT_FLOAT_REF=101]="DT_FLOAT_REF",t[t.DT_DOUBLE_REF=102]="DT_DOUBLE_REF",t[t.DT_INT32_REF=103]="DT_INT32_REF",t[t.DT_UINT8_REF=104]="DT_UINT8_REF",t[t.DT_INT16_REF=105]="DT_INT16_REF",t[t.DT_INT8_REF=106]="DT_INT8_REF",t[t.DT_STRING_REF=107]="DT_STRING_REF",t[t.DT_COMPLEX64_REF=108]="DT_COMPLEX64_REF",t[t.DT_INT64_REF=109]="DT_INT64_REF",t[t.DT_BOOL_REF=110]="DT_BOOL_REF",t[t.DT_QINT8_REF=111]="DT_QINT8_REF",t[t.DT_QUINT8_REF=112]="DT_QUINT8_REF",t[t.DT_QINT32_REF=113]="DT_QINT32_REF",t[t.DT_BFLOAT16_REF=114]="DT_BFLOAT16_REF",t[t.DT_QINT16_REF=115]="DT_QINT16_REF",t[t.DT_QUINT16_REF=116]="DT_QUINT16_REF",t[t.DT_UINT16_REF=117]="DT_UINT16_REF",t[t.DT_COMPLEX128_REF=118]="DT_COMPLEX128_REF",t[t.DT_HALF_REF=119]="DT_HALF_REF",t[t.DT_RESOURCE_REF=120]="DT_RESOURCE_REF",t[t.DT_VARIANT_REF=121]="DT_VARIANT_REF",t[t.DT_UINT32_REF=122]="DT_UINT32_REF",t[t.DT_UINT64_REF=123]="DT_UINT64_REF"}(Vu||(Vu={})),function(t){let e;!function(t){t[t.LEGACY=0]="LEGACY",t[t.V1=1]="V1",t[t.V2=2]="V2"}(e=t.CheckpointFormatVersion||(t.CheckpointFormatVersion={}))}(ju||(ju={}));Error;new Set(["Switch","Merge","Enter","Exit","NextIteration","StatelessIf","StatelessWhile","if","While"]),new Set(["NonMaxSuppressionV2","NonMaxSuppressionV3","NonMaxSuppressionV5","Where"]),new Set(["HashTable","HashTableV2","LookupTableImport","LookupTableImportV2","LookupTableFind","LookupTableFindV2","LookupTableSize","LookupTableSizeV2"]);n(70285);var Gu,qu=n(34334);function Zu(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:new Map,s=arguments.length>3&&void 0!==arguments[3]?arguments[3]:new Set;if(null==t)return null;if("function"===typeof Blob&&t instanceof Blob)return t.slice();if(s.has(t))throw new Error("Circular references are not supported.");if(n.has(t))return n.get(t);const i=e(t);if(i.recurse&&null!==i.value)throw new Error("A deep map function may not return both a value and recurse=true.");if(i.recurse){if(Xu(t)){const i=Array.isArray(t)?[]:{};s.add(t);for(const a in t){const r=Zu(t[a],e,n,s);i[a]=r}return s.delete(t),t.__proto__&&(i.__proto__=t.__proto__),i}throw new Error("Can't recurse into non-iterable type: ".concat(t))}return n.set(t,i.value),i.value}function Ku(t){return Ju(t,arguments.length>1&&void 0!==arguments[1]?arguments[1]:Yu)}function Ju(t,e){let n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:new Set;const s=t[0];if(n.has(s))throw new Error("Circular references are not supported.");const i=e(t);if(i.recurse&&null!==i.value)throw new Error("A deep zip function may not return both a value and recurse=true.");if(i.recurse){if(Xu(s)){const i=Array.isArray(s)?[]:{};n.add(s);for(const a in s){const s=Ju(t.map((t=>t[a])),e,n);i[a]=s}return n.delete(s),i}throw new Error("Can't recurse into non-iterable type: ".concat(s))}return i.value}function Yu(t){return null===t?null:Xu(t[0])?{value:null,recurse:!0}:{value:t,recurse:!1}}function Xu(t){let e=!1;if(i.env().get("IS_BROWSER"))e=t instanceof TextDecoder;else{const{StringDecoder:s}=n(80551);e=t instanceof s}return null!=t&&!ArrayBuffer.isView(t)&&(Array.isArray(t)||"object"===typeof t&&!(t instanceof i.Tensor)&&!(t instanceof Promise)&&!e)}function Qu(t){return function(t,e){return Zu(t,e)}(t,$u)}function $u(t){return t instanceof i.Tensor?{value:t.clone(),recurse:!1}:Xu(t)?{value:null,recurse:!0}:{value:t,recurse:!1}}class tc{constructor(t){if(this.capacity=t,this.begin=0,this.end=0,null==t)throw new RangeError("Can't create a ring buffer of unknown capacity.");if(t<1)throw new RangeError("Can't create ring buffer of capacity < 1.");this.data=new Array(t),this.doubledCapacity=2*t}wrap(t){for(;t<0;)t+=this.doubledCapacity;return t%this.doubledCapacity}get(t){if(t<0)throw new RangeError("Can't get item at a negative index.");return this.data[t%this.capacity]}set(t,e){if(t<0)throw new RangeError("Can't set item at a negative index.");this.data[t%this.capacity]=e}length(){let t=this.end-this.begin;return t<0&&(t=this.doubledCapacity+t),t}isFull(){return this.length()===this.capacity}isEmpty(){return 0===this.length()}push(t){if(this.isFull())throw new RangeError("Ring buffer is full.");this.set(this.end,t),this.end=this.wrap(this.end+1)}pushAll(t){for(const e of t)this.push(e)}pop(){if(this.isEmpty())throw new RangeError("Ring buffer is empty.");this.end=this.wrap(this.end-1);const t=this.get(this.end);return this.set(this.end,void 0),t}unshift(t){if(this.isFull())throw new RangeError("Ring buffer is full.");this.begin=this.wrap(this.begin-1),this.set(this.begin,t)}shift(){if(this.isEmpty())throw new RangeError("Ring buffer is empty.");const t=this.get(this.begin);return this.set(this.begin,void 0),this.begin=this.wrap(this.begin+1),t}shuffleExcise(t){if(this.isEmpty())throw new RangeError("Ring buffer is empty.");const e=this.wrap(this.begin+t),n=this.get(e);return this.set(e,this.pop()),n}}class ec extends tc{constructor(){super(ec.INITIAL_CAPACITY)}isFull(){return!1}push(t){super.isFull()&&this.expand(),super.push(t)}unshift(t){super.isFull()&&this.expand(),super.unshift(t)}expand(){const t=2*this.capacity,e=new Array(t),n=this.length();for(let s=0;s<n;s++)e[s]=this.get(this.wrap(this.begin+s));this.data=e,this.capacity=t,this.doubledCapacity=2*this.capacity,this.begin=0,this.end=n}}function nc(t){return new rc(t)}function sc(t,e){return new yc(t,e)}ec.INITIAL_CAPACITY=32;class ic{async toArray(){const t=[];let e=await this.next();for(;!e.done;)t.push(e.value),e=await this.next();return t}async toArrayForTest(){const t=this.prefetch(100),e=[];let n=await t.next();for(;!n.done;)e.push(n.value),n=await t.next();return e}async resolveFully(){let t=await this.next();for(;!t.done;)t=await this.next()}async resolveWhile(t){let e=await this.next(),n=t(e.value);for(;!e.done&&n;)e=await this.next(),n=t(e.value)}handleErrors(t){return new dc(this,t)}filter(t){return new hc(this,t)}map(t){return new pc(this,t)}mapAsync(t){return new fc(this,t)}serialMapAsync(t){return new fc(this,t).serial()}flatmap(t){return new gc(this,t)}async forEachAsync(t){return this.map(t).resolveFully()}async serialForEach(t){return this.serialMapAsync(t).resolveWhile((t=>!0===t))}rowMajorBatch(t){return new cc(this,t,!(arguments.length>1&&void 0!==arguments[1])||arguments[1])}columnMajorBatch(t){let e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1],n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Yu;return this.rowMajorBatch(t,e).map((t=>Ku(t,n)))}concatenate(t,e){return new yc(new ac([this,t]),e)}take(t){return t<0||null==t?this:new uc(this,t)}skip(t){return t<0||null==t?this:new lc(this,t)}prefetch(t){return new bc(this,t)}shuffle(t,e){return new kc(this,t,e)}serial(){return new oc(this)}}class ac extends ic{constructor(t){super(),this.items=t,this.trav=0}summary(){return"Array of ".concat(this.items.length," items")}async next(){if(this.trav>=this.items.length)return{value:null,done:!0};const t=this.items[this.trav];return this.trav++,{value:Qu(t),done:!1}}}class rc extends ic{constructor(t){super(),this.nextFn=t}summary(){return"Function call"}async next(){try{return this.nextFn()}catch(t){throw t.message="Error thrown while iterating through a dataset: ".concat(t.message),t}}}class oc extends ic{constructor(t){super(),this.upstream=t,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return"".concat(this.upstream.summary()," -> Serial")}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){return this.upstream.next()}}class lc extends ic{constructor(t,e){super(),this.upstream=t,this.maxCount=e,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return"".concat(this.upstream.summary()," -> Skip")}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){for(;this.count++<this.maxCount;){const t=await this.upstream.next();if(t.done)return t;i.dispose(t.value)}return this.upstream.next()}}class uc extends ic{constructor(t,e){super(),this.upstream=t,this.maxCount=e,this.count=0}summary(){return"".concat(this.upstream.summary()," -> Take")}async next(){return this.count++>=this.maxCount?{value:null,done:!0}:this.upstream.next()}}class cc extends ic{constructor(t,e){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(),this.upstream=t,this.batchSize=e,this.enableSmallLastBatch=n,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return"".concat(this.upstream.summary()," -> RowMajorBatch")}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){const t=[];for(;t.length<this.batchSize;){const e=await this.upstream.next();if(e.done)return this.enableSmallLastBatch&&t.length>0?{value:t,done:!1}:{value:null,done:!0};t.push(e.value)}return{value:t,done:!1}}}class hc extends ic{constructor(t,e){super(),this.upstream=t,this.predicate=e,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return"".concat(this.upstream.summary()," -> Filter")}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){for(;;){const t=await this.upstream.next();if(t.done||this.predicate(t.value))return t;i.dispose(t.value)}}}class pc extends ic{constructor(t,e){super(),this.upstream=t,this.transform=e}summary(){return"".concat(this.upstream.summary()," -> Map")}async next(){const t=await this.upstream.next();if(t.done)return{value:null,done:!0};const e=i.tensor_util.getTensorsInContainer(t.value),n=this.transform(t.value),s=i.tensor_util.getTensorsInContainer(n);for(const a of e)i.tensor_util.isTensorInList(a,s)||a.dispose();return{value:n,done:!1}}}class dc extends ic{constructor(t,e){super(),this.upstream=t,this.handler=e,this.count=0,this.lastRead=Promise.resolve({value:null,done:!1})}summary(){return"".concat(this.upstream.summary()," -> handleErrors")}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){for(;;)try{return await this.upstream.next()}catch(t){if(!this.handler(t))return{value:null,done:!0}}}}class fc extends ic{constructor(t,e){super(),this.upstream=t,this.transform=e}summary(){return"".concat(this.upstream.summary()," -> AsyncMap")}async next(){const t=await this.upstream.next();if(t.done)return{value:null,done:!0};const e=i.tensor_util.getTensorsInContainer(t.value),n=await this.transform(t.value),s=i.tensor_util.getTensorsInContainer(n);for(const a of e)i.tensor_util.isTensorInList(a,s)||a.dispose();return{value:n,done:!1}}}class mc extends ic{constructor(){super(),this.outputQueue=new ec,this.lastRead=Promise.resolve({value:null,done:!1})}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}async serialNext(){for(;0===this.outputQueue.length();)if(!await this.pump())return{value:null,done:!0};return{value:this.outputQueue.shift(),done:!1}}}class gc extends mc{constructor(t,e){super(),this.upstream=t,this.transform=e}summary(){return"".concat(this.upstream.summary()," -> Flatmap")}async pump(){const t=await this.upstream.next();if(t.done)return!1;const e=i.tensor_util.getTensorsInContainer(t.value),n=this.transform(t.value),s=i.tensor_util.getTensorsInContainer(n);this.outputQueue.pushAll(n);for(const a of e)i.tensor_util.isTensorInList(a,s)||a.dispose();return!0}}class yc extends ic{constructor(t,e){super(),this.baseErrorHandler=e,this.lastRead=null,this.iterator=null,this.moreIterators=t}summary(){return"".concat("TODO: fill in upstream of chained summaries"," -> Chained")}async next(){return this.lastRead=this.readFromChain(this.lastRead),this.lastRead}async readFromChain(t){if(await t,null==this.iterator){const t=await this.moreIterators.next();if(t.done)return{value:null,done:!0};this.iterator=t.value,null!=this.baseErrorHandler&&(this.iterator=this.iterator.handleErrors(this.baseErrorHandler))}const e=await this.iterator.next();return e.done?(this.iterator=null,this.readFromChain(t)):e}}!function(t){t[t.FAIL=0]="FAIL",t[t.SHORTEST=1]="SHORTEST",t[t.LONGEST=2]="LONGEST"}(Gu||(Gu={}));class bc extends ic{constructor(t,e){super(),this.upstream=t,this.bufferSize=e,this.buffer=new tc(e)}summary(){return"".concat(this.upstream.summary()," -> Prefetch")}refill(){for(;!this.buffer.isFull();){const t=this.upstream.next();this.buffer.push(t)}}next(){return this.refill(),this.buffer.shift()}}class kc extends bc{constructor(t,e,n){super(t,e),this.upstream=t,this.windowSize=e,this.upstreamExhausted=!1,this.random=qu.alea(n||i.util.now().toString()),this.lastRead=Promise.resolve({value:null,done:!1})}async next(){return this.lastRead=this.lastRead.then((()=>this.serialNext())),this.lastRead}randomInt(t){return Math.floor(this.random()*t)}chooseIndex(){return this.randomInt(this.buffer.length())}async serialNext(){for(this.upstreamExhausted||this.refill();!this.buffer.isEmpty();){const t=this.chooseIndex(),e=await this.buffer.shuffleExcise(t);if(!e.done)return this.refill(),e;this.upstreamExhausted=!0}return{value:null,done:!0}}}class wc{constructor(){this.size=null}batch(t){let e=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];const n=this;let s;return i.util.assert(t>0,(()=>"batchSize needs to be positive, but it is\n      ".concat(t))),s=this.size===1/0||null==this.size?this.size:e?Math.ceil(this.size/t):Math.floor(this.size/t),vc((async()=>(await n.iterator()).columnMajorBatch(t,e,Ic)),s)}concatenate(t){const e=this;let n;return n=this.size===1/0||t.size===1/0?1/0:null!=this.size&&null!=t.size?this.size+t.size:null,vc((async()=>(await e.iterator()).concatenate(await t.iterator())),n)}filter(t){const e=this;let n;return n=this.size===1/0?1/0:null,vc((async()=>(await e.iterator()).filter((e=>i.tidy((()=>t(e)))))),n)}async forEachAsync(t){return(await this.iterator()).forEachAsync(t)}map(t){const e=this;return vc((async()=>(await e.iterator()).map((e=>i.tidy((()=>t(e)))))),this.size)}mapAsync(t){const e=this;return vc((async()=>(await e.iterator()).mapAsync(t)),this.size)}prefetch(t){if(null==t)throw new RangeError("`Dataset.prefetch()` requires bufferSize to be specified.");const e=this;return vc((async()=>(await e.iterator()).prefetch(t)),this.size)}repeat(t){const e=this;let n;return n=null!=this.size&&t>0?this.size*t:0===t?0:null!=this.size&&(void 0===t||t<0)?1/0:null,vc((async()=>sc(nc((async()=>({value:await e.iterator(),done:!1}))).take(t))),n)}skip(t){const e=this;let n;return n=null!=this.size&&t>=0&&this.size>=t?this.size-t:null!=this.size&&(this.size<t||void 0===t||t<0)?0:null,vc((async()=>(await e.iterator()).skip(t)),n)}shuffle(t,e){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(null==t||t<0)throw null==this.size?new RangeError("`Dataset.shuffle()` requires bufferSize to be specified."):new RangeError("`Dataset.shuffle()` requires bufferSize to be specified.  If your data fits in main memory (for regular JS objects), and/or GPU memory (for `tf.Tensor`s), consider setting "+"bufferSize to the dataset size (".concat(this.size," elements)"));const s=this,a=qu.alea(e||i.util.now().toString());return vc((async()=>{let e=a.int32();return n&&(e+=a.int32()),(await s.iterator()).shuffle(t,e.toString())}),this.size)}take(t){const e=this;let n;return n=null!=this.size&&this.size>t?t:null!=this.size&&this.size<=t?this.size:null,vc((async()=>(await e.iterator()).take(t)),n)}async toArray(){if(this.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(await this.iterator()).toArray()}async toArrayForTest(){if(this.size===1/0)throw new Error("Can not convert infinite data stream to array.");return(await this.iterator()).toArrayForTest()}}function vc(t){let e=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null;return new class extends wc{constructor(){super(...arguments),this.size=e}async iterator(){return t()}}}function Ic(t){if(null===t)return null;if(function(t){return null==t||null===(e=t)||"object"!==typeof e&&"function"!==typeof e||Array.isArray(t)||"object"===typeof t&&t instanceof i.Tensor||i.util.isTypedArray(t);var e}(t[0])){return{value:function(t){if(0===t.length)throw new Error("Can't make a batch of zero elements.");return t[0]instanceof i.Tensor?i.stack(t):i.tensor(t)}(t),recurse:!1}}return{value:null,recurse:!0}}wc.MAX_BUFFER_SIZE=1e4;Symbol("out"),Symbol("field"),Symbol("quote"),Symbol("quoteafterquote"),Symbol("quoteinquote");var Nc=n(3143);const Sc=i.kernel_impls.whereImpl;class xc extends i.KernelBackend{nextDataId(){return xc.nextDataId++}constructor(){super(),this.blockSize=48,this.firstUse=!0,this.data=new i.DataStorage(this,(0,i.engine)())}write(t,e,n){this.firstUse&&(this.firstUse=!1,(0,i.env)().get("IS_NODE")&&i.backend_util.warn("\n============================\nHi, looks like you are running TensorFlow.js in Node.js. To speed things up dramatically, install our node backend, visit https://github.com/tensorflow/tfjs-node for more details. \n============================"));const s={id:this.nextDataId()};return this.data.set(s,{values:t,dtype:n,refCount:1}),s}makeTensorInfo(t,e,n){let s;if("string"===e&&null!=n&&n.length>0&&i.util.isString(n[0])){const a=n.map((t=>i.util.encodeString(t)));s=this.write(a,t,e)}else s=this.write(n,t,e);return{dataId:s,shape:t,dtype:e}}refCount(t){if(this.data.has(t)){return this.data.get(t).refCount}return 0}incRef(t){this.data.get(t).refCount++}decRef(t){if(this.data.has(t)){this.data.get(t).refCount--}}move(t,e,n,s,i){this.data.set(t,{values:e,dtype:s,refCount:i})}numDataIds(){return this.data.numDataIds()}async read(t){return this.readSync(t)}readSync(t){const{dtype:e,complexTensorInfos:n}=this.data.get(t);if("complex64"===e){const t=this.readSync(n.real.dataId),e=this.readSync(n.imag.dataId);return i.backend_util.mergeRealAndImagArrays(t,e)}return i.util.convertBackendValuesAndArrayBuffer(this.data.get(t).values,e)}bufferSync(t){const e=this.readSync(t.dataId);if("string"===t.dtype)try{const n=e.map((t=>i.util.decodeString(t)));return(0,i.buffer)(t.shape,t.dtype,n)}catch(n){throw new Error("Failed to decode encoded string bytes into utf-8")}return(0,i.buffer)(t.shape,t.dtype,e)}makeOutput(t,e,n){return(0,i.engine)().makeTensorFromTensorInfo(this.makeTensorInfo(e,n,t),this)}disposeData(t){let e=arguments.length>1&&void 0!==arguments[1]&&arguments[1];if(this.data.has(t)){if(this.data.get(t).refCount--,!e&&this.data.get(t).refCount>0)return!1;const{complexTensorInfos:n}=this.data.get(t);null!=n&&(this.disposeData(n.real.dataId,!0),this.disposeData(n.imag.dataId,!0)),this.data.delete(t)}return!0}disposeIntermediateTensorInfo(t){this.disposeData(t.dataId)}async time(t){const e=i.util.now();t();return{kernelMs:i.util.now()-e}}memory(){return{unreliable:!0,reasons:["The reported memory is an upper bound. Due to automatic garbage collection, the true allocated memory may be less."]}}where(t){(0,Nc.C)([t],"where");const e=this.readSync(t.dataId);return Sc(t.shape,e)}dispose(){}floatPrecision(){return 32}epsilon(){return super.epsilon()}}xc.nextDataId=0;(0,i.registerBackend)("cpu",(()=>new xc),1);var Tc=n(34729);const zc=(0,Tc.v)(i.Elu,(t=>t>=0?t:Math.exp(t)-1)),Ac={kernelName:i.Elu,backendName:"cpu",kernelFunc:zc};var Fc=n(3701);function Dc(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{alpha:r}=s;(0,Nc.C)([a],"leakyRelu");const o=i.util.sizeFromShape(a.shape),l=n.data.get(a.dataId).values,u=i.util.getTypedArrayFromDType("float32",o);for(let i=0;i<l.length;i++)u[i]=l[i]<0?r*l[i]:l[i];return n.makeTensorInfo(a.shape,"float32",u)}const Cc={kernelName:i.LeakyRelu,backendName:"cpu",kernelFunc:Dc};var Ec=n(82312);const _c=(0,Ec.Z)(((t,e)=>t<0?e*t:t));function Mc(t){const{inputs:e,backend:n}=t,{x:s,alpha:i}=e;(0,Nc.C)([s,i],"prelu");const a=n.data.get(s.dataId).values,r=n.data.get(i.dataId).values,[o,l]=_c(s.shape,i.shape,a,r,"float32");return n.makeTensorInfo(l,"float32",o)}const Rc={kernelName:i.Prelu,backendName:"cpu",kernelFunc:Mc},Lc=(0,Tc.v)(i.Relu,(t=>Math.max(0,t))),Oc={kernelName:i.Relu,backendName:"cpu",kernelFunc:Lc},Wc=(0,Tc.v)(i.Relu6,(t=>Math.min(Math.max(0,t),6))),Bc={kernelName:i.Relu6,backendName:"cpu",kernelFunc:Wc};var Pc=n(42511);function Uc(t,e,n,s,i){if("linear"===n)return(0,Fc.D)({inputs:{x:e},backend:t});if("relu"===n)return Lc({inputs:{x:e},backend:t});if("elu"===n)return zc({inputs:{x:e},backend:t});if("relu6"===n)return Wc({inputs:{x:e},backend:t});if("prelu"===n)return Mc({inputs:{x:e,alpha:s},backend:t});if("leakyrelu"===n)return Dc({inputs:{x:e},backend:t,attrs:{alpha:i}});if("sigmoid"===n)return(0,Pc.ry)({inputs:{x:e},backend:t});throw new Error("Activation ".concat(n," has not been implemented for the CPU backend."))}var Hc=n(49864);function Vc(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{shape:r}=s,o=i.util.sizeFromShape(a.shape),l=i.util.inferFromImplicitShape(r,o),u=i.util.sizeFromShape(l);i.util.assert(o===u,(()=>"The new shape (".concat(l,") has ").concat(u," elements and the old ")+"shape (".concat(a.shape,") has ").concat(o," elements. The new shape and old ")+"shape must have the same number of elements.")),n.incRef(a.dataId);const c=n.data.get(a.dataId);if(null!=c.complexTensorInfos){const t=c.complexTensorInfos.real,e=c.complexTensorInfos.imag;t.shape=l,e.shape=l}return{dataId:a.dataId,shape:l,dtype:a.dtype}}const jc={kernelName:i.Reshape,backendName:"cpu",kernelFunc:Vc};function Gc(t){const{inputs:e,backend:n,attrs:s}=t,{a:a,b:r}=e,{transposeA:o,transposeB:l}=s;(0,Nc.C)([a,r],"matMul");const u=a.shape.length,c=r.shape.length,h=o?a.shape[u-2]:a.shape[u-1],p=l?r.shape[c-1]:r.shape[c-2],d=o?a.shape[u-1]:a.shape[u-2],f=l?r.shape[c-2]:r.shape[c-1],m=a.shape.slice(0,-2),g=r.shape.slice(0,-2),y=i.util.sizeFromShape(m),b=i.util.sizeFromShape(g),k=i.broadcast_util.assertAndGetBroadcastShape(a.shape.slice(0,-2),r.shape.slice(0,-2)).concat([d,f]);i.util.assert(h===p,(()=>"Error in matMul: inner shapes (".concat(h,") and (")+"".concat(p,") of Tensors with shapes ").concat(a.shape," and ")+"".concat(r.shape," and transposeA=").concat(o)+" and transposeB=".concat(l," must match.")));const w=l?[b,f,p]:[b,p,f],v=Vc({inputs:{x:a},backend:n,attrs:{shape:o?[y,h,d]:[y,d,h]}}),I=Vc({inputs:{x:r},backend:n,attrs:{shape:w}}),N=o?v.shape[1]:v.shape[2],S=o?v.shape[2]:v.shape[1],x=l?I.shape[1]:I.shape[2],T=Math.max(y,b),z=n.data.get(v.dataId).values,A=n.data.get(I.dataId).values,F=i.util.computeStrides(v.shape),D=i.util.computeStrides(I.shape),[C,E,_]=o?[F[0],1,F[1]]:[F[0],F[1],1],[M,R,L]=l?[1,D[1],D[0]]:[D[1],1,D[0]],O=S*x,W=(0,i.buffer)([T,S,x],v.dtype),B=W.values,P=n.blockSize;for(let i=0;i<T;i++){const t=i%y,e=i%b;for(let n=0;n<S;n+=P){const s=Math.min(n+P,S);for(let a=0;a<x;a+=P){const r=Math.min(a+P,x);for(let o=0;o<N;o+=P){const l=Math.min(o+P,N);for(let u=n;u<s;u++)for(let n=a;n<r;n++){let s=0;for(let i=o;i<l;i++){s+=z[t*C+u*E+i*_]*A[i*M+n*R+e*L]}B[i*O+(u*x+n)]+=s}}}}}return n.disposeIntermediateTensorInfo(v),n.disposeIntermediateTensorInfo(I),n.makeTensorInfo(k,W.dtype,W.values)}const qc={kernelName:i.BatchMatMul,backendName:"cpu",kernelFunc:Gc};const Zc={kernelName:i._FusedMatMul,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{a:i,b:a,bias:r,preluActivationWeights:o}=e,{transposeA:l,transposeB:u,activation:c,leakyreluAlpha:h}=s;let p,d,f;const m=[];p=Gc({inputs:{a:i,b:a},attrs:{transposeA:l,transposeB:u},backend:n}),r&&(d=(0,Hc.WQ)({inputs:{a:p,b:r},backend:n}),m.push(p),p=d),c&&(f=Uc(n,p,c,o,h),m.push(p),p=f);for(const g of m)n.disposeIntermediateTensorInfo(g);return p}};var Kc=n(85903);const Jc=(0,Tc.v)(i.Acos,(t=>Math.acos(t))),Yc={kernelName:i.Acos,backendName:"cpu",kernelFunc:Jc},Xc=(0,Tc.v)(i.Acosh,(t=>Math.acosh(t))),Qc={kernelName:i.Acosh,backendName:"cpu",kernelFunc:Xc};const $c={kernelName:i.AddN,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,s=e;(0,Nc.C)(e,"addN");const a=s.map((t=>n.data.get(t.dataId).values)),r=(0,i.buffer)(s[0].shape,s[0].dtype),o=r.values;for(let i=0;i<s.length;i++){const t=a[i];for(let e=0;e<o.length;e++)o[e]+=t[e]}return n.makeTensorInfo(r.shape,r.dtype,r.values)}};var th=n(89868);const eh={kernelName:i.All,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,keepDims:o}=s;(0,Nc.C)(a,"all");const l=i.util.parseAxisParam(r,a.shape);let u=l;const c=i.backend_util.getAxesPermutation(u,a.shape.length);let h=a;null!=c&&(h=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:c}}),u=i.backend_util.getInnerMostAxes(u.length,a.shape.length)),i.backend_util.assertAxesAreInnerMostDims("all",u,h.shape.length);const[p,d]=i.backend_util.computeOutAndReduceShapes(h.shape,u),f=i.util.sizeFromShape(d),m=i.util.makeZerosTypedArray(i.util.sizeFromShape(p),h.dtype),g=n.data.get(h.dataId).values;for(let i=0;i<m.length;++i){const t=i*f;let e=g[t];for(let n=0;n<f;++n){const s=g[t+n];e=e&&s}m[i]=e}null!=c&&n.disposeIntermediateTensorInfo(h);const y=n.makeTensorInfo(p,h.dtype,m);if(o){const t=Vc({inputs:{x:y},backend:n,attrs:{shape:i.backend_util.expandShapeToKeepDim(p,l)}});return n.disposeIntermediateTensorInfo(y),t}return y}};const nh={kernelName:i.Any,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,keepDims:o}=s;(0,Nc.C)(a,"any");const l=i.util.parseAxisParam(r,a.shape);let u=l;const c=i.backend_util.getAxesPermutation(u,a.shape.length);let h=a;null!=c&&(h=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:c}}),u=i.backend_util.getInnerMostAxes(u.length,a.shape.length)),i.backend_util.assertAxesAreInnerMostDims("any",u,h.shape.length);const[p,d]=i.backend_util.computeOutAndReduceShapes(h.shape,u),f=i.util.sizeFromShape(d),m=i.util.makeZerosTypedArray(i.util.sizeFromShape(p),h.dtype),g=n.data.get(h.dataId).values;for(let i=0;i<m.length;++i){const t=i*f;let e=g[t];for(let n=0;n<f;++n){const s=g[t+n];e=e||s}m[i]=e}null!=c&&n.disposeIntermediateTensorInfo(h);const y=n.makeTensorInfo(p,h.dtype,m);if(o){const t=Vc({inputs:{x:y},backend:n,attrs:{shape:i.backend_util.expandShapeToKeepDim(p,l)}});return n.disposeIntermediateTensorInfo(y),t}return y}};const sh={kernelName:i.ArgMax,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r}=s;(0,Nc.C)(a,"argMax");let o=i.util.parseAxisParam(r,a.shape);const l=i.backend_util.getAxesPermutation(o,a.shape.length);let u=a;const c=[];null!=l&&(u=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:l}}),c.push(u),o=i.backend_util.getInnerMostAxes(o.length,u.shape.length)),o=[o[0]],i.backend_util.assertAxesAreInnerMostDims("argMax",o,u.shape.length);const[h,p]=i.backend_util.computeOutAndReduceShapes(u.shape,o),d=i.util.sizeFromShape(h),f=i.util.makeZerosTypedArray(d,"int32"),m=i.util.sizeFromShape(p),g=n.data.get(u.dataId).values;for(let i=0;i<f.length;++i){const t=i*m;let e=g[t],n=0;for(let s=0;s<m;++s){const i=g[t+s];i>e&&(e=i,n=s)}f[i]=n}return c.forEach((t=>n.disposeIntermediateTensorInfo(t))),n.makeTensorInfo(h,"int32",f)}};const ih={kernelName:i.ArgMin,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r}=s;(0,Nc.C)(a,"argMin");let o=i.util.parseAxisParam(r,a.shape);const l=i.backend_util.getAxesPermutation(o,a.shape.length);let u=a;const c=[];null!=l&&(u=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:l}}),c.push(u),o=i.backend_util.getInnerMostAxes(o.length,u.shape.length)),o=[o[0]],i.backend_util.assertAxesAreInnerMostDims("argMin",o,u.shape.length);const[h,p]=i.backend_util.computeOutAndReduceShapes(u.shape,o),d=i.util.sizeFromShape(h),f=i.util.makeZerosTypedArray(d,"int32"),m=i.util.sizeFromShape(p),g=n.data.get(u.dataId).values;for(let i=0;i<f.length;++i){const t=i*m;let e=g[t],n=0;for(let s=0;s<m;++s){const i=g[t+s];i<e&&(e=i,n=s)}f[i]=n}return c.forEach((t=>n.disposeIntermediateTensorInfo(t))),n.makeTensorInfo(h,"int32",f)}},ah=(0,Tc.v)(i.Asin,(t=>Math.asin(t))),rh={kernelName:i.Asin,backendName:"cpu",kernelFunc:ah},oh=(0,Tc.v)(i.Asinh,(t=>Math.asinh(t))),lh={kernelName:i.Asinh,backendName:"cpu",kernelFunc:oh},uh=(0,Tc.v)(i.Atan,(t=>Math.atan(t))),ch={kernelName:i.Atan,backendName:"cpu",kernelFunc:uh};var hh=n(85101);const ph=(0,Ec.Z)(((t,e)=>Math.atan2(t,e))),dh=(0,hh.j)(i.Atan2,ph),fh={kernelName:i.Atan2,backendName:"cpu",kernelFunc:dh},mh=(0,Tc.v)(i.Atanh,(t=>Math.atanh(t))),gh={kernelName:i.Atanh,backendName:"cpu",kernelFunc:mh};function yh(t,e,n,s,a,r){const o=a.strideHeight,l=a.strideWidth,u=a.dilationHeight,c=a.dilationWidth,h=a.effectiveFilterHeight,p=a.effectiveFilterWidth,d=a.padInfo.top,f=a.padInfo.left,m="max"===r?Number.NEGATIVE_INFINITY:Number.POSITIVE_INFINITY,g=(0,i.buffer)(a.outShape,n),y=g.values,b=a.outShape[1]*a.outShape[2]*a.outShape[3],k=a.outShape[2]*a.outShape[3],w=a.outShape[3];for(let i=0;i<a.batchSize;++i){const e=i*b,n=i*s[0];for(let i=0;i<a.inChannels;++i)for(let g=0;g<a.outHeight;++g){const b=g*o-d,v=Math.max(0,b),I=Math.min(a.inHeight,h+b),N=e+g*k;for(let e=0;e<a.outWidth;++e){const o=e*l-f,h=Math.max(0,o),d=Math.min(a.inWidth,p+o);let g=m,b=0,k=0;for(let e=v;e<I;e+=u){const a=n+e*s[1];for(let e=h;e<d;e+=c){const n=t[a+e*s[2]+i];"max"===r&&n>g?g=n:"avg"===r&&(b+=n,k++)}if(isNaN(g))break}y[N+e*w+i]="avg"===r?b/k:g}}}return g}function bh(t,e,n,s){let a=arguments.length>4&&void 0!==arguments[4]&&arguments[4],r=arguments.length>5&&void 0!==arguments[5]&&arguments[5];const o=(0,i.buffer)(s.outShape,"int32"),l=s.strideHeight,u=s.strideWidth,c=s.dilationHeight,h=s.dilationWidth,p=s.effectiveFilterHeight,d=s.effectiveFilterWidth,f=s.padInfo.top,m=s.padInfo.left,g=(0,i.buffer)(e,n,t);for(let i=0;i<s.batchSize;++i)for(let t=0;t<s.inChannels;++t)for(let e=0;e<s.outHeight;++e){const n=e*l-f;let y=n;for(;y<0;)y+=c;const b=Math.min(s.inHeight,p+n);for(let l=0;l<s.outWidth;++l){const p=l*u-m;let f=p;for(;f<0;)f+=h;const k=Math.min(s.inWidth,d+p);let w=Number.NEGATIVE_INFINITY,v=-1;for(let e=y;e<b;e+=c){const o=e-n;for(let n=f;n<k;n+=h){const l=n-p,u=g.get(i,e,n,t);u>w&&(w=u,v=a?r?((i*s.inHeight+e)*s.inWidth+n)*s.inChannels+t:(e*s.inWidth+n)*s.inChannels+t:o*d+l)}}o.set(v,i,e,l,t)}}return o}function kh(t,e,n,s,a,r){const o=a.strideDepth,l=a.strideHeight,u=a.strideWidth,c=a.dilationDepth,h=a.dilationHeight,p=a.dilationWidth,d=a.effectiveFilterDepth,f=a.effectiveFilterHeight,m=a.effectiveFilterWidth,g=a.padInfo.front,y=a.padInfo.top,b=a.padInfo.left,k="max"===r?Number.NEGATIVE_INFINITY:Number.POSITIVE_INFINITY,w=(0,i.buffer)(a.outShape,n),v=w.values,I=a.outShape[1]*a.outShape[2]*a.outShape[3]*a.outShape[4],N=a.outShape[2]*a.outShape[3]*a.outShape[4],S=a.outShape[3]*a.outShape[4],x=a.outShape[4];for(let i=0;i<a.batchSize;++i){const e=i*I,n=i*s[0];for(let i=0;i<a.inChannels;++i)for(let w=0;w<a.outDepth;++w){const I=w*o-g;let T=I;for(;T<0;)T+=c;const z=Math.min(a.inDepth,d+I),A=e+w*N;for(let e=0;e<a.outHeight;++e){const o=e*l-y;let d=o;for(;d<0;)d+=h;const g=Math.min(a.inHeight,f+o),w=A+e*S;for(let e=0;e<a.outWidth;++e){const o=e*u-b;let l=o;for(;l<0;)l+=p;const f=Math.min(a.inWidth,m+o),y=w+e*x;let I=k,N=0,S=0;for(let e=T;e<z;e+=c){const a=n+e*s[1];for(let e=d;e<g;e+=h){const n=a+e*s[2];for(let e=l;e<f;e+=p){const a=t[n+e*s[3]+i];if("max"===r&&a>I?I=a:"avg"===r&&(N+=a,S++),isNaN(I))break}if(isNaN(I))break}if(isNaN(I))break}v[y+i]="avg"===r?N/Math.max(S,1):I}}}}return w}const wh={kernelName:i.AvgPool,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e;(0,Nc.C)(a,"avgPool");const{filterSize:r,strides:o,pad:l,dimRoundingMode:u}=s;i.util.assert(i.backend_util.eitherStridesOrDilationsAreOne(o,1),(()=>"Error in avgPool: Either strides or dilations must be 1. "+"Got strides ".concat(o," and dilations '").concat(1,"'")));const c=i.backend_util.computePool2DInfo(a.shape,r,o,1,l,u);let h;if(1===c.filterWidth&&1===c.filterHeight&&i.util.arraysEqual(c.inShape,c.outShape))h=(0,Fc.D)({inputs:{x:a},backend:n});else{const t=n.data.get(a.dataId).values,e=i.util.computeStrides(a.shape),s=yh(t,a.shape,a.dtype,e,c,"avg");h=n.makeTensorInfo(c.outShape,a.dtype,s.values)}return h}};const vh={kernelName:i.AvgPool3D,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{filterSize:r,strides:o,pad:l,dimRoundingMode:u,dataFormat:c}=s;(0,Nc.C)(a,"avgPool3d");const h=i.backend_util.computePool3DInfo(a.shape,r,o,1,l,u,c),p=kh(n.data.get(a.dataId).values,a.shape,a.dtype,i.util.computeStrides(a.shape),h,"avg");return n.makeTensorInfo(p.shape,"float32",p.values)}};const Ih={kernelName:i.AvgPool3DGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,input:r}=e,{filterSize:o,strides:l,pad:u,dimRoundingMode:c}=s;(0,Nc.C)([a,r],"avgPool3DGrad");const h=i.backend_util.computePool3DInfo(r.shape,o,l,1,u,c),p=h.strideDepth,d=h.strideHeight,f=h.strideWidth,m=h.filterDepth,g=h.filterHeight,y=h.filterWidth,b=h.dilationDepth,k=h.dilationHeight,w=h.dilationWidth,v=h.effectiveFilterDepth,I=h.effectiveFilterHeight,N=h.effectiveFilterWidth,S=v-1-h.padInfo.front,x=N-1-h.padInfo.left,T=I-1-h.padInfo.top,z=(0,i.buffer)(r.shape,"float32"),A=1/(m*g*y),F=n.bufferSync(a);for(let i=0;i<h.batchSize;++i)for(let t=0;t<h.inChannels;++t)for(let e=0;e<h.inDepth;++e)for(let n=0;n<h.inHeight;++n)for(let s=0;s<h.inWidth;++s){const a=e-S,r=n-T,o=s-x;let l=0;for(let e=0;e<v;e+=b){const n=(a+e)/p;if(!(n<0||n>=h.outDepth||Math.floor(n)!==n))for(let e=0;e<I;e+=k){const s=(r+e)/d;if(!(s<0||s>=h.outHeight||Math.floor(s)!==s))for(let e=0;e<N;e+=w){const a=(o+e)/f;if(a<0||a>=h.outWidth||Math.floor(a)!==a)continue;l+=F.get(i,n,s,a,t)}}}z.set(l*A,i,e,n,s,t)}return n.makeTensorInfo(z.shape,z.dtype,z.values)}};const Nh={kernelName:i.AvgPoolGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,input:r}=e,o=r;(0,Nc.C)([a,r],"avgPoolGrad");const{filterSize:l,strides:u,pad:c}=s,h=i.backend_util.computePool2DInfo(o.shape,l,u,1,c),p=h.strideHeight,d=h.strideWidth,f=h.filterHeight,m=h.filterWidth,g=h.dilationHeight,y=h.dilationWidth,b=h.effectiveFilterHeight,k=h.effectiveFilterWidth,w=k-1-h.padInfo.left,v=b-1-h.padInfo.top,I=(0,i.buffer)(o.shape,"float32"),N=1/(f*m),S=n.data.get(a.dataId).values,x=(0,i.buffer)(a.shape,"float32",S);for(let i=0;i<h.batchSize;++i)for(let t=0;t<h.inChannels;++t)for(let e=0;e<h.inHeight;++e)for(let n=0;n<h.inWidth;++n){const s=e-v,a=n-w;let r=0;for(let e=0;e<b;e+=g){const n=(s+e)/p;if(!(n<0||n>=h.outHeight||Math.floor(n)!==n))for(let e=0;e<k;e+=y){const s=(a+e)/d;if(s<0||s>=h.outWidth||Math.floor(s)!==s)continue;r+=x.get(i,n,s,t)}}I.set(r*N,i,e,n,t)}return n.makeTensorInfo(I.shape,I.dtype,I.values)}};const Sh={kernelName:i.FusedBatchNorm,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,scale:r,offset:o,mean:l,variance:u}=e;i.util.assert(l.shape.length===u.shape.length,(()=>"Batch normalization gradient requires mean and variance to have equal ranks.")),i.util.assert(null==o||l.shape.length===o.shape.length,(()=>"Batch normalization gradient requires mean and offset to have equal ranks.")),i.util.assert(null==r||l.shape.length===r.shape.length,(()=>"Batch normalization gradient requires mean and scale to have equal ranks.")),(0,Nc.C)([a,l,u,r,o],"batchNorm");let{varianceEpsilon:c}=s;null==c&&(c=.001);const h=n.data.get(a.dataId).values,p=n.data.get(l.dataId).values,d=n.data.get(u.dataId).values,f=r?n.data.get(r.dataId).values:new Float32Array([1]),m=o?n.data.get(o.dataId).values:new Float32Array([0]),g=new Float32Array(h.length),y=m.length,b=f.length,k=d.length,w=p.length;let v=0,I=0,N=0,S=0;for(let i=0;i<h.length;++i)g[i]=m[v++]+(h[i]-p[I++])*f[N++]/Math.sqrt(d[S++]+c),v>=y&&(v=0),I>=w&&(I=0),N>=b&&(N=0),S>=k&&(S=0);return n.makeTensorInfo(a.shape,a.dtype,g)}};var xh=n(34357);const Th={kernelName:i.BatchToSpaceND,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{blockShape:r,crops:o}=s;(0,Nc.C)([a],"batchToSpaceND");const l=r.reduce(((t,e)=>t*e)),u=i.backend_util.getReshaped(a.shape,r,l),c=i.backend_util.getPermuted(u.length,r.length),h=i.backend_util.getReshapedPermuted(a.shape,r,l),p=i.backend_util.getSliceBeginCoords(o,r.length),d=i.backend_util.getSliceSize(h,o,r.length),f=Vc({inputs:{x:a},backend:n,attrs:{shape:u}}),m=(0,th.m)({inputs:{x:f},backend:n,attrs:{perm:c}}),g=Vc({inputs:{x:m},backend:n,attrs:{shape:h}}),y=(0,xh.di)({inputs:{x:g},backend:n,attrs:{begin:p,size:d}});return n.disposeIntermediateTensorInfo(f),n.disposeIntermediateTensorInfo(m),n.disposeIntermediateTensorInfo(g),y}};var zh=n(92412);const Ah={kernelName:i.Bincount,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i,weights:a}=e,{size:r}=s,o=n.data.get(i.dataId).values,l=n.data.get(a.dataId).values,u=(0,zh.X)(o,l,a.dtype,a.shape,r);return n.makeTensorInfo([r],a.dtype,u)}};var Fh=n(89475);const Dh={kernelName:i.BroadcastArgs,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{s0:s,s1:a}=e,r=n.data.get(s.dataId).values,o=n.data.get(a.dataId).values,l=i.backend_util.assertAndGetBroadcastShape(Array.from(r),Array.from(o));return n.makeTensorInfo([l.length],"int32",Int32Array.from(l))}};var Ch=n(668),Eh=n(74034);const _h=(0,Tc.v)(i.ClipByValue,((t,e)=>{const n=e;return t>n.clipValueMax?n.clipValueMax:t<n.clipValueMin?n.clipValueMin:t})),Mh={kernelName:i.ClipByValue,backendName:"cpu",kernelFunc:_h};var Rh=n(73075);const Lh={kernelName:i.ComplexAbs,backendName:"cpu",kernelFunc:t=>{const{x:e}=t.inputs,n=t.backend,s=new Float32Array(i.util.sizeFromShape(e.shape)),a=n.data.get(e.dataId),r=a.complexTensorInfos.real,o=a.complexTensorInfos.imag,l=n.data.get(r.dataId).values,u=n.data.get(o.dataId).values;for(let i=0;i<l.length;i++){const t=l[i],e=u[i];s[i]=Math.hypot(t,e)}return n.makeOutput(s,e.shape,"float32")}};var Oh=n(9044);function Wh(t){const{inputs:e,backend:n}=t,{input:s}=e,i=n.data.get(s.dataId).complexTensorInfos.imag,a=n.data.get(i.dataId).values;return n.makeTensorInfo(i.shape,i.dtype,a)}const Bh={kernelName:i.Imag,backendName:"cpu",kernelFunc:Wh};var Ph=n(35995);function Uh(t){const{inputs:e,backend:n,attrs:s}=t,{axis:a}=s,r=i.util.parseAxisParam(a,e[0].shape)[0],o=e.map((t=>t.shape));i.backend_util.assertParamsConsistent(o,r);let l=i.backend_util.computeOutShape(e.map((t=>t.shape)),r);if(0===i.util.sizeFromShape(l))return n.makeTensorInfo(l,e[0].dtype,[]);const u=e.filter((t=>i.util.sizeFromShape(t.shape)>0));if(1===u.length)return(0,Fc.D)({inputs:{x:u[0]},backend:n});if("complex64"===u[0].dtype){const t=u.map((t=>(0,Ph.x)({inputs:{input:t},backend:n}))),e=u.map((t=>Wh({inputs:{input:t},backend:n}))),s=Uh({inputs:t,backend:n,attrs:{axis:r}}),i=Uh({inputs:e,backend:n,attrs:{axis:r}}),a=(0,Rh.f)({inputs:{real:s,imag:i},backend:n});return t.forEach((t=>n.disposeIntermediateTensorInfo(t))),e.forEach((t=>n.disposeIntermediateTensorInfo(t))),n.disposeIntermediateTensorInfo(s),n.disposeIntermediateTensorInfo(i),a}const c=u.map((t=>{const e=i.util.sizeFromShape(t.shape.slice(r));return Vc({inputs:{x:t},backend:n,attrs:{shape:[-1,e]}})})),h=c.map((t=>({vals:n.data.get(t.dataId).values,shape:t.shape})));l=i.backend_util.computeOutShape(c.map((t=>t.shape)),1);const p=1===c[0].shape[0],d=(0,Oh.h)(h,l,e[0].dtype,p),f=i.backend_util.computeOutShape(u.map((t=>t.shape)),r),m=n.makeTensorInfo(f,e[0].dtype,d);return c.forEach((t=>n.disposeIntermediateTensorInfo(t))),m}const Hh={kernelName:i.Concat,backendName:"cpu",kernelFunc:Uh};function Vh(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,filter:r}=e,{strides:o,pad:l,dataFormat:u,dilations:c,dimRoundingMode:h}=s;(0,Nc.C)([a,r],"conv2d");const p=i.backend_util.convertConv2DDataFormat(u),d=i.backend_util.computeConv2DInfo(a.shape,r.shape,o,c,l,h,!1,p),f=d.filterHeight,m=d.filterWidth,g=d.dilationHeight,y=d.dilationWidth,b=d.padInfo.left,k=d.padInfo.top,w="channelsLast"===d.dataFormat,v=new i.TensorBuffer(d.outShape,a.dtype),I=i.util.computeStrides(a.shape),N=i.util.computeStrides(r.shape),S=I[0],x=w?I[1]:I[2],T=w?I[2]:1,z=w?1:I[1],A=v.strides[0],F=w?v.strides[1]:v.strides[2],D=w?v.strides[2]:1,C=w?1:v.strides[1],E=n.data.get(a.dataId).values,_=n.data.get(r.dataId).values,M=v.values;for(let i=0;i<d.batchSize;++i){const t=i*S,e=i*A;for(let n=0;n<d.outHeight;++n){const s=e+n*F,i=n*d.strideHeight-k;for(let e=0;e<f;++e){const n=i+e*g;if(n<0||n>=d.inHeight)continue;const a=e*N[0],r=t+n*x;for(let t=0;t<d.outWidth;++t){const e=s+t*D,n=t*d.strideWidth-b;for(let t=0;t<m;++t){const s=n+t*y;if(s<0||s>=d.inWidth)continue;const i=r+s*T;let o=a+t*N[1];for(let t=0;t<d.inChannels;++t){const n=E[i+t*z];for(let t=0;t<d.outChannels;++t)M[e+t*C]+=n*_[o+t];o+=d.outChannels}}}}}}return n.makeTensorInfo(v.shape,v.dtype,M)}const jh={kernelName:i.Conv2D,backendName:"cpu",kernelFunc:Vh};const Gh={kernelName:i.Conv2DBackpropFilter,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,dy:r}=e,{strides:o,pad:l,dataFormat:u,dimRoundingMode:c,filterShape:h}=s;(0,Nc.C)([a,r],"conv2dBackpropFilter");const p=i.backend_util.convertConv2DDataFormat(u),d=i.backend_util.computeConv2DInfo(a.shape,h,o,1,l,c,!1,p),{strideHeight:f,strideWidth:m,filterHeight:g,filterWidth:y}=d,b="channelsLast"===d.dataFormat,k=new i.TensorBuffer(d.filterShape,"float32"),w=d.padInfo.left,v=d.padInfo.top,I=n.data.get(a.dataId).values,N=n.data.get(r.dataId).values,S=new i.TensorBuffer(a.shape,a.dtype,I),x=new i.TensorBuffer(r.shape,r.dtype,N);for(let i=0;i<g;++i){const t=Math.max(0,Math.ceil((v-i)/f)),e=Math.min(d.outHeight,(d.inHeight+v-i)/f);for(let n=0;n<y;++n){const s=Math.max(0,Math.ceil((w-n)/m)),a=Math.min(d.outWidth,(d.inWidth+w-n)/m);for(let r=0;r<d.inChannels;++r)for(let o=0;o<d.outChannels;++o){let l=0;for(let u=0;u<d.batchSize;++u)for(let c=t;c<e;++c){const t=i+c*f-v;for(let e=s;e<a;++e){const s=n+e*m-w;l+=b?S.get(u,t,s,r)*x.get(u,c,e,o):S.get(u,r,t,s)*x.get(u,o,c,e)}}k.set(l,i,n,r,o)}}}return n.makeTensorInfo(k.shape,k.dtype,k.values)}};const qh={kernelName:i.Conv2DBackpropInput,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,filter:r}=e,{inputShape:o,strides:l,pad:u,dataFormat:c,dimRoundingMode:h}=s;(0,Nc.C)([a,r],"conv2dBackpropInput");const p=i.util.computeStrides(r.shape),d=i.util.computeStrides(a.shape);let f=i.backend_util.convertConv2DDataFormat(c);const m=i.backend_util.computeConv2DInfo(o,r.shape,l,1,u,h,!1,f),g=new i.TensorBuffer(m.inShape,"float32"),y=g.values,b=n.data.get(a.dataId).values,k=n.data.get(r.dataId).values,[w,v,I]=p,{batchSize:N,filterHeight:S,filterWidth:x,inChannels:T,inHeight:z,inWidth:A,outChannels:F,outHeight:D,outWidth:C,strideHeight:E,strideWidth:_}=m;f=m.dataFormat;const M=S-1-m.padInfo.top,R=x-1-m.padInfo.left,L="channelsLast"===f,O=g.strides[0],W=L?g.strides[1]:g.strides[2],B=L?g.strides[2]:1,P=L?1:g.strides[1],U=d[0],H=L?d[1]:d[2],V=L?d[2]:1,j=L?1:d[1];for(let i=0;i<N;++i)for(let t=0;t<T;++t)for(let e=0;e<z;++e){const n=e-M,s=Math.max(0,Math.ceil(n/E)),a=Math.min(D,(S+n)/E);for(let r=0;r<A;++r){const o=r-R,l=Math.max(0,Math.ceil(o/_)),u=Math.min(C,(x+o)/_);let c=0;for(let e=s;e<a;++e){const s=e*E-n;for(let n=l;n<u;++n){const a=U*i+H*e+V*n,r=w*(S-1-s)+v*(x-1-(n*_-o))+I*t;for(let t=0;t<F;++t){c+=b[a+j*t]*k[r+t]}}}y[O*i+W*e+B*r+P*t]=c}}return n.makeTensorInfo(g.shape,g.dtype,g.values)}};const Zh={kernelName:i.Conv3D,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,filter:r}=e,{strides:o,pad:l,dilations:u}=s;(0,Nc.C)([a,r],"conv3d");const c=i.backend_util.computeConv3DInfo(a.shape,r.shape,o,u,l),{filterDepth:h,filterHeight:p,filterWidth:d,dilationDepth:f,dilationHeight:m,dilationWidth:g,padInfo:y}=c,b=y.front,k=y.left,w=y.top,v=new i.TensorBuffer(c.outShape,a.dtype),I=n.data.get(a.dataId).values,N=n.data.get(r.dataId).values,S=v.values,x=i.util.computeStrides(a.shape),T=i.util.computeStrides(r.shape);for(let i=0;i<c.batchSize;++i){const t=i*x[0],e=i*v.strides[0];for(let n=0;n<c.outDepth;++n){const s=e+n*v.strides[1],i=n*c.strideDepth-b;for(let e=0;e<h;++e){const n=i+e*f;if(n<0||n>=c.inDepth)continue;const a=e*T[0],r=t+n*x[1];for(let t=0;t<c.outHeight;++t){const e=s+t*v.strides[2],n=t*c.strideHeight-w;for(let t=0;t<p;++t){const s=n+t*m;if(s<0||s>=c.inHeight)continue;const i=a+t*T[1],o=r+s*x[2];for(let t=0;t<c.outWidth;++t){const n=e+t*c.outChannels,s=t*c.strideWidth-k;for(let t=0;t<d;++t){const e=s+t*g;if(e<0||e>=c.inWidth)continue;const a=i+t*T[2],r=o+e*c.inChannels;let l=a;for(let t=0;t<c.inChannels;++t){const e=I[r+t];for(let t=0;t<c.outChannels;++t)S[n+t]+=e*N[l+t];l+=c.outChannels}}}}}}}}return n.makeTensorInfo(v.shape,v.dtype,v.values)}};const Kh={kernelName:i.Conv3DBackpropFilterV2,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,dy:r}=e,{strides:o,pad:l,filterShape:u}=s;(0,Nc.C)([a,r],"conv3dBackpropFilterV2");const c=i.util.computeStrides(a.shape),h=i.util.computeStrides(r.shape),p=i.backend_util.computeConv3DInfo(a.shape,u,o,1,l),d=p.strideDepth,f=p.strideHeight,m=p.strideWidth,g=p.filterDepth,y=p.filterHeight,b=p.filterWidth,k=new i.TensorBuffer(p.filterShape,"float32"),w=k.values,[v,I,N,S]=k.strides,x=n.data.get(r.dataId).values,[T,z,A,F]=h,D=n.data.get(a.dataId).values,[C,E,_,M]=c,R=p.padInfo.front,L=p.padInfo.left,O=p.padInfo.top;for(let i=0;i<g;++i){const t=Math.max(0,Math.ceil((R-i)/d)),e=Math.min(p.outDepth,(p.inDepth+R-i)/d),n=i*v;for(let s=0;s<y;++s){const a=Math.max(0,Math.ceil((O-s)/f)),r=Math.min(p.outHeight,(p.inHeight+O-s)/f),o=s*I+n;for(let n=0;n<b;++n){const l=Math.max(0,Math.ceil((L-n)/m)),u=Math.min(p.outWidth,(p.inWidth+L-n)/m),c=n*N+o;for(let o=0;o<p.inChannels;++o){const h=o*S+c;for(let c=0;c<p.outChannels;++c){let g=0;for(let h=0;h<p.batchSize;++h){const p=h*C,y=h*T;for(let h=t;h<e;++h){const t=(i+h*d-R)*E+p,e=h*z+y;for(let i=a;i<r;++i){const a=(s+i*f-O)*_+t,r=i*A+e;for(let t=l;t<u;++t){const e=t*F+r;g+=D[(n+t*m-L)*M+a+o]*x[e+c]}}}}w[h+c]=g}}}}}return n.makeTensorInfo(k.shape,k.dtype,k.values)}};const Jh={kernelName:i.Conv3DBackpropInputV2,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,filter:r}=e,{pad:o,strides:l,inputShape:u}=s;(0,Nc.C)([a],"conv3dBackpropInputV2");const c=i.util.computeStrides(a.shape),h=i.util.computeStrides(r.shape),p=i.backend_util.computeConv3DInfo(u,r.shape,l,1,o),d=new i.TensorBuffer(p.inShape,"float32"),f=d.values,[m,g,y,b]=d.strides,k=n.data.get(a.dataId).values,[w,v,I,N]=c,S=n.data.get(r.dataId).values,[x,T,z,A]=h,{batchSize:F,filterDepth:D,filterHeight:C,filterWidth:E,inChannels:_,inDepth:M,inHeight:R,inWidth:L,outChannels:O,outDepth:W,outHeight:B,outWidth:P,strideDepth:U,strideHeight:H,strideWidth:V}=p,j=D-1-p.padInfo.front,G=C-1-p.padInfo.top,q=E-1-p.padInfo.left;for(let i=0;i<F;++i)for(let t=0;t<_;++t)for(let e=0;e<M;++e){const n=e-j,s=Math.max(0,Math.ceil(n/U)),a=Math.min(W,(D+n)/U);for(let r=0;r<R;++r){const o=r-G,l=Math.max(0,Math.ceil(o/H)),u=Math.min(B,(C+o)/H);for(let c=0;c<L;++c){const h=c-q,p=Math.max(0,Math.ceil(h/V)),d=Math.min(P,(E+h)/V);let F=0;for(let e=s;e<a;++e){const s=e*U-n;for(let n=l;n<u;++n){const a=n*H-o;for(let r=p;r<d;++r){const o=w*i+v*e+I*n+N*r,l=x*(D-1-s)+T*(C-1-a)+z*(E-1-(r*V-h))+A*t;for(let t=0;t<O;++t){F+=k[o+t]*S[l+t]}}}}f[m*i+g*e+y*r+b*c+t]=F}}}return n.makeTensorInfo(d.shape,d.dtype,d.values)}},Yh=(0,Tc.v)(i.Cos,(t=>Math.cos(t))),Xh={kernelName:i.Cos,backendName:"cpu",kernelFunc:Yh},Qh=(0,Tc.v)(i.Cosh,(t=>Math.cosh(t))),$h={kernelName:i.Cosh,backendName:"cpu",kernelFunc:Qh};const tp={kernelName:i.CropAndResize,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{image:a,boxes:r,boxInd:o}=e,{cropSize:l,method:u,extrapolationValue:c}=s,[h,p,d,f]=a.shape,m=r.shape[0],[g,y]=l,b=(0,i.buffer)([m,g,y,f],"float32"),k=n.data.get(r.dataId).values,w=n.data.get(o.dataId).values,v=n.data.get(a.dataId).values,I=i.util.computeStrides(a.shape),N=i.util.computeStrides(b.shape);for(let i=0;i<m;i++){const t=4*i,e=k[t],n=k[t+1],s=k[t+2],a=k[t+3],r=w[i];if(r>=h)continue;const o=g>1?(s-e)*(p-1)/(g-1):0,l=y>1?(a-n)*(d-1)/(y-1):0;for(let h=0;h<g;h++){const t=g>1?e*(p-1)+h*o:.5*(e+s)*(p-1);if(t<0||t>p-1)for(let e=0;e<y;e++)for(let t=0;t<f;t++){const n=t+e*N[2]+h*N[1]+i*N[0];b.values[n]=c}else if("bilinear"===u){const e=Math.floor(t),s=Math.ceil(t),o=t-e;for(let t=0;t<y;t++){const u=y>1?n*(d-1)+t*l:.5*(n+a)*(d-1);if(u<0||u>d-1){for(let e=0;e<f;e++){const n=e+t*N[2]+h*N[1]+i*N[0];b.values[n]=c}continue}const p=Math.floor(u),m=Math.ceil(u),g=u-p;for(let n=0;n<f;n++){let a=n+p*I[2]+e*I[1]+r*I[0];const l=v[a];a=n+m*I[2]+e*I[1]+r*I[0];const u=v[a];a=n+p*I[2]+s*I[1]+r*I[0];const c=v[a];a=n+m*I[2]+s*I[1]+r*I[0];const d=l+(u-l)*g,f=c+(v[a]-c)*g;a=n+t*N[2]+h*N[1]+i*N[0],b.values[a]=d+(f-d)*o}}}else for(let e=0;e<y;++e){const s=y>1?n*(d-1)+e*l:.5*(n+a)*(d-1);if(s<0||s>d-1){for(let t=0;t<f;t++){const n=t+e*N[2]+h*N[1]+i*N[0];b.values[n]=c}continue}const o=Math.round(s),u=Math.round(t);for(let t=0;t<f;t++){const n=t+o*I[2]+u*I[1]+r*I[0],s=t+e*N[2]+h*N[1]+i*N[0];b.values[s]=v[n]}}}}return n.makeTensorInfo(b.shape,b.dtype,b.values)}};const ep={kernelName:i.Cumprod,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,exclusive:o,reverse:l}=s;(0,Nc.C)(a,"cumprod");const u=i.backend_util.getAxesPermutation([r],a.shape.length);let c=a;null!=u&&(c=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:u}}));const h=i.backend_util.getInnerMostAxes(1,a.shape.length)[0];if(h!==c.shape.length-1)throw new Error("backend.cumprod in CPU expects an inner-most "+"axis=".concat(c.shape.length-1," but got axis=").concat(h));const p=(0,i.upcastType)(c.dtype,"int32"),d=i.util.makeOnesTypedArray(i.util.sizeFromShape(c.shape),p),f=n.data.get(c.dataId).values,m=c.shape[c.shape.length-1],g=l?(t,e)=>t+m-e-1:(t,e)=>t+e;for(let i=0;i<f.length;i+=m)for(let t=0;t<m;t++){const e=g(i,t);if(0===t)d[e]=o?1:f[e];else{const n=g(i,t-1);d[e]=o?f[n]*d[n]:f[e]*d[n]}}const y=n.makeTensorInfo(c.shape,p,d);if(null!=u){const t=i.backend_util.getUndoAxesPermutation(u),e=(0,th.m)({inputs:{x:y},backend:n,attrs:{perm:t}});return n.disposeIntermediateTensorInfo(y),n.disposeIntermediateTensorInfo(c),e}return y}};const np={kernelName:i.Cumsum,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,exclusive:o,reverse:l}=s;(0,Nc.C)(a,"cumsum");const u=i.backend_util.getAxesPermutation([r],a.shape.length);let c=a;null!=u&&(c=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:u}}));const h=i.backend_util.getInnerMostAxes(1,a.shape.length)[0];if(h!==c.shape.length-1)throw new Error("backend.cumsum in CPU expects an inner-most "+"axis=".concat(c.shape.length-1," but got axis=").concat(h));const p=(0,i.upcastType)(c.dtype,"int32"),d=i.util.makeZerosTypedArray(i.util.sizeFromShape(c.shape),p),f=n.data.get(c.dataId).values,m=c.shape[c.shape.length-1],g=l?(t,e)=>t+m-e-1:(t,e)=>t+e;for(let i=0;i<f.length;i+=m)for(let t=0;t<m;t++){const e=g(i,t);if(0===t)d[e]=o?0:f[e];else{const n=g(i,t-1);d[e]=o?f[n]+d[n]:f[e]+d[n]}}const y=n.makeTensorInfo(c.shape,p,d);if(null!=u){const t=i.backend_util.getUndoAxesPermutation(u),e=(0,th.m)({inputs:{x:y},backend:n,attrs:{perm:t}});return n.disposeIntermediateTensorInfo(y),n.disposeIntermediateTensorInfo(c),e}return y}};const sp={kernelName:i.DenseBincount,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i,weights:a}=e,{size:r,binaryOutput:o}=s;if(1===i.shape.length){const t=n.data.get(i.dataId).values,e=n.data.get(a.dataId).values,s=(0,zh.X)(t,e,a.dtype,a.shape,r);return n.makeTensorInfo([r],a.dtype,s)}if(2===i.shape.length){const t=n.bufferSync(i),e=n.bufferSync(a),s=(0,zh.N)(t,e,r,o);return n.makeTensorInfo(s.shape,a.dtype,s.values)}throw new Error("Error in denseBincount: input must be at most rank 2, but got rank"+"".concat(i.shape.length,"."))}};const ip={kernelName:i.DepthToSpace,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{blockSize:r,dataFormat:o}=s;i.util.assert("NHWC"===o,(()=>"Only NHWC dataFormat supported on CPU for depthToSpace. Got ".concat(o)));const l=a.shape[0],u=a.shape[1],c=a.shape[2],h=a.shape[3],p=u*r,d=c*r,f=h/(r*r),m=n.data.get(a.dataId).values,g=new Float32Array(l*p*d*f);let y=0;for(let i=0;i<l;++i)for(let t=0;t<p;++t){const e=Math.floor(t/r),n=t%r;for(let t=0;t<d;++t){const s=Math.floor(t/r),a=(n*r+t%r)*f;for(let t=0;t<f;++t){const n=t+a+h*(s+c*(e+u*i));g[y++]=m[n]}}}return n.makeTensorInfo([l,p,d,f],a.dtype,g)}};function ap(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,filter:r}=e,{strides:o,pad:l,dilations:u,dimRoundingMode:c}=s;(0,Nc.C)([a,r],"depthwiseConv2DNative");const h=i.util.computeStrides(a.shape),p=i.util.computeStrides(r.shape);let d=u;null==d&&(d=[1,1]),i.util.assert(i.backend_util.eitherStridesOrDilationsAreOne(o,d),(()=>"Error in depthwiseConv2d: Either strides or dilations must be "+"1. Got strides ".concat(o," and dilations '").concat(d,"'")));const f=i.backend_util.computeConv2DInfo(a.shape,r.shape,o,d,l,c,!0),{filterHeight:m,filterWidth:g,dilationHeight:y,dilationWidth:b,padInfo:k}=f,w=k.left,v=k.top,I=f.outChannels/f.inChannels,N=new i.TensorBuffer(f.outShape,a.dtype),S=n.data.get(a.dataId).values,x=n.data.get(r.dataId).values,T=N.values;for(let i=0;i<f.batchSize;++i){const t=i*h[0],e=i*N.strides[0];for(let n=0;n<f.outHeight;++n){const s=e+n*N.strides[1],i=n*f.strideHeight-v;for(let e=0;e<m;++e){const n=i+e*y;if(n<0||n>=f.inHeight)continue;const a=e*p[0],r=t+n*h[1];for(let t=0;t<f.outWidth;++t){const e=s+t*N.strides[2],n=t*f.strideWidth-w;for(let t=0;t<g;++t){const s=n+t*b;if(s<0||s>=f.inWidth)continue;const i=a+t*p[1],o=r+s*f.inChannels;let l=e,u=i;for(let t=0;t<f.inChannels;++t){const e=S[o+t];for(let t=0;t<I;++t)T[l+t]+=e*x[u+t];l+=I,u+=I}}}}}}return n.makeTensorInfo(N.shape,N.dtype,N.values)}const rp={kernelName:i.DepthwiseConv2dNative,backendName:"cpu",kernelFunc:ap};const op={kernelName:i.DepthwiseConv2dNativeBackpropFilter,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,dy:r}=e,{strides:o,dilations:l,pad:u,dimRoundingMode:c,filterShape:h}=s;(0,Nc.C)([a,r],"depthwiseConv2dNativeBackpropFilter");const p=i.backend_util.computeConv2DInfo(a.shape,h,o,l,u,c,!0),{strideHeight:d,strideWidth:f,filterHeight:m,filterWidth:g}=p,y=new i.TensorBuffer(p.filterShape,"float32"),b=p.padInfo.left,k=p.padInfo.top,w=p.outChannels/p.inChannels,v=n.data.get(a.dataId).values,I=new i.TensorBuffer(a.shape,a.dtype,v),N=n.data.get(r.dataId).values,S=new i.TensorBuffer(r.shape,r.dtype,N);for(let i=0;i<m;++i){const t=Math.max(0,Math.ceil((k-i)/d)),e=Math.min(p.outHeight,(p.inHeight+k-i)/d);for(let n=0;n<g;++n){const s=Math.max(0,Math.ceil((b-n)/f)),a=Math.min(p.outWidth,(p.inWidth+b-n)/f);for(let r=0;r<p.outChannels;++r){const o=Math.trunc(r/w),l=r%w;let u=0;for(let c=0;c<p.batchSize;++c)for(let l=t;l<e;++l){const t=i+l*d-k;for(let e=s;e<a;++e){const s=n+e*f-b;u+=I.get(c,t,s,o)*S.get(c,l,e,r)}}y.set(u,i,n,o,l)}}}return n.makeTensorInfo(y.shape,y.dtype,y.values)}};const lp={kernelName:i.DepthwiseConv2dNativeBackpropInput,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,filter:r}=e,{strides:o,dilations:l,pad:u,dimRoundingMode:c,inputShape:h}=s;(0,Nc.C)([a,r],"depthwiseConv2DNativeBackpropInput");const p=i.util.computeStrides(a.shape),d=i.util.computeStrides(r.shape),f=i.backend_util.computeConv2DInfo(h,r.shape,o,l,u,c,!0),m=new i.TensorBuffer(f.inShape,"float32"),g=m.values,[y,b,k]=m.strides,w=n.data.get(a.dataId).values,[v,I,N]=p,S=n.data.get(r.dataId).values,[x,T,z]=d,{batchSize:A,filterHeight:F,filterWidth:D,inChannels:C,inHeight:E,inWidth:_,outChannels:M,outHeight:R,outWidth:L,strideHeight:O,strideWidth:W}=f,B=F-1-f.padInfo.top,P=D-1-f.padInfo.left,U=M/C;for(let i=0;i<A;++i)for(let t=0;t<C;++t)for(let e=0;e<E;++e){const n=e-B,s=Math.max(0,Math.ceil(n/O)),a=Math.min(R,(F+n)/O);for(let r=0;r<_;++r){const o=r-P,l=Math.max(0,Math.ceil(o/W)),u=Math.min(L,(D+o)/W);let c=0;for(let e=s;e<a;++e){const s=e*O-n;for(let n=l;n<u;++n){const a=v*i+I*e+N*n,r=x*(F-1-s)+T*(D-1-(n*W-o))+z*t;for(let e=0;e<U;++e){c+=w[a+(t*U+e)]*S[r+e]}}}g[y*i+b*e+k*r+t]=c}}return n.makeTensorInfo(m.shape,m.dtype,m.values)}};const up={kernelName:i.Diag,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{x:s}=e,a=i.util.sizeFromShape(s.shape),r=n.data.get(s.dataId).values,o=(0,i.buffer)([a,a],s.dtype),l=o.values;for(let i=0;i<r.length;i++)l[i*a+i]=r[i];const u=[...s.shape,...s.shape];return n.makeTensorInfo(u,o.dtype,o.values)}},cp={kernelName:i.Dilation2D,backendName:"cpu",kernelFunc:t=>{let{inputs:e,backend:n,attrs:s}=t;const{x:a,filter:r}=e,{strides:o,pad:l,dilations:u}=s,c=n,h=c.data.get(a.dataId).values,p=a.shape.length,d=c.data.get(r.dataId).values,f=r.shape.length,{batchSize:m,inHeight:g,inWidth:y,inChannels:b,outHeight:k,outWidth:w,padInfo:v,strideHeight:I,strideWidth:N,filterHeight:S,filterWidth:x,dilationHeight:T,dilationWidth:z,outShape:A}=i.backend_util.computeDilation2DInfo(a.shape,r.shape,o,l,"NHWC",u),F=i.util.sizeFromShape(A),D=A.length,C=i.util.getArrayFromDType(a.dtype,F);for(let E=0;E<m;++E)for(let t=0;t<k;++t){const e=t*I-v.top;for(let n=0;n<w;++n){const s=n*N-v.left;for(let o=0;o<b;++o){let l=Number.MIN_SAFE_INTEGER;for(let t=0;t<S;++t){const n=e+t*T;if(n>=0&&n<g)for(let e=0;e<x;++e){const u=s+e*z;if(u>=0&&u<y){const s=i.util.locToIndex([E,n,u,o],p,i.util.computeStrides(a.shape)),c=i.util.locToIndex([t,e,o],f,i.util.computeStrides(r.shape)),m=h[s]+d[c];m>l&&(l=m)}}}C[i.util.locToIndex([E,t,n,o],D,i.util.computeStrides(A))]=l}}}return{dataId:c.write(i.util.toTypedArray(C,a.dtype),A,a.dtype),shape:A,dtype:a.dtype}}},hp={kernelName:i.Dilation2DBackpropFilter,backendName:"cpu",kernelFunc:t=>{let{inputs:e,backend:n,attrs:s}=t;const{x:a,filter:r,dy:o}=e,{strides:l,pad:u,dilations:c}=s,h=n,p=i.util.toNestedArray(a.shape,h.data.get(a.dataId).values),d=i.util.toNestedArray(r.shape,h.data.get(r.dataId).values),{batchSize:f,inHeight:m,inWidth:g,inChannels:y,outHeight:b,outWidth:k,padInfo:w,strideHeight:v,strideWidth:I,filterHeight:N,filterWidth:S,dilationHeight:x,dilationWidth:T,outShape:z}=i.backend_util.computeDilation2DInfo(a.shape,r.shape,l,u,"NHWC",c);i.util.assert(o.rank===z.length,(()=>"Error in ".concat(i.Dilation2DBackpropFilter,", dy ")+"must have the same rank as output ".concat(z.length,", but got ")+"".concat(o.rank)));const A=i.util.toNestedArray(z,h.data.get(o.dataId).values),F=i.util.makeZerosNestedTypedArray(r.shape,r.dtype);for(let i=0;i<f;++i)for(let t=0;t<b;++t){const e=t*v-w.top;for(let n=0;n<k;++n){const s=n*I-w.left;for(let a=0;a<y;++a){let r=Number.MIN_SAFE_INTEGER,o=0,l=0;for(let t=0;t<N;++t){const n=e+t*x;if(n>=0&&n<m)for(let e=0;e<S;++e){const u=s+e*T;if(u>=0&&u<g){const s=p[i][n][u][a]+d[t][e][a];s>r&&(r=s,o=t,l=e)}}}F[o][l][a]+=A[i][t][n][a]}}}return{dataId:h.write(i.util.toTypedArray(F,a.dtype),r.shape,r.dtype),shape:r.shape,dtype:r.dtype}}},pp={kernelName:i.Dilation2DBackpropInput,backendName:"cpu",kernelFunc:t=>{let{inputs:e,backend:n,attrs:s}=t;const{x:a,filter:r,dy:o}=e,{strides:l,pad:u,dilations:c}=s,h=n,p=i.util.toNestedArray(a.shape,h.data.get(a.dataId).values),d=i.util.toNestedArray(r.shape,h.data.get(r.dataId).values),{batchSize:f,inHeight:m,inWidth:g,inChannels:y,outHeight:b,outWidth:k,padInfo:w,strideHeight:v,strideWidth:I,filterHeight:N,filterWidth:S,dilationHeight:x,dilationWidth:T,outShape:z}=i.backend_util.computeDilation2DInfo(a.shape,r.shape,l,u,"NHWC",c);i.util.assert(o.rank===z.length,(()=>"Error in ".concat(i.Dilation2DBackpropInput,", dy ")+"must have the same rank as output ".concat(z.length,", but got ")+"".concat(o.rank)));const A=i.util.toNestedArray(z,h.data.get(o.dataId).values),F=i.util.makeZerosNestedTypedArray(a.shape,a.dtype);for(let i=0;i<f;++i)for(let t=0;t<b;++t){const e=t*v-w.top;for(let n=0;n<k;++n){const s=n*I-w.left;for(let a=0;a<y;++a){let r=Number.MIN_SAFE_INTEGER,o=e<0?0:e,l=s<0?0:s;for(let t=0;t<N;++t){const n=e+t*x;if(n>=0&&n<m)for(let e=0;e<S;++e){const u=s+e*T;if(u>=0&&u<g){const s=p[i][n][u][a]+d[t][e][a];s>r&&(r=s,o=n,l=u)}}}F[i][o][l][a]+=A[i][t][n][a]}}}return{dataId:h.write(i.util.toTypedArray(F,a.dtype),a.shape,a.dtype),shape:a.shape,dtype:a.dtype}}};const dp={kernelName:i.Draw,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{image:i}=e,{canvas:a,options:r}=s,{contextOptions:o,imageOptions:l}=r||{},u=(null===l||void 0===l?void 0:l.alpha)||1,c=(null===o||void 0===o?void 0:o.contextType)||"2d";if("2d"!==c)throw new Error("Context type ".concat(o.contextType," is not supported by the CPU backend."));const h=a.getContext(c,(null===o||void 0===o?void 0:o.contextAttributes)||{});if(null==h)throw new Error("Could not get the context with ".concat(c," type."));const[p,d]=i.shape.slice(0,2),f=2===i.shape.length?1:i.shape[2],m=n.data.get(i.dataId).values,g="float32"===i.dtype?255:1,y=new Uint8ClampedArray(d*p*4);for(let k=0;k<p*d;++k){const t=[0,0,0,255*u];for(let n=0;n<f;n++){const e=m[k*f+n];if("float32"===i.dtype){if(e<0||e>1)throw new Error("Tensor values for a float32 Tensor must be in the "+"range [0 - 1] but encountered ".concat(e,"."))}else if("int32"===i.dtype&&(e<0||e>255))throw new Error("Tensor values for a int32 Tensor must be in the "+"range [0 - 255] but encountered ".concat(e,"."));1===f?(t[0]=e*g,t[1]=e*g,t[2]=e*g):t[n]=e*g}const e=4*k;y[e+0]=Math.round(t[0]),y[e+1]=Math.round(t[1]),y[e+2]=Math.round(t[2]),y[e+3]=Math.round(t[3])}a.width=d,a.height=p;const b=new ImageData(y,d,p);return h.putImageData(b,0,0),i}};var fp=n(50063),mp=n(51908);function gp(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,keepDims:o}=s;let l;(0,Nc.C)(a,"sum"),l="bool"===a.dtype?(0,Ch.wg)({inputs:{x:a},backend:n,attrs:{dtype:"int32"}}):(0,Fc.D)({inputs:{x:a},backend:n});const u=l.shape.length,c=i.util.parseAxisParam(r,l.shape),h=i.backend_util.getAxesPermutation(c,u);let p=c,d=l;null!=h&&(d=(0,th.m)({inputs:{x:l},backend:n,attrs:{perm:h}}),p=i.backend_util.getInnerMostAxes(p.length,u)),i.backend_util.assertAxesAreInnerMostDims("sum",p,d.shape.length);const[f,m]=i.backend_util.computeOutAndReduceShapes(d.shape,p),g=i.backend_util.upcastType(d.dtype,"int32");let y=(0,mp.U)(n,f,g);const b=i.util.sizeFromShape(m),k=n.data.get(y.dataId).values,w=n.data.get(d.dataId).values;for(let i=0;i<k.length;++i){const t=i*b;let e=0;for(let n=0;n<b;++n)e+=w[t+n];k[i]=e}if(o){const t=y;y=Vc({inputs:{x:y},backend:n,attrs:{shape:i.backend_util.expandShapeToKeepDim(y.shape,c)}}),n.disposeIntermediateTensorInfo(t)}return n.disposeIntermediateTensorInfo(l),null!=h&&n.disposeIntermediateTensorInfo(d),y}const yp={kernelName:i.Sum,backendName:"cpu",kernelFunc:gp};const bp={kernelName:i.Einsum,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{equation:a}=s,r=e,{allDims:o,summedDims:l,idDims:u}=i.backend_util.decodeEinsumEquation(a,r.length);i.backend_util.checkEinsumDimSizes(o.length,u,r);const{path:c,steps:h}=i.backend_util.getEinsumComputePath(l,u),p=h.length;let d=null,f=o.length;const m=[];for(let g=0;g<p;++g){for(const t of h[g]){const{permutationIndices:e,expandDims:s}=i.backend_util.getEinsumPermutation(f,u[t]);let a;i.backend_util.isIdentityPermutation(e)?a=r[t]:(a=(0,th.m)({inputs:{x:r[t]},backend:n,attrs:{perm:e}}),m.push(a));const o=a.shape.slice();for(let t=0;t<s.length;++t)o.splice(s[t],0,1);i.util.arraysEqual(a.shape,o)||(a=Vc({inputs:{x:a},backend:n,attrs:{shape:o}}),m.push(a)),null===d?d=a:(d=(0,fp.lw)({inputs:{a:a,b:d},backend:n}),m.push(d))}g<p-1&&(c[g]>=0&&(d=gp({inputs:{x:d},backend:n,attrs:{axis:c[g]-(o.length-f),keepDims:!1}}),m.push(d)),f--)}for(const i of m)i!==d&&n.disposeIntermediateTensorInfo(i);return d}};const kp={kernelName:i.EluGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{dy:s,y:a}=e;(0,Nc.C)([s,a],"eluGrad");const r=new Float32Array(i.util.sizeFromShape(a.shape)),o=n.data.get(a.dataId).values,l=n.data.get(s.dataId).values;for(let i=0;i<o.length;++i){const t=o[i];r[i]=t>=0?l[i]:l[i]*(t+1)}return n.makeTensorInfo(a.shape,"float32",r)}};var wp=n(3439);const vp=i.backend_util.ERF_P,Ip=i.backend_util.ERF_A1,Np=i.backend_util.ERF_A2,Sp=i.backend_util.ERF_A3,xp=i.backend_util.ERF_A4,Tp=i.backend_util.ERF_A5,zp=(0,Tc.v)(i.Erf,(t=>{const e=Math.sign(t),n=Math.abs(t),s=1/(1+vp*n);return e*(1-((((Tp*s+xp)*s+Sp)*s+Np)*s+Ip)*s*Math.exp(-n*n))})),Ap={kernelName:i.Erf,backendName:"cpu",kernelFunc:zp};var Fp=n(84308);function Dp(t){const{inputs:e,backend:n,attrs:s}=t,{input:a}=e,{dim:r}=s,o=a.shape.length,l=a.shape.slice();let u=r;return r<0&&(i.util.assert(-(o+1)<=r,(()=>"Axis must be in the interval [".concat(-(o+1),", ").concat(o,"]"))),u=o+r+1),l.splice(u,0,1),Vc({inputs:{x:a},backend:n,attrs:{shape:l}})}const Cp={kernelName:i.ExpandDims,backendName:"cpu",kernelFunc:Dp};var Ep=n(17046);const _p=(0,Ec.Z)(((t,e)=>t/e)),Mp=(0,hh.j)(i.RealDiv,_p),Rp={kernelName:i.RealDiv,backendName:"cpu",kernelFunc:Mp};var Lp=n(24413);function Op(t,e,n){const s=t.shape,a=s[0],r=s[1],o=n.data.get(t.dataId),l=o.complexTensorInfos.real,u=o.complexTensorInfos.imag,c=[a,r],h=i.util.sizeFromShape(c),p=i.util.getTypedArrayFromDType("float32",h),d=i.util.getTypedArrayFromDType("float32",h);for(let y=0;y<a;y++){const t=(0,xh.di)({inputs:{x:l},backend:n,attrs:{begin:[y,0],size:[1,r]}}),s=(0,xh.di)({inputs:{x:u},backend:n,attrs:{begin:[y,0],size:[1,r]}}),a=(0,Rh.f)({inputs:{real:t,imag:s},backend:n}),{real:o,imag:c}=Wp(a,e,n),h=i.backend_util.mergeRealAndImagArrays(o,c);for(let e=0;e<r;e++){const t=i.backend_util.getComplexWithIndex(h,e);p[y*r+e]=t.real,d[y*r+e]=t.imag}n.disposeIntermediateTensorInfo(t),n.disposeIntermediateTensorInfo(s),n.disposeIntermediateTensorInfo(a)}const f=n.makeTensorInfo(c,"float32",p),m=n.makeTensorInfo(c,"float32",d),g=(0,Rh.f)({inputs:{real:f,imag:m},backend:n});return n.disposeIntermediateTensorInfo(f),n.disposeIntermediateTensorInfo(m),g}function Wp(t,e,n){const s=i.util.sizeFromShape(t.shape),a=n.data.get(t.dataId),r=n.data.get(a.complexTensorInfos.real.dataId).values,o=n.data.get(a.complexTensorInfos.imag.dataId).values;if(0===((l=s)&l-1)){const a=Bp(r,o,s,e,n),l=[t.shape[0],t.shape[1]];if(e){const t=n.makeTensorInfo(l,"float32",a.real),e=n.makeTensorInfo(l,"float32",a.imag),r=n.makeTensorInfo([],"float32",i.util.createScalarValue(s,"float32")),o=(0,Fc.D)({inputs:{x:r},backend:n}),u=Rp.kernelFunc({inputs:{a:t,b:r},backend:n}),c=Rp.kernelFunc({inputs:{a:e,b:o},backend:n}),h=n.data.get(u.dataId).values,p=n.data.get(c.dataId).values;return n.disposeIntermediateTensorInfo(t),n.disposeIntermediateTensorInfo(e),n.disposeIntermediateTensorInfo(r),n.disposeIntermediateTensorInfo(o),n.disposeIntermediateTensorInfo(u),n.disposeIntermediateTensorInfo(c),{real:h,imag:p}}return a}{const t=function(t,e,n){const s=new Float32Array(2*e);for(let a=0;a<e;a++){let r=0,o=0;for(let s=0;s<e;s++){const l=i.backend_util.exponent(a*s,e,n),u=i.backend_util.getComplexWithIndex(t,s);r+=u.real*l.real-u.imag*l.imag,o+=u.real*l.imag+u.imag*l.real}n&&(r/=e,o/=e),i.backend_util.assignToTypedArray(s,r,o,a)}return s}(i.backend_util.mergeRealAndImagArrays(r,o),s,e);return i.backend_util.splitRealAndImagArrays(t)}var l}function Bp(t,e,n,s,a){if(1===n)return{real:t,imag:e};const r=i.backend_util.mergeRealAndImagArrays(t,e),o=n/2,l=i.backend_util.complexWithEvenIndex(r),u=l.real,c=l.imag,h=[u.length],p=a.makeTensorInfo(h,"float32",u),d=a.makeTensorInfo(h,"float32",c),f=(0,Rh.f)({inputs:{real:p,imag:d},backend:a}),m=i.backend_util.complexWithOddIndex(r),g=m.real,y=m.imag,b=[g.length],k=a.makeTensorInfo(b,"float32",g),w=a.makeTensorInfo(b,"float32",y),v=(0,Rh.f)({inputs:{real:k,imag:w},backend:a}),I=Bp(u,c,o,s,a),N=I.real,S=I.imag,x=[N.length],T=a.makeTensorInfo(x,"float32",N),z=a.makeTensorInfo(x,"float32",S),A=(0,Rh.f)({inputs:{real:T,imag:z},backend:a}),F=Bp(g,y,o,s,a),D=F.real,C=F.imag,E=[D.length],_=a.makeTensorInfo(E,"float32",D),M=a.makeTensorInfo(E,"float32",C),R=(0,Rh.f)({inputs:{real:_,imag:M},backend:a}),L=i.backend_util.exponents(n,s),O=[L.real.length],W=a.makeTensorInfo(O,"float32",L.real),B=a.makeTensorInfo(O,"float32",L.imag),P=(0,Rh.f)({inputs:{real:W,imag:B},backend:a}),U=(0,fp.lw)({inputs:{a:P,b:R},backend:a}),H=(0,Hc.WQ)({inputs:{a:A,b:U},backend:a}),V=(0,Lp.jb)({inputs:{a:A,b:U},backend:a}),j=(0,Ph.x)({inputs:{input:H},backend:a}),G=(0,Ph.x)({inputs:{input:V},backend:a}),q=Wh({inputs:{input:H},backend:a}),Z=Wh({inputs:{input:V},backend:a}),K=Uh({inputs:[j,G],backend:a,attrs:{axis:0}}),J=Uh({inputs:[q,Z],backend:a,attrs:{axis:0}}),Y=a.data.get(K.dataId).values,X=a.data.get(J.dataId).values;return a.disposeIntermediateTensorInfo(p),a.disposeIntermediateTensorInfo(d),a.disposeIntermediateTensorInfo(f),a.disposeIntermediateTensorInfo(k),a.disposeIntermediateTensorInfo(w),a.disposeIntermediateTensorInfo(v),a.disposeIntermediateTensorInfo(T),a.disposeIntermediateTensorInfo(z),a.disposeIntermediateTensorInfo(A),a.disposeIntermediateTensorInfo(_),a.disposeIntermediateTensorInfo(M),a.disposeIntermediateTensorInfo(R),a.disposeIntermediateTensorInfo(W),a.disposeIntermediateTensorInfo(B),a.disposeIntermediateTensorInfo(P),a.disposeIntermediateTensorInfo(U),a.disposeIntermediateTensorInfo(H),a.disposeIntermediateTensorInfo(V),a.disposeIntermediateTensorInfo(j),a.disposeIntermediateTensorInfo(q),a.disposeIntermediateTensorInfo(G),a.disposeIntermediateTensorInfo(Z),a.disposeIntermediateTensorInfo(K),a.disposeIntermediateTensorInfo(J),{real:Y,imag:X}}const Pp={kernelName:i.FFT,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{input:s}=e,a=i.util.sizeFromShape(s.shape),r=s.shape[s.shape.length-1],o=Vc({inputs:{x:s},backend:n,attrs:{shape:[a/r,r]}}),l=Op(o,!1,n),u=Vc({inputs:{x:l},backend:n,attrs:{shape:s.shape}});return n.disposeIntermediateTensorInfo(o),n.disposeIntermediateTensorInfo(l),u}};function Up(t){const{backend:e,attrs:n}=t,{shape:s,value:a,dtype:r}=n,o=r||i.util.inferDtype(a),l=i.util.getArrayFromDType(o,i.util.sizeFromShape(s));return function(t,e){t.fill(e)}(l,a),e.makeTensorInfo(s,o,l)}const Hp={kernelName:i.Fill,backendName:"cpu",kernelFunc:Up};const Vp={kernelName:i.FlipLeftRight,backendName:"cpu",kernelFunc:t=>{let{inputs:e,attrs:n,backend:s}=t;const{image:a}=e,r=s,o=i.util.getTypedArrayFromDType(a.dtype,i.util.sizeFromShape(a.shape)),[l,u,c,h]=a.shape,p=r.data.get(a.dataId).values;for(let i=0;i<l;i++){const t=i*c*u*h;for(let e=0;e<u;e++){const n=e*(c*h);for(let e=0;e<c;e++){const s=e*h;for(let i=0;i<h;i++){const a=Math.round(c-e-1),r=t+n+s+i;let l=p[r];if(a>=0&&a<c){l=p[t+n+a*h+i]}o[r]=l}}}}return{dataId:r.write(o,a.shape,a.dtype),shape:a.shape,dtype:a.dtype}}};var jp=n(7742);const Gp=(0,Ec.Z)(((t,e)=>Math.floor(t/e))),qp=(0,hh.j)(i.FloorDiv,Gp,null,"int32"),Zp={kernelName:i.FloorDiv,backendName:"cpu",kernelFunc:qp};const Kp={kernelName:i.FusedConv2D,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i,filter:a,bias:r,preluActivationWeights:o}=e,{strides:l,pad:u,dataFormat:c,dilations:h,dimRoundingMode:p,activation:d,leakyreluAlpha:f}=s;let m=Vh({inputs:{x:i,filter:a},backend:n,attrs:{strides:l,pad:u,dataFormat:c,dilations:h,dimRoundingMode:p}});if(r){const t=m;if("NCHW"===c&&1===r.shape.length&&1!==r.shape[0]){const t=Vc({inputs:{x:r},backend:n,attrs:{shape:[r.shape[0],1,1]}});m=(0,Hc.WQ)({inputs:{a:m,b:t},backend:n}),n.disposeIntermediateTensorInfo(t)}else m=(0,Hc.WQ)({inputs:{a:m,b:r},backend:n});n.disposeIntermediateTensorInfo(t)}if(d){const t=m;if("NCHW"===c&&"prelu"===d&&1===o.shape.length&&1!==o.shape[0]){const t=Vc({inputs:{x:o},backend:n,attrs:{shape:[o.shape[0],1,1]}});m=Uc(n,m,d,t,f),n.disposeIntermediateTensorInfo(t)}else m=Uc(n,m,d,o,f);n.disposeIntermediateTensorInfo(t)}return m}};const Jp={kernelName:i.FusedDepthwiseConv2D,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i,filter:a,bias:r,preluActivationWeights:o}=e,{strides:l,pad:u,dataFormat:c,dilations:h,dimRoundingMode:p,activation:d,leakyreluAlpha:f}=s;let m=ap({inputs:{x:i,filter:a},backend:n,attrs:{strides:l,pad:u,dataFormat:c,dilations:h,dimRoundingMode:p}});if(r){const t=m;m=(0,Hc.WQ)({inputs:{a:m,b:r},backend:n}),n.disposeIntermediateTensorInfo(t)}if(d){const t=m;m=Uc(n,m,d,o,f),n.disposeIntermediateTensorInfo(t)}return m}};var Yp=n(15003);const Xp={kernelName:i.GatherNd,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{params:s,indices:a}=e,r=i.util.sizeFromShape(s.shape),o=a.shape,l=o[o.length-1],[u,c,h,p]=i.backend_util.prepareAndValidate(s,a);if(0===c)return n.makeTensorInfo(u,s.dtype,[]);const d=n.data.get(a.dataId).values,f=n.bufferSync(s),m=(0,Yp.q)(d,f,s.dtype,c,l,h,p,s.shape,r);return n.makeTensorInfo(u,s.dtype,m.values)}};var Qp=n(92525);const $p={kernelName:i.GatherV2,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,indices:r}=e,{axis:o,batchDims:l}=s;(0,Nc.C)([a,r],"gatherV2");const u=i.util.parseAxisParam(o,a.shape)[0],c=n.data.get(r.dataId).values,h=a.shape[u];for(let v=0;v<c.length;++v){const t=c[v];i.util.assert(t<=h-1&&t>=0,(()=>"GatherV2: the index value ".concat(t," is not in [0, ").concat(h-1,"]")))}let p=l;null==l&&(p=0);const d=i.util.sizeFromShape(r.shape),f=i.backend_util.segment_util.collectGatherOpShapeInfo(a,r,u,p),m=Vc({inputs:{x:a},backend:n,attrs:{shape:[f.batchSize,f.outerSize,f.dimSize,f.sliceSize]}}),g=Vc({inputs:{x:r},backend:n,attrs:{shape:[f.batchSize,d/f.batchSize]}}),y=[f.batchSize,f.outerSize,d/f.batchSize,f.sliceSize],b=n.bufferSync(g),k=n.bufferSync(m),w=(0,Qp.G)(k,b,y);return n.disposeIntermediateTensorInfo(m),n.disposeIntermediateTensorInfo(g),n.makeTensorInfo(f.outputShape,w.dtype,w.values)}};var td=n(11217),ed=n(98829);const nd={kernelName:i.IFFT,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{input:s}=e,a=i.util.sizeFromShape(s.shape),r=s.shape[s.shape.length-1],o=Vc({inputs:{x:s},backend:n,attrs:{shape:[a/r,r]}}),l=Op(o,!0,n),u=Vc({inputs:{x:l},backend:n,attrs:{shape:s.shape}});return n.disposeIntermediateTensorInfo(o),n.disposeIntermediateTensorInfo(l),u}},sd=(0,Tc.v)(i.IsFinite,(t=>Number.isFinite(t)?1:0),"bool"),id={kernelName:i.IsFinite,backendName:"cpu",kernelFunc:sd},ad=(0,Tc.v)(i.IsInf,(t=>Math.abs(t)===1/0?1:0),"bool"),rd={kernelName:i.IsInf,backendName:"cpu",kernelFunc:ad},od=(0,Tc.v)(i.IsNan,(t=>Number.isNaN(t)?1:0),"bool"),ld={kernelName:i.IsNan,backendName:"cpu",kernelFunc:od};var ud=n(47242),cd=n(32784),hd=n(23633);const pd={kernelName:i.LinSpace,backendName:"cpu",kernelFunc:function(t){const{backend:e,attrs:n}=t,{start:s,stop:i,num:a}=n,r=(0,hd.G)(s,i,a);return e.makeTensorInfo([r.length],"float32",r)}};var dd=n(70477);const fd=(0,Tc.v)(i.Log1p,(t=>Math.log1p(t))),md={kernelName:i.Log1p,backendName:"cpu",kernelFunc:fd},gd=(0,Ec.Z)(((t,e)=>t&&e)),yd=(0,hh.j)(i.LogicalAnd,gd,null,"bool"),bd={kernelName:i.LogicalAnd,backendName:"cpu",kernelFunc:yd},kd=(0,Tc.v)(i.LogicalNot,(t=>t?0:1),"bool"),wd={kernelName:i.LogicalNot,backendName:"cpu",kernelFunc:kd},vd=(0,Ec.Z)(((t,e)=>t||e)),Id=(0,hh.j)(i.LogicalOr,vd,null,"bool"),Nd={kernelName:i.LogicalOr,backendName:"cpu",kernelFunc:Id};const Sd={kernelName:i.LRN,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{depthRadius:r,bias:o,alpha:l,beta:u}=s;(0,Nc.C)(a,"LRN");const c=a.shape[3],h=c-1,p=n.data.get(a.dataId).values,d=i.util.sizeFromShape(a.shape),f=new Float32Array(d);function m(t){const e=t%c;let n=t-e+Math.max(0,e-r);const s=t-e+Math.min(e+r,h);let i=0;for(;n<=s;n++){const t=p[n];i+=t*t}return i}for(let i=0;i<d;i++){const t=m(i),e=p[i]*Math.pow(o+l*t,-u);f[i]=e}return n.makeTensorInfo(a.shape,a.dtype,f)}};const xd={kernelName:i.LRNGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,y:r,dy:o}=e,{depthRadius:l,bias:u,alpha:c,beta:h}=s;(0,Nc.C)(o,"LRNGrad");const p=i.util.sizeFromShape(o.shape),d=o.shape[3],f=n.data.get(o.dataId).values,m=n.data.get(a.dataId).values,g=n.data.get(r.dataId).values,y=new Float32Array(p),b=p;for(let i=0;i<b;i++){const t=i%d,e=i-t+Math.max(0,t-l),n=i-t+Math.min(d,t+l+1);let s=0;for(let i=e;i<n;i++)s+=Math.pow(m[i],2);s=c*s+u;for(let a=e;a<n;a++){let t=-2*c*h*m[a]*g[i]/s;i===a&&(t+=Math.pow(s,-h)),t*=f[i],y[a]+=t}}return n.makeTensorInfo(o.shape,a.dtype,y)}};var Td=n(56238),zd=n(82137);function Ad(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{reductionIndices:r,keepDims:o}=s,l=n;let u=a.shape;const c=u.length,h=i.util.parseAxisParam(r,u);let p=h;const d=i.backend_util.getAxesPermutation(p,c);let f=l.data.get(a.dataId).values;if(null!=d){const t=new Array(c);for(let e=0;e<t.length;e++)t[e]=u[d[e]];f=(0,zd._)(f,u,a.dtype,d,t),p=i.backend_util.getInnerMostAxes(p.length,c),u=t}(0,Nc.C)(a,"max"),i.backend_util.assertAxesAreInnerMostDims("max",p,c);const[m,g]=i.backend_util.computeOutAndReduceShapes(u,p),y=i.util.sizeFromShape(g),b=(0,Td.j)(f,y,m,a.dtype),k=l.write(b,m,a.dtype);let w=m;if(o){w=i.backend_util.expandShapeToKeepDim(m,h)}return{dataId:k,shape:w,dtype:a.dtype}}const Fd={kernelName:i.Max,backendName:"cpu",kernelFunc:Ad};var Dd=n(58617);const Cd={kernelName:i.MaxPool,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e;(0,Nc.C)(a,"maxPool");const{filterSize:r,strides:o,pad:l,dimRoundingMode:u}=s;i.util.assert(i.backend_util.eitherStridesOrDilationsAreOne(o,1),(()=>"Error in maxPool: Either strides or dilations must be 1. "+"Got strides ".concat(o," and dilations '").concat(1,"'")));const c=i.backend_util.computePool2DInfo(a.shape,r,o,1,l,u);let h;if(1===c.filterWidth&&1===c.filterHeight&&i.util.arraysEqual(c.inShape,c.outShape))h=(0,Fc.D)({inputs:{x:a},backend:n});else{const t=n.data.get(a.dataId).values,e=i.util.computeStrides(a.shape),s=yh(t,a.shape,a.dtype,e,c,"max");h=n.makeTensorInfo(c.outShape,a.dtype,s.values)}return h}};const Ed={kernelName:i.MaxPool3D,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{filterSize:r,strides:o,pad:l,dimRoundingMode:u,dataFormat:c}=s;(0,Nc.C)(a,"maxPool3d");const h=i.backend_util.computePool3DInfo(a.shape,r,o,1,l,u,c),p=kh(n.data.get(a.dataId).values,a.shape,a.dtype,i.util.computeStrides(a.shape),h,"max");return n.makeTensorInfo(p.shape,"float32",p.values)}};const _d={kernelName:i.MaxPool3DGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,input:r}=e,{filterSize:o,strides:l,pad:u,dimRoundingMode:c}=s;(0,Nc.C)([a,r],"maxPool3DGrad");const h=i.backend_util.computePool3DInfo(r.shape,o,l,1,u,c),p=function(t,e){const n=(0,i.buffer)(e.outShape,"int32"),s=e.strideDepth,a=e.strideHeight,r=e.strideWidth,o=e.dilationDepth,l=e.dilationHeight,u=e.dilationWidth,c=e.effectiveFilterDepth,h=e.effectiveFilterHeight,p=e.effectiveFilterWidth,d=e.padInfo.front,f=e.padInfo.top,m=e.padInfo.left;for(let i=0;i<e.batchSize;++i)for(let g=0;g<e.inChannels;++g)for(let y=0;y<e.outDepth;++y){const b=y*s-d;let k=b;for(;k<0;)k+=o;const w=Math.min(e.inDepth,c+b);for(let s=0;s<e.outHeight;++s){const c=s*a-f;let d=c;for(;d<0;)d+=l;const v=Math.min(e.inHeight,h+c);for(let a=0;a<e.outWidth;++a){const f=a*r-m;let I=f;for(;I<0;)I+=u;const N=Math.min(e.inWidth,p+f);let S=Number.NEGATIVE_INFINITY,x=-1;for(let e=k;e<w;e+=o){const n=e-b;for(let s=d;s<v;s+=l){const a=s-c;for(let r=I;r<N;r+=u){const o=r-f,l=t.get(i,e,s,r,g);l>=S&&(S=l,x=n*h*p+a*h+o)}}}n.set(x,i,y,s,a,g)}}}return n}(n.bufferSync(r),h),d=h.strideDepth,f=h.strideHeight,m=h.strideWidth,g=h.dilationDepth,y=h.dilationHeight,b=h.dilationWidth,k=h.effectiveFilterDepth,w=h.effectiveFilterHeight,v=h.effectiveFilterWidth,I=k-1-h.padInfo.front,N=v-1-h.padInfo.left,S=w-1-h.padInfo.top,x=(0,i.buffer)(r.shape,"float32"),T=n.bufferSync(a);for(let i=0;i<h.batchSize;++i)for(let t=0;t<h.inChannels;++t)for(let e=0;e<h.inDepth;++e)for(let n=0;n<h.inHeight;++n)for(let s=0;s<h.inWidth;++s){const a=e-I,r=n-S,o=s-N;let l=0;for(let e=0;e<k;e+=g){const n=(a+e)/d;if(!(n<0||n>=h.outDepth||Math.floor(n)!==n))for(let s=0;s<w;s+=y){const a=(r+s)/f;if(!(a<0||a>=h.outHeight||Math.floor(a)!==a))for(let r=0;r<v;r+=b){const u=(o+r)/m;if(u<0||u>=h.outWidth||Math.floor(u)!==u)continue;const c=k*w*v-1-p.get(i,n,a,u,t)===e*w*v+s*v+r?1:0;if(0===c)continue;l+=T.get(i,n,a,u,t)*c}}}x.set(l,i,e,n,s,t)}return n.makeTensorInfo(x.shape,x.dtype,x.values)}};const Md={kernelName:i.MaxPoolGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{dy:a,input:r,output:o}=e,l=r;(0,Nc.C)([r,o],"maxPoolGrad");const{filterSize:u,strides:c,pad:h,dimRoundingMode:p}=s,d=i.backend_util.computePool2DInfo(l.shape,u,c,1,h,p),f=n.data.get(l.dataId).values,m=(0,i.buffer)(d.outShape,l.dtype,bh(f,l.shape,l.dtype,d).values),g=d.strideHeight,y=d.strideWidth,b=d.dilationHeight,k=d.dilationWidth,w=d.effectiveFilterHeight,v=d.effectiveFilterWidth,I=v-1-d.padInfo.left,N=w-1-d.padInfo.top,S=(0,i.buffer)(l.shape,"float32"),x=n.data.get(a.dataId).values,T=(0,i.buffer)(a.shape,"float32",x);for(let i=0;i<d.batchSize;++i)for(let t=0;t<d.inChannels;++t)for(let e=0;e<d.inHeight;++e)for(let n=0;n<d.inWidth;++n){const s=e-N,a=n-I;let r=0;for(let e=0;e<w;e+=b){const n=(s+e)/g;if(!(n<0||n>=d.outHeight||Math.floor(n)!==n))for(let s=0;s<v;s+=k){const o=(a+s)/y;if(o<0||o>=d.outWidth||Math.floor(o)!==o)continue;const l=w*v-1-m.get(i,n,o,t)===e*v+s?1:0;if(0===l)continue;r+=T.get(i,n,o,t)*l}}S.set(r,i,e,n,t)}return n.makeTensorInfo(S.shape,S.dtype,S.values)}};const Rd={kernelName:i.MaxPoolWithArgmax,backendName:"cpu",kernelFunc:t=>{let{inputs:e,attrs:n,backend:s}=t;const{x:a}=e,{filterSize:r,strides:o,pad:l,includeBatchInIndex:u}=n,c=s;(0,Nc.C)(a,"MaxPoolWithArgmax");const h=c.data.get(a.dataId).values,p=i.backend_util.computePool2DInfo(a.shape,r,o,[1,1],l),[d,f]=function(t,e,n,s,a){const r=yh(t,0,n,i.util.computeStrides(e),a,"max"),o=bh(t,e,n,a,!0,s);return[r.values,o.values]}(h,a.shape,a.dtype,u,p),m=c.write(d,p.outShape,a.dtype),g=c.write(f,p.outShape,a.dtype);return[{dataId:m,shape:p.outShape,dtype:a.dtype},{dataId:g,shape:p.outShape,dtype:"int32"}]}};const Ld={kernelName:i.Mean,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,keepDims:o}=s,l=i.util.parseAxisParam(r,a.shape),u=i.backend_util.computeOutAndReduceShapes(a.shape,l)[1],c=i.util.sizeFromShape(u),h=[],p=n.makeTensorInfo([],"float32",new Float32Array([c]));h.push(p);const d=(0,Ch.wg)({inputs:{x:a},backend:n,attrs:{dtype:"float32"}});h.push(d);const f=Mp({inputs:{a:d,b:p},backend:n});h.push(f);const m=gp({inputs:{x:f},backend:n,attrs:{axis:r,keepDims:o}});return h.forEach((t=>n.disposeIntermediateTensorInfo(t))),m}};const Od={kernelName:i.Min,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{axis:r,keepDims:o}=s;(0,Nc.C)(a,"min");const l=i.util.parseAxisParam(r,a.shape);let u=l;const c=i.backend_util.getAxesPermutation(u,a.shape.length);let h=a;null!=c&&(h=(0,th.m)({inputs:{x:a},backend:n,attrs:{perm:c}}),u=i.backend_util.getInnerMostAxes(u.length,a.shape.length)),i.backend_util.assertAxesAreInnerMostDims("min",u,h.shape.length);const[p,d]=i.backend_util.computeOutAndReduceShapes(h.shape,u),f=i.util.sizeFromShape(d),m=i.util.makeZerosTypedArray(i.util.sizeFromShape(p),h.dtype),g=n.data.get(h.dataId).values;for(let i=0;i<m.length;++i){const t=i*f;let e=g[t];for(let n=0;n<f;++n){const s=g[t+n];(Number.isNaN(s)||s<e)&&(e=s)}m[i]=e}null!=c&&n.disposeIntermediateTensorInfo(h);const y=n.makeTensorInfo(p,h.dtype,m);if(o){const t=Vc({inputs:{x:y},backend:n,attrs:{shape:i.backend_util.expandShapeToKeepDim(p,l)}});return n.disposeIntermediateTensorInfo(y),t}return y}};var Wd=n(2491);const Bd={kernelName:i.MirrorPad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{paddings:r,mode:o}=s;(0,Nc.C)(a,"mirrorPad");const l=r.map(((t,e)=>t[0]+a.shape[e]+t[1])),u=r.map((t=>t[0])),c=r.map(((t,e)=>t[0]+a.shape[e])),h="reflect"===o?0:1,p=n.data.get(a.dataId).values,d=a.shape.length,f=i.util.computeStrides(a.shape),m=i.util.sizeFromShape(l),g=l.length,y=i.util.computeStrides(l),b=i.util.getTypedArrayFromDType(a.dtype,m);for(let k=0;k<m;k++){let t=i.util.indexToLoc(k,g,y);for(let n=0;n<g;n++)t[n]<u[n]?t[n]=2*u[n]-t[n]-h:t[n]>=c[n]&&(t[n]=2*(c[n]-1)-t[n]+h);t=t.map(((t,e)=>t-u[e]));const e=i.util.locToIndex(t,d,f);b[k]=p[e]}return{dataId:n.write(b,l,a.dtype),shape:l,dtype:a.dtype}}},Pd=(0,Ec.Z)(((t,e)=>{const n=t%e;return t<0&&e<0||t>=0&&e>=0?n:(n+e)%e})),Ud=(0,hh.j)(i.Mod,Pd),Hd={kernelName:i.Mod,backendName:"cpu",kernelFunc:Ud};function Vd(t){const{inputs:e,backend:n,attrs:s}=t,{logits:a}=e,{dim:r}=s,o=a.shape.length;let l=r;if(-1===l&&(l=o-1),l!==o-1)throw Error("Softmax along a non-last dimension is not yet supported. "+"Logits was rank ".concat(o," and dim was ").concat(l));const u=i.util.parseAxisParam([l],a.shape),c=Ad({inputs:{x:a},backend:n,attrs:{reductionIndices:u,keepDims:!1}}),h=i.backend_util.expandShapeToKeepDim(c.shape,u),p=Vc({inputs:{x:c},backend:n,attrs:{shape:h}}),d=(0,Lp.jb)({inputs:{a:a,b:p},backend:n}),f=(0,Fp.oN)({inputs:{x:d},backend:n}),m=gp({inputs:{x:f},backend:n,attrs:{axis:u,keepDims:!1}}),g=Vc({inputs:{x:m},backend:n,attrs:{shape:h}}),y=Mp({inputs:{a:f,b:g},backend:n});return n.disposeIntermediateTensorInfo(c),n.disposeIntermediateTensorInfo(p),n.disposeIntermediateTensorInfo(d),n.disposeIntermediateTensorInfo(f),n.disposeIntermediateTensorInfo(m),n.disposeIntermediateTensorInfo(g),y}const jd={kernelName:i.Softmax,backendName:"cpu",kernelFunc:Vd};const Gd={kernelName:i.Multinomial,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{logits:a}=e,{numSamples:r,seed:o,normalized:l}=s;(0,Nc.C)(a,"multinomial");const u=l?a:Vd({inputs:{logits:a},backend:n,attrs:{dim:-1}}),c=u.shape[0],h=u.shape[1],p=n.data.get(u.dataId).values,d=[c,r],f=i.util.makeZerosTypedArray(i.util.sizeFromShape(d),"int32");for(let i=0;i<c;++i){const t=i*h,e=new Float32Array(h-1);e[0]=p[t];for(let i=1;i<e.length;++i)e[i]=e[i-1]+p[t+i];const n=qu.alea(o.toString()),s=i*r;for(let i=0;i<r;++i){const t=n();f[s+i]=e.length;for(let n=0;n<e.length;n++)if(t<e[n]){f[s+i]=n;break}}}return l||n.disposeIntermediateTensorInfo(u),n.makeTensorInfo(d,"int32",f)}};var qd=n(20321);const Zd=i.kernel_impls.nonMaxSuppressionV3Impl;const Kd={kernelName:i.NonMaxSuppressionV3,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{boxes:i,scores:a}=e,{maxOutputSize:r,iouThreshold:o,scoreThreshold:l}=s;(0,Nc.C)(i,"NonMaxSuppression");const u=n.data.get(i.dataId).values,c=n.data.get(a.dataId).values,{selectedIndices:h}=Zd(u,c,r,o,l);return n.makeTensorInfo([h.length],"int32",new Int32Array(h))}},Jd=i.kernel_impls.nonMaxSuppressionV4Impl;const Yd={kernelName:i.NonMaxSuppressionV4,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{boxes:i,scores:a}=e,{maxOutputSize:r,iouThreshold:o,scoreThreshold:l,padToMaxOutputSize:u}=s;(0,Nc.C)(i,"NonMaxSuppressionPadded");const c=n.data.get(i.dataId).values,h=n.data.get(a.dataId).values,{selectedIndices:p,validOutputs:d}=Jd(c,h,r,o,l,u);return[n.makeTensorInfo([p.length],"int32",new Int32Array(p)),n.makeTensorInfo([],"int32",new Int32Array([d]))]}},Xd=i.kernel_impls.nonMaxSuppressionV5Impl;const Qd={kernelName:i.NonMaxSuppressionV5,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{boxes:i,scores:a}=e,{maxOutputSize:r,iouThreshold:o,scoreThreshold:l,softNmsSigma:u}=s;(0,Nc.C)(i,"NonMaxSuppressionWithScore");const c=n.data.get(i.dataId).values,h=n.data.get(a.dataId).values,p=r,d=o,f=l,m=u,{selectedIndices:g,selectedScores:y}=Xd(c,h,p,d,f,m);return[n.makeTensorInfo([g.length],"int32",new Int32Array(g)),n.makeTensorInfo([y.length],"float32",new Float32Array(y))]}};var $d=n(38440);const tf={kernelName:i.OneHot,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{indices:a}=e,{dtype:r,depth:o,onValue:l,offValue:u}=s;(0,Nc.C)(a,"oneHot");const c=i.util.sizeFromShape(a.shape),h=new Float32Array(c*o);h.fill(u);const p=n.data.get(a.dataId).values;for(let i=0;i<c;++i)p[i]>=0&&p[i]<o&&(h[i*o+p[i]]=l);return n.makeTensorInfo([...a.shape,o],r,h)}};function ef(t){const{inputs:e,backend:n}=t,{x:s}=e;if("string"===s.dtype)throw new Error("zerosLike is not supported for string tensors");if("complex64"===s.dtype){const t=(0,Ph.x)({inputs:{input:s},backend:n}),e=ef({inputs:{x:t},backend:n}),i=Wh({inputs:{input:s},backend:n}),a=ef({inputs:{x:i},backend:n}),r=(0,Rh.f)({inputs:{real:e,imag:a},backend:n});return n.disposeIntermediateTensorInfo(t),n.disposeIntermediateTensorInfo(e),n.disposeIntermediateTensorInfo(i),n.disposeIntermediateTensorInfo(a),r}return Up({backend:n,attrs:{shape:s.shape,value:0,dtype:s.dtype}})}const nf={kernelName:i.ZerosLike,backendName:"cpu",kernelFunc:ef};const sf={kernelName:i.OnesLike,backendName:"cpu",kernelFunc:function t(e){const{inputs:n,backend:s}=e,{x:i}=n;if("string"===i.dtype)throw new Error("onesLike is not supported for string tensors");if("complex64"===i.dtype){const e=(0,Ph.x)({inputs:{input:i},backend:s}),n=t({inputs:{x:e},backend:s}),a=Wh({inputs:{input:i},backend:s}),r=ef({inputs:{x:a},backend:s}),o=(0,Rh.f)({inputs:{real:n,imag:r},backend:s});return s.disposeIntermediateTensorInfo(e),s.disposeIntermediateTensorInfo(n),s.disposeIntermediateTensorInfo(a),s.disposeIntermediateTensorInfo(r),o}return Up({backend:s,attrs:{shape:i.shape,value:1,dtype:i.dtype}})}};function af(t){const{inputs:e,backend:n,attrs:s}=t,{axis:a}=s;if(1===e.length)return Dp({inputs:{input:e[0]},backend:n,attrs:{dim:a}});const r=e[0].shape,o=e[0].dtype;e.forEach((t=>{i.util.assertShapesMatch(r,t.shape,"All tensors passed to stack must have matching shapes"),i.util.assert(o===t.dtype,(()=>"All tensors passed to stack must have matching dtypes"))}));const l=[],u=Uh({inputs:e.map((t=>{const e=Dp({inputs:{input:t},backend:n,attrs:{dim:a}});return l.push(e),e})),backend:n,attrs:{axis:a}});return l.forEach((t=>n.disposeIntermediateTensorInfo(t))),u}const rf={kernelName:i.Pack,backendName:"cpu",kernelFunc:af};const of={kernelName:i.PadV2,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{paddings:r,constantValue:o}=s;(0,Nc.C)(a,"pad");const l=r.map(((t,e)=>t[0]+a.shape[e]+t[1])),u=r.map((t=>t[0])),c=n.data.get(a.dataId).values,h=i.util.sizeFromShape(a.shape),p=a.shape.length,d=i.util.computeStrides(a.shape),f=i.util.sizeFromShape(l),m=l.length,g=i.util.computeStrides(l),y=i.util.getTypedArrayFromDType(a.dtype,f);0!==o&&y.fill(o);for(let b=0;b<h;b++){const t=i.util.indexToLoc(b,p,d).map(((t,e)=>t+u[e]));y[i.util.locToIndex(t,m,g)]=c[b]}return{dataId:n.write(y,l,a.dtype),shape:l,dtype:a.dtype}}},lf=(0,Ec.Z)(((t,e)=>Math.pow(t,e))),uf=(0,hh.j)(i.Pow,lf),cf={kernelName:i.Pow,backendName:"cpu",kernelFunc:uf};var hf=n(51206),pf=n(45335);const df={kernelName:i.RaggedGather,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{paramsNestedSplits:i,paramsDenseValues:a,indices:r}=e,{outputRaggedRank:o}=s,l=i.map((t=>n.data.get(t.dataId).values)),u=i.map((t=>t.shape)),c=n.data.get(a.dataId).values,h=n.data.get(r.dataId).values,[p,d,f]=(0,pf.u)(l,u,c,a.shape,a.dtype,h,r.shape,o),m=p.map((t=>n.makeTensorInfo([t.length],"int32",t))),g=n.makeTensorInfo(f,a.dtype,d);return m.concat([g])}};var ff=n(14417);const mf={kernelName:i.RaggedRange,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{starts:s,limits:i,deltas:a}=e,r=n.data.get(s.dataId).values,o=n.data.get(i.dataId).values,l=n.data.get(a.dataId).values,[u,c]=(0,ff._)(r,s.shape,s.dtype,o,i.shape,l,a.shape);return[n.makeTensorInfo([u.length],"int32",u),n.makeTensorInfo([c.length],s.dtype,c)]}};var gf=n(78545);const yf={kernelName:i.RaggedTensorToTensor,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{shape:i,values:a,defaultValue:r,rowPartitionTensors:o}=e,{rowPartitionTypes:l}=s,u=n.data.get(i.dataId).values,c=n.data.get(a.dataId).values,h=n.data.get(r.dataId).values,p=o.map((t=>n.data.get(t.dataId).values)),d=o.map((t=>t.shape)),[f,m]=(0,gf.K)(u,i.shape,c,a.shape,a.dtype,h,r.shape,p,d,l);return n.makeTensorInfo(f,a.dtype,m)}};var bf=n(56635);const kf={kernelName:i.Range,backendName:"cpu",kernelFunc:function(t){const{backend:e,attrs:n}=t,{start:s,stop:i,dtype:a,step:r}=n,o=(0,bf.q)(s,i,r,a);return e.makeTensorInfo([o.length],a,o)}},wf=(0,Tc.v)(i.Reciprocal,(t=>1/t)),vf={kernelName:i.Reciprocal,backendName:"cpu",kernelFunc:wf};const If={kernelName:i.ResizeBilinear,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{images:a}=e,{alignCorners:r,halfPixelCenters:o,size:l}=s;(0,Nc.C)(a,"resizeBilinear");const u=i.util.computeStrides(a.shape),[c,h]=l,[p,d,f,m]=a.shape,g=n.data.get(a.dataId).values,y=new Float32Array(i.util.sizeFromShape([p,c,h,m])),b=[r&&c>1?d-1:d,r&&h>1?f-1:f],k=[r&&c>1?c-1:c,r&&h>1?h-1:h];let w=0;const v=b[0]/k[0],I=b[1]/k[1];for(let i=0;i<p;i++)for(let t=0;t<c;t++){let e;e=o?v*(t+.5)-.5:v*t;const n=Math.max(0,Math.floor(e)),s=e-n,a=Math.min(d-1,Math.ceil(e)),r=i*u[0]+n*u[1],l=i*u[0]+a*u[1];for(let t=0;t<h;t++){let e;e=o?I*(t+.5)-.5:I*t;const n=Math.max(0,Math.floor(e)),i=e-n,a=Math.min(f-1,Math.ceil(e)),c=r+n*u[2],h=l+n*u[2],p=r+a*u[2],d=l+a*u[2];for(let t=0;t<m;t++){const e=g[c+t],n=g[h+t],a=e+(g[p+t]-e)*i,r=a+(n+(g[d+t]-n)*i-a)*s;y[w++]=r}}}return n.makeTensorInfo([p,c,h,m],"float32",y)}};const Nf={kernelName:i.ResizeBilinearGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{images:a,dy:r}=e,{alignCorners:o}=s;(0,Nc.C)([r,a],"resizeBilinearGrad");const l=i.util.computeStrides(a.shape),[u,c,h,p]=a.shape,[,d,f]=r.shape,m=new Float32Array(u*c*h*p),g=[o&&d>1?c-1:c,o&&f>1?h-1:h],y=[o&&d>1?d-1:d,o&&f>1?f-1:f],b=g[0]/y[0],k=g[1]/y[1],w=n.data.get(r.dataId).values;let v=0;for(let i=0;i<u;i++){const t=i*l[0];for(let e=0;e<d;e++){const n=e*b,s=Math.floor(n),i=Math.min(Math.ceil(n),c-1),a=t+s*l[1],r=t+i*l[1],o=n-s,u=1-o;for(let t=0;t<f;t++){const e=t*k,n=Math.floor(e),s=Math.min(Math.ceil(e),h-1),i=e-n,c=1-i,d=a+n*l[2],f=a+s*l[2],g=r+n*l[2],y=r+s*l[2],b=u*c,I=u*i,N=o*c,S=o*i;for(let t=0;t<p;t++){const e=w[v++];m[d+t]+=e*b,m[f+t]+=e*I,m[g+t]+=e*N,m[y+t]+=e*S}}}}return n.makeTensorInfo([u,h,c,p],"float32",m)}};const Sf={kernelName:i.ResizeNearestNeighbor,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{images:a}=e,{alignCorners:r,halfPixelCenters:o,size:l}=s;(0,Nc.C)(a,"resizeNearestNeighbor");const u=i.util.computeStrides(a.shape),[c,h]=l,[p,d,f,m]=a.shape,g=n.data.get(a.dataId).values,y=new Float32Array(p*c*h*m),b=[r&&c>1?d-1:d,r&&h>1?f-1:f],k=[r&&c>1?c-1:c,r&&h>1?h-1:h],w=b[0]/k[0],v=b[1]/k[1];let I=0;for(let i=0;i<p;i++){const t=i*u[0];for(let e=0;e<c;e++){const n=o?w*(e+.5):w*e;let s=Math.min(d-1,r?Math.round(n):Math.floor(n));o&&(s=Math.max(0,s));const i=t+s*u[1];for(let t=0;t<h;t++){const e=o?v*(t+.5):v*t;let n=Math.min(f-1,r?Math.round(e):Math.floor(e));o&&(n=Math.max(0,n));const s=i+n*u[2];for(let t=0;t<m;t++){const e=g[s+t];y[I++]=e}}}}return n.makeTensorInfo([p,c,h,m],a.dtype,y)}};const xf={kernelName:i.ResizeNearestNeighborGrad,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{images:a,dy:r}=e,{alignCorners:o}=s;(0,Nc.C)([r,a],"resizeNearestNeighborGrad");const l=i.util.computeStrides(a.shape),u=i.util.computeStrides(r.shape),[c,h,p,d]=a.shape,[,f,m]=r.shape,g=new Float32Array(c*h*p*d),y=n.data.get(r.dataId).values,b=[o&&f>1?h-1:h,o&&m>1?p-1:p],k=[o&&f>1?f-1:f,o&&m>1?m-1:m],w=b[0]/k[0],v=b[1]/k[1],I=1/w,N=1/v,S=2*Math.ceil(I)+2,x=2*Math.ceil(N)+2;for(let i=0;i<c;i++){const t=i*l[0];for(let e=0;e<h;e++){const n=t+e*l[1],s=Math.floor(e*I),i=Math.floor(s-S/2);for(let a=0;a<p;a++){const s=n+a*l[2],r=Math.floor(a*N),c=Math.floor(r-x/2);for(let n=0;n<d;n++){let r=0;for(let s=0;s<S;s++){const l=s+i;if(l<0||l>=f)continue;const d=t+l*u[1],g=l*w;if(e===Math.min(h-1,o?Math.round(g):Math.floor(g)))for(let t=0;t<x;t++){const e=t+c;if(e<0||e>=m)continue;const s=d+e*u[2],i=e*v;a===Math.min(p-1,o?Math.round(i):Math.floor(i))&&(r+=y[s+n])}}g[s+n]=r}}}}return n.makeTensorInfo(a.shape,a.dtype,g)}};const Tf={kernelName:i.Reverse,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{dims:r}=s;(0,Nc.C)(a,"reverse");const o=a.shape.length,l=i.util.parseAxisParam(r,a.shape);if(0===o)return(0,Fc.D)({inputs:{x:a},backend:n});const u=new i.TensorBuffer(a.shape,a.dtype),c=n.bufferSync(a);for(let i=0;i<u.size;i++){const t=u.indexToLoc(i),e=t.slice();l.forEach((t=>e[t]=a.shape[t]-1-e[t])),u.set(c.get(...e),...t)}return n.makeTensorInfo(u.shape,u.dtype,u.values)}},zf={kernelName:i.RotateWithOffset,backendName:"cpu",kernelFunc:t=>{let{inputs:e,attrs:n,backend:s}=t;const{image:a}=e,{radians:r,fillValue:o,center:l}=n,u=s,c=i.util.getTypedArrayFromDType(a.dtype,i.util.sizeFromShape(a.shape)),[h,p,d,f]=a.shape,[m,g]=i.backend_util.getImageCenter(l,p,d),y=Math.sin(r),b=Math.cos(r),k=u.data.get(a.dataId).values;for(let i=0;i<h;i++){const t=i*d*p*f;for(let e=0;e<p;e++){const n=e*(d*f);for(let s=0;s<d;s++){const i=s*f;for(let a=0;a<f;a++){const r=[h,e,s,a],l=r[2],u=r[1];let w=(l-m)*b-(u-g)*y,v=(l-m)*y+(u-g)*b;w=Math.round(w+m),v=Math.round(v+g);let I=o;if("number"!==typeof o&&(I=3===a?255:o[a]),w>=0&&w<d&&v>=0&&v<p){I=k[t+v*(d*f)+w*f+a]}c[t+n+i+a]=I}}}}return{dataId:u.write(c,a.shape,a.dtype),shape:a.shape,dtype:a.dtype}}},Af=(0,Tc.v)(i.Round,(t=>{const e=Math.floor(t);return t-e<.5?Math.floor(t):t-e>.5?Math.ceil(t):e%2===0?e:e+1})),Ff={kernelName:i.Round,backendName:"cpu",kernelFunc:Af};var Df=n(3555),Cf=n(526);const Ef={kernelName:i.ScatterNd,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{indices:a,updates:r}=e,{shape:o}=s,{sliceRank:l,numUpdates:u,sliceSize:c,strides:h,outputSize:p}=i.backend_util.calculateShapes(r,a,o),d=n.bufferSync(a),f=n.bufferSync(r),m=(0,Cf.b)(d,f,o,p,c,u,l,h,0,!0);return n.makeTensorInfo(o,m.dtype,m.values)}};function _f(t,e){let n=0,s=t.length,i=0;for(;n<s;)i=Math.floor((n+s)/2),t[i]<e?n=i+1:s=i;return s}function Mf(t,e){let n=0,s=t.length,i=0;for(;n<s;)i=Math.floor((n+s)/2),t[i]<=e?n=i+1:s=i;return s}const Rf={kernelName:i.SearchSorted,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{sortedSequence:a,values:r}=e,{side:o}=s,l=function(t,e,n,s,a,r){const o=i.util.getArrayFromDType("int32",n*a);for(let i=0;i<n;++i){const n=t.slice(i*s,(i+1)*s),l=i*a;for(let t=0;t<a;++t)o[l+t]="left"===r?_f(n,e[t+l]):Mf(n,e[t+l])}return o}(n.data.get(a.dataId).values,n.data.get(r.dataId).values,a.shape[0],a.shape[1],r.shape[1],o);return n.makeTensorInfo(r.shape,"int32",l)}};const Lf={kernelName:i.Select,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{condition:s,t:a,e:r}=e;(0,Nc.C)([s,a,r],"select");const o=s.shape.length,l=n.data.get(s.dataId).values,u=n.data.get(a.dataId).values,c=n.data.get(r.dataId).values,h=(0,i.upcastType)(a.dtype,r.dtype),p=i.util.makeZerosTypedArray(i.util.sizeFromShape(a.shape),h);let d=0;const f=0===o||o>1||1===a.shape.length?1:i.util.sizeFromShape(a.shape.slice(1));for(let i=0;i<l.length;i++)for(let t=0;t<f;t++)1===l[i]?p[d++]=u[i]:p[d++]=c[i];return n.makeTensorInfo(a.shape,h,p)}},Of=i.backend_util.SELU_SCALEALPHA,Wf=i.backend_util.SELU_SCALE,Bf=(0,Tc.v)(i.Selu,(t=>t>=0?Wf*t:Of*(Math.exp(t)-1))),Pf={kernelName:i.Selu,backendName:"cpu",kernelFunc:Bf},Uf=(0,Tc.v)(i.Sign,(t=>t<0?-1:t>0?1:0)),Hf={kernelName:i.Sign,backendName:"cpu",kernelFunc:Uf},Vf=(0,Tc.v)(i.Sin,(t=>Math.sin(t))),jf={kernelName:i.Sin,backendName:"cpu",kernelFunc:Vf},Gf=(0,Tc.v)(i.Sinh,(t=>Math.sinh(t))),qf={kernelName:i.Sinh,backendName:"cpu",kernelFunc:Gf},Zf=Math.log(1.1920928955078125e-7)+2,Kf=(0,Tc.v)(i.Softplus,(t=>{const e=t>-Zf,n=t<Zf,s=Math.exp(t);let i;return i=n?s:e?t:Math.log(1+s),i})),Jf={kernelName:i.Softplus,backendName:"cpu",kernelFunc:Kf};const Yf={kernelName:i.SpaceToBatchND,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{blockShape:r,paddings:o}=s;(0,Nc.C)([a],"spaceToBatchND");const l=i.util.sizeFromShape(r),u=[[0,0]];u.push(...o);for(let i=1+r.length;i<a.shape.length;++i)u.push([0,0]);const c=of.kernelFunc({inputs:{x:a},backend:n,attrs:{paddings:u,constantValue:0}}),h=i.backend_util.getReshaped(c.shape,r,l,!1),p=i.backend_util.getPermuted(h.length,r.length,!1),d=i.backend_util.getReshapedPermuted(c.shape,r,l,!1),f=Vc({inputs:{x:c},backend:n,attrs:{shape:h}}),m={x:f},g={perm:p},y=(0,th.m)({inputs:m,backend:n,attrs:g}),b=Vc({inputs:{x:y},backend:n,attrs:{shape:d}});return n.disposeIntermediateTensorInfo(c),n.disposeIntermediateTensorInfo(f),n.disposeIntermediateTensorInfo(y),b}};var Xf=n(22785);const Qf={kernelName:i.SparseFillEmptyRows,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{indices:s,values:i,denseShape:a,defaultValue:r}=e;if(1!==a.shape.length)throw new Error("Dense shape must be a vector, saw:\n        ".concat(a.shape));if(2!==s.shape.length)throw new Error("Indices must be a matrix, saw:\n        ".concat(s.shape));if(1!==i.shape.length)throw new Error("Values must be a vector, saw:\n        ".concat(i.shape));if(0!==r.shape.length)throw new Error("Default value must be a scalar, saw:\n        ".concat(r.shape));const o=n.data.get(s.dataId).values,l=n.data.get(i.dataId).values,u=n.data.get(a.dataId).values,c=n.data.get(r.dataId).values[0],[h,p,d,f,m]=(0,Xf.y)(o,s.shape,s.dtype,l,i.dtype,u,c);return[n.makeTensorInfo(p,s.dtype,h),n.makeTensorInfo([p[0]],i.dtype,d),n.makeTensorInfo([f.length],"bool",new Uint8Array(f.map((t=>Number(t))))),n.makeTensorInfo([m.length],s.dtype,new Int32Array(m))]}};var $f=n(50706);const tm={kernelName:i.SparseReshape,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{inputIndices:s,inputShape:i,newShape:a}=e;if(2!==s.shape.length)throw new Error("Input indices should be a matrix but received shape\n        ".concat(s.shape));if(1!==i.shape.length)throw new Error("Input shape should be a vector but received shape\n        ".concat(i.shape));if(1!==a.shape.length)throw new Error("Target shape should be a vector but received shape ".concat(a.shape));const r=Array.from(n.data.get(i.dataId).values),o=n.data.get(s.dataId).values,l=Array.from(n.data.get(a.dataId).values),[u,c,h]=(0,$f.l)(o,s.shape,s.dtype,r,l);return[n.makeTensorInfo(c,s.dtype,u),n.makeTensorInfo([h.length],a.dtype,new Int32Array(h))]}};var em=n(53326);const nm={kernelName:i.SparseSegmentMean,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{data:s,indices:i,segmentIds:a}=e;if(s.shape.length<1)throw new Error("Data should be at least 1 dimensional but received scalar");if(1!==i.shape.length)throw new Error("Indices should be a vector but received shape\n          ".concat(i.shape));if(1!==a.shape.length)throw new Error("Segment ids should be a vector but received shape\n          ".concat(a.shape));if(i.shape[0]!==a.shape[0])throw new Error("segmentIds and indices should have same size.");const r=n.data.get(s.dataId).values,o=n.data.get(i.dataId).values,l=n.data.get(a.dataId).values,[u,c]=(0,em.z)(r,s.shape,s.dtype,o,l,!0);return n.makeTensorInfo(c,s.dtype,u)}};const sm={kernelName:i.SparseSegmentSum,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{data:s,indices:i,segmentIds:a}=e;if(s.shape.length<1)throw new Error("Data should be at least 1 dimensional but received scalar");if(1!==i.shape.length)throw new Error("Indices should be a vector but received shape\n         ".concat(i.shape));if(1!==a.shape.length)throw new Error("Segment ids should be a vector but received shape\n         ".concat(a.shape));if(i.shape[0]!==a.shape[0])throw new Error("segmentIds and indices should have same size.");const r=n.data.get(s.dataId).values,o=n.data.get(i.dataId).values,l=n.data.get(a.dataId).values,[u,c]=(0,em.z)(r,s.shape,s.dtype,o,l);return n.makeTensorInfo(c,s.dtype,u)}};const im={kernelName:i.SparseToDense,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{sparseIndices:a,sparseValues:r,defaultValue:o}=e,{outputShape:l}=s,{sliceRank:u,numUpdates:c,sliceSize:h,strides:p,outputSize:d}=i.backend_util.calculateShapes(r,a,l),f=!1,m=n.bufferSync(a);let g;switch(r.dtype){case"bool":{const t=n.bufferSync(r),e=Boolean(n.data.get(o.dataId).values[0]);g=(0,Cf.b)(m,t,l,d,h,c,u,p,e,f);break}case"float32":{const t=n.bufferSync(r),e=n.data.get(o.dataId).values[0];g=(0,Cf.b)(m,t,l,d,h,c,u,p,e,f);break}case"int32":{const t=n.bufferSync(r),e=n.data.get(o.dataId).values[0];g=(0,Cf.b)(m,t,l,d,h,c,u,p,e,f);break}case"string":{const t=n.bufferSync(r),e=i.util.decodeString(n.data.get(o.dataId).values[0]);g=(0,Cf.b)(m,t,l,d,h,c,u,p,e,f);break}default:throw new Error("Unsupported type ".concat(r.dtype))}return n.makeTensorInfo(l,g.dtype,g.values)}};const am={kernelName:i.SplitV,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{numOrSizeSplits:r,axis:o}=s,l=i.util.parseAxisParam(o,a.shape)[0],u=i.backend_util.prepareSplitSize(a,r,l),c=new Array(a.shape.length).fill(0),h=a.shape.slice();return u.map((t=>{const e=[...h];e[l]=t;const s=(0,xh.di)({inputs:{x:a},backend:n,attrs:{begin:c,size:e}});return c[l]+=t,s}))}};var rm=n(98041);const om={kernelName:i.Square,backendName:"cpu",kernelFunc:t=>{let{inputs:e,backend:n}=t;const{x:s}=e,i=n;(0,Nc.C)(s,"square");const a=i.data.get(s.dataId).values,r=new Float32Array(a.length);for(let o=0;o<a.length;++o){const t=a[o];r[o]=t*t}return{dataId:i.write(r,s.shape,s.dtype),shape:s.shape,dtype:s.dtype}}},lm=(0,Ec.Z)(((t,e)=>{const n=t-e;return n*n})),um=(0,hh.j)(i.SquaredDifference,lm),cm={kernelName:i.SquaredDifference,backendName:"cpu",kernelFunc:um};var hm=n(67844);const pm=(0,Tc.v)(i.Step,((t,e)=>{const n=e;return isNaN(t)?NaN:t>0?1:n.alpha})),dm={kernelName:i.Step,backendName:"cpu",kernelFunc:pm};var fm=n(3445);const mm={kernelName:i.StridedSlice,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a}=e,{begin:r,end:o,strides:l,beginMask:u,endMask:c,ellipsisMask:h,newAxisMask:p,shrinkAxisMask:d}=s;(0,Nc.C)(a,"stridedSlice");const{finalShapeSparse:f,finalShape:m,isIdentity:g,sliceDim0:y,isSimpleSlice:b,begin:k,end:w,strides:v}=i.slice_util.sliceInfo(a.shape,r,o,l,u,c,h,p,d);let I;if(g)I=Vc({inputs:{x:a},backend:n,attrs:{shape:m}});else if(y||b){i.util.assert(a.shape.length>=1,(()=>"Input must have rank at least 1, got: ".concat(a.shape.length)));const t=i.slice_util.computeOutShape(k,w,v),e=(0,xh.di)({inputs:{x:a},backend:n,attrs:{begin:k,size:t}});I=Vc({inputs:{x:e},backend:n,attrs:{shape:m}}),n.disposeIntermediateTensorInfo(e)}else{const t=n.bufferSync(a),e=(0,fm.e)(f,t,v,k);I=n.makeTensorInfo(m,e.dtype,e.values)}return I}};var gm=n(58793);const ym={kernelName:i.StringNGrams,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{separator:i,nGramWidths:a,leftPad:r,rightPad:o,padWidth:l,preserveShortSequences:u}=s,{data:c,dataSplits:h}=e,p=n.data.get(c.dataId).values,d=n.data.get(h.dataId).values,[f,m]=(0,gm.G)(p,d,i,a,r,o,l,u);return[n.makeTensorInfo([f.length],"string",f),n.makeTensorInfo(h.shape,"int32",m)]}};var bm=n(37185);const km={kernelName:i.StringSplit,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{skipEmpty:i}=s,{input:a,delimiter:r}=e;if("string"!==a.dtype)throw new Error("Input must be of datatype string");if(1!==a.shape.length)throw new Error("Input must be a vector, got shape: ".concat(a.shape));if(0!==r.shape.length)throw new Error("Delimiter must be a scalar, got shape: ".concat(r.shape));const o=n.data.get(a.dataId).values,l=n.data.get(r.dataId).values[0],[u,c,h]=(0,bm.S)(o,l,i),p=c.length;return[n.makeTensorInfo([p,2],"int32",u),n.makeTensorInfo([p],"string",c),n.makeTensorInfo([2],"int32",new Int32Array(h))]}};var wm=n(82536);const vm={kernelName:i.StringToHashBucketFast,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{numBuckets:i}=s,{input:a}=e;if("string"!==a.dtype)throw new Error("Input must be of datatype string");if(i<=0)throw new Error("Number of buckets must be at least 1");const r=n.data.get(a.dataId).values,o=(0,wm.f)(r,i);return n.makeTensorInfo(a.shape,"int32",o)}},Im=(0,Tc.v)(i.Tan,(t=>Math.tan(t))),Nm={kernelName:i.Tan,backendName:"cpu",kernelFunc:Im},Sm=(0,Tc.v)(i.Tanh,(t=>Math.tanh(t))),xm={kernelName:i.Tanh,backendName:"cpu",kernelFunc:Sm};const Tm={kernelName:i.TensorScatterUpdate,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n}=t,{tensor:s,indices:a,updates:r}=e,{sliceRank:o,numUpdates:l,sliceSize:u,strides:c,outputSize:h}=i.backend_util.calculateShapes(r,a,s.shape),p=n.bufferSync(a),d=n.bufferSync(r),f=n.bufferSync(s),m=(0,Cf.b)(p,d,s.shape,h,u,l,o,c,f,!1);return n.makeTensorInfo(s.shape,m.dtype,m.values)}};var zm=n(18024);const Am={kernelName:i.Tile,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i}=e,{reps:a}=s;(0,Nc.C)(i,"tile");const r=(0,zm.D)(n.bufferSync(i),a);return n.makeTensorInfo(r.shape,r.dtype,r.values)}};var Fm=n(78036);const Dm={kernelName:i.TopK,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:i}=e,{k:a,sorted:r}=s;(0,Nc.C)(i,"topk");const o=n.data.get(i.dataId).values,[l,u]=(0,Fm.x)(o,i.shape,i.dtype,a,r);return[n.makeTensorInfo(l.shape,l.dtype,l.values),n.makeTensorInfo(u.shape,u.dtype,u.values)]}};const Cm={kernelName:i.Transform,backendName:"cpu",kernelFunc:function(t){const{inputs:e,attrs:n,backend:s}=t,{image:a,transforms:r}=e,{interpolation:o,fillMode:l,fillValue:u,outputShape:c}=n,[h,p,d,f]=a.shape,[m,g]=null!=c?c:[p,d],y=[h,m,g,f],b=i.util.computeStrides(a.shape),k=b[0],w=b[1],v=b[2],I=i.util.computeStrides(y),N=I[0],S=I[1],x=I[2],T=i.util.getTypedArrayFromDType(a.dtype,i.util.sizeFromShape(y));T.fill(u);const z=s.data.get(a.dataId).values,A=s.data.get(r.dataId).values;for(let i=0;i<h;++i){const t=1===r.shape[0]?A:A.subarray(8*i,8*i+8);for(let e=0;e<m;++e)for(let n=0;n<g;++n)for(let s=0;s<f;++s){let a;const r=t[6]*n+t[7]*e+1;if(0===r)continue;const c=(t[0]*n+t[1]*e+t[2])/r,h=(t[3]*n+t[4]*e+t[5])/r,f=Em(c,d,l),m=Em(h,p,l);switch(o){case"nearest":a=Mm(z,p,d,k,w,v,i,m,f,s,u);break;case"bilinear":a=Rm(z,p,d,k,w,v,i,m,f,s,u);break;default:throw new Error("Error in Transform: Expect 'nearest' or "+"'bilinear', but got ".concat(o))}T[i*N+e*S+n*x+s]=a}return s.makeTensorInfo(y,a.dtype,T)}return{dataId:s.write(T,y,a.dtype),shape:a.shape,dtype:a.dtype}}};function Em(t,e,n){switch(n){case"reflect":return function(t,e){let n=t;if(n<0)if(e<=1)n=0;else{const t=2*e;n<t&&(n=t*Math.trunc(-n/t)+n),n=n<-e?n+t:-n-1}else if(n>e-1)if(e<=1)n=0;else{const t=2*e;n-=t*Math.trunc(n/t),n>=e&&(n=t-n-1)}return i.util.clamp(0,n,e-1)}(t,e);case"wrap":return function(t,e){let n=t;if(n<0)if(e<=1)n=0;else{const t=e-1;n+=e*(Math.trunc(-n/t)+1)}else if(n>e-1)if(e<=1)n=0;else{const t=e-1;n-=e*Math.trunc(n/t)}return i.util.clamp(0,n,e-1)}(t,e);case"nearest":return function(t,e){return i.util.clamp(0,t,e-1)}(t,e);default:return function(t){return t}(t)}}function _m(t,e,n,s,i,a,r,o,l,u,c){return 0<=o&&o<e&&0<=l&&l<n?t[r*s+o*i+l*a+u]:c}function Mm(t,e,n,s,i,a,r,o,l,u,c){return _m(t,e,n,s,i,a,r,Math.round(o),Math.round(l),u,c)}function Rm(t,e,n,s,i,a,r,o,l,u,c){const h=Math.floor(o),p=Math.floor(l),d=h+1,f=p+1;return(d-o)*((f-l)*_m(t,e,n,s,i,a,r,h,p,u,c)+(l-p)*_m(t,e,n,s,i,a,r,h,f,u,c))+(o-h)*((f-l)*_m(t,e,n,s,i,a,r,d,p,u,c)+(l-p)*_m(t,e,n,s,i,a,r,d,f,u,c))}var Lm=n(23705);const Om={kernelName:i.Unique,backendName:"cpu",kernelFunc:function(t){const{inputs:e,attrs:n,backend:s}=t,{axis:i}=n,{x:a}=e;(0,Nc.C)(a,"unique");const r=s.data.get(a.dataId).values,{outputValues:o,outputShape:l,indices:u}=(0,Lm.w)(r,i,a.shape,a.dtype);return[s.makeTensorInfo(l,a.dtype,o),s.makeTensorInfo([u.length],"int32",u)]}};const Wm={kernelName:i.Unpack,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{value:i}=e;let{axis:a}=s;a<0&&(a+=i.shape.length);const r=i.shape.length,o=i.shape[a],l=new Array(r-1);let u=0;for(let d=0;d<r;d++)d!==a&&(l[u++]=i.shape[d]);const c=new Array(r).fill(0),h=i.shape.slice();h[a]=1;const p=new Array(o);for(let d=0;d<p.length;d++){c[a]=d;const t=(0,xh.di)({inputs:{x:i},backend:n,attrs:{begin:c,size:h}});p[d]=Vc({inputs:{x:t},backend:n,attrs:{shape:l}}),n.disposeIntermediateTensorInfo(t)}return p}};const Bm={kernelName:i.UnsortedSegmentSum,backendName:"cpu",kernelFunc:function(t){const{inputs:e,backend:n,attrs:s}=t,{x:a,segmentIds:r}=e,{numSegments:o}=s;(0,Nc.C)(a,"unsortedSegmentSum");const l=[],u=[],c=a.shape.length-r.shape.length;let h=r;for(let i=0;i<c;++i){const t=Dp({inputs:{input:h},backend:n,attrs:{dim:i+1}});h=t,u.push(t)}for(let d=0;d<o;++d){const t=i.util.createScalarValue(d,"int32"),e=n.makeTensorInfo([],"int32",t),s=(0,wp.LC)({inputs:{a:e,b:h},backend:n}),r=(0,Ch.wg)({inputs:{x:s},backend:n,attrs:{dtype:"float32"}}),o=(0,fp.lw)({inputs:{a:r,b:a},backend:n}),c=gp({inputs:{x:o},backend:n,attrs:{axis:0,keepDims:!1}});l.push(c),u.push(e),u.push(s),u.push(r),u.push(o),u.push(c)}const p=af({inputs:l,backend:n,attrs:{axis:0}});return u.forEach((t=>n.disposeIntermediateTensorInfo(t))),p}},Pm=[Zc,Kc.lO,Yc,Qc,Hc.UK,$c,eh,nh,sh,ih,rh,lh,ch,fh,gh,wh,vh,Ih,Nh,qc,Sh,Th,Ah,Fh.LY,Dh,Ch.Ml,Eh.uf,Mh,Rh.v,Lh,Hh,jh,Gh,qh,Zh,Kh,Jh,Xh,$h,tp,ep,np,sp,ip,rp,op,lp,up,cp,hp,pp,dp,bp,Ac,kp,wp.RY,Ap,Fp.AC,Cp,Ep.Yp,Pp,Hp,Vp,jp.Hc,Zp,Kp,Jp,Xp,$p,td.xp,ed.VM,Fc.F,nd,Bh,id,rd,ld,Cc,ud.YW,cd.Q,pd,dd.Fx,md,bd,wd,Nd,Sd,xd,Fd,Dd.l9,Cd,Ed,_d,Md,Rd,Ld,Od,Wd.Nu,Bd,Hd,Gd,fp.tJ,qd.hd,Kd,Yd,Qd,$d.AL,tf,sf,rf,of,cf,Rc,hf.S3,df,mf,yf,kf,Ph.r,Rp,vf,Oc,Bc,jc,If,Nf,Sf,xf,Tf,zf,Ff,Df.Lt,Ef,Rf,Lf,Pf,Pc.X3,Hf,jf,qf,xh.lv,jd,Jf,Yf,Qf,tm,nm,sm,im,am,rm.Fu,om,cm,hm.C,dm,mm,ym,km,vm,Lp.Zl,yp,Nm,xm,Tm,Am,Dm,Cm,th.W,Om,Wm,Bm,nf];for(const Hm of Pm)(0,i.registerKernel)(Hm);var Um=n(59788);i.version_core,Um.$p}}]);
//# sourceMappingURL=2667.e0110f6d.chunk.js.map